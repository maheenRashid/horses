require 'image'
npy4th = require 'npy4th'
require 'data_aflw';
require 'cunn'
require 'cudnn'
require 'nn';
require 'optim'
require 'stn'
npy4th=require 'npy4th';
require 'torchx';
require 'gnuplot';
dump=require 'dump';

function getLossD(pred_output_all,gt_output_all)
	-- print (pred_output_all:size())
	-- print (gt_output_all:size());
	loss_all=torch.zeros(pred_output_all:size()):type(pred_output_all:type());

	for im_idx=1,pred_output_all:size(1) do

		local gt_output=gt_output_all[im_idx];
		local pred_output=pred_output_all[im_idx];

		local gt_index = gt_output[{{},3}];
		local gt_output = gt_output[{{},{1,2}}]:clone();
		gt_output=gt_output:view(gt_output:nElement());
		
		local x = pred_output-gt_output;
		local loss = torch.zeros(x:size(1),1);
		for i=1,loss:size(1) do
			local loss_curr=x[i];
			if gt_index[math.floor((i+1)/2)]<0 then
				loss[i]=0;
			else
				if torch.abs(loss_curr)<1 then
					loss[i]=loss_curr;
				elseif loss_curr<0 then
					loss[i]=-1;
				else
					loss[i]=1;
				end
			end
		end

		loss_all[im_idx]=loss;

	end
	return loss_all;
end

function getLoss(pred_output_all,gt_output_all)
	loss_all=torch.zeros(pred_output_all:size(1)):type(pred_output_all:type());

	for im_idx=1,pred_output_all:size(1) do

		local gt_output=gt_output_all[im_idx];
		local pred_output=pred_output_all[im_idx];

		local gt_index=gt_output[{{},3}];
		local gt_output=gt_output[{{},{1,2}}]:clone();
		gt_output=gt_output:view(gt_output:nElement());
		local x = torch.abs(pred_output-gt_output);
		local loss = torch.zeros(x:size(1));
		local loss_total=0;
		for i=1,loss:size(1) do
			local loss_curr=x[i];
			if gt_index[math.floor((i+1)/2)]<0 then
				loss[i]=0;
			else
				if loss_curr<1 then
					loss[i]=loss_curr*loss_curr*0.5;
				else
					loss[i]=loss_curr-0.5;
				end
				loss_total=loss_total+loss[i];
			end
		end

		loss = loss_total/torch.sum(gt_index:gt(0));
		loss_all[im_idx]=loss;

	end
	return torch.mean(loss_all);
end

local fevalScore = function(x)
    if x ~= parameters then
	    parameters:copy(x)
    end
    
    td:getTrainingData();
    td.training_set.data=td.training_set.data:cuda();
	td.training_set.label=td.training_set.label:cuda();
	local batch_inputs=td.training_set.data;
	local batch_targets=td.training_set.label;
    
    gradParameters:zero()
    local outputs=net:forward(batch_inputs);
    local dloss = getLossD(outputs,batch_targets);
    local loss = getLoss(outputs,batch_targets);

    net:backward(batch_inputs, dloss)
    
    return loss, gradParameters;
end

function plotLossFigure(losses,losses_iter,val_losses,val_losses_iter,out_file_loss_plot) 
	gnuplot.pngfigure(out_file_loss_plot)
	-- print (out_file_loss_plot)
	local losses_tensor = torch.Tensor{losses_iter,losses};
    if #val_losses>0 then
    	local val_losses_tensor=torch.Tensor{val_losses_iter,val_losses}
		gnuplot.plot({'Train Loss',losses_tensor[1],losses_tensor[2]},{'Val Loss',val_losses_tensor[1],val_losses_tensor[2]});
		gnuplot.grid(true)
	else
		gnuplot.plot({'Train Loss ',losses_tensor[1],losses_tensor[2]});

	end
	gnuplot.title('Losses'..losses_iter[#losses_iter])
	gnuplot.xlabel('Iterations');
	gnuplot.ylabel('Loss');
	gnuplot.plotflush();
	-- gnuplot.pngfigure(out_file_loss_plot);
end


function main(params) 

	local data_path=params.data_path;
	local out_dir=params.outDir
    local net_file=params.model
    if params.limit<0 then
    	params.limit=nil;
    end
    local val_data_path;
    local val_human_path
    if params.testAfter>0 then
    	val_data_path= params.val_data_path
    end

    paths.mkdir(out_dir);
    local out_dir_intermediate=paths.concat(out_dir,'intermediate');
    local out_dir_final=paths.concat(out_dir,'final');
    paths.mkdir(out_dir_intermediate);
    paths.mkdir(out_dir_final);
    
    local out_file_net=paths.concat(out_dir_final,'model_all_final.dat');
    local out_file_loss=paths.concat(out_dir_final,'loss_final.npy');
    local out_file_loss_val=paths.concat(out_dir_final,'loss_final_val.npy');
    
    local out_file_intermediate_pre = paths.concat(out_dir_intermediate,'model_all_');
    local out_file_loss_intermediate_pre = paths.concat(out_dir_intermediate,'loss_all_');

    local out_file_loss_plot=paths.concat(out_dir_intermediate,'loss_all.png');
    local out_file_log=paths.concat(out_dir_intermediate,'log.txt');
    local logger=torch.DiskFile(out_file_log,'w');

	
    -- log = torch.DiskFile(out_file_log,'w');
    -- log:writeString(params);
    -- local str_curr;
    -- str_curr=
    logger:writeString(dump.tostring(params)..'\n');
    print (params);

    cutorch.setDevice(params.gpu);

    local optimState       
    local optimMethod      

	optimMethod = optim.adam
	optimState={learningRate=params.learningRate,learningRateDecay=params.learningRateDecay ,beta1=params.beta1 ,beta2=params.beta2 ,epsilon=params.epsilon }


    logger:writeString(dump.tostring('loading network')..'\n');
    print ('loading network');
    net = torch.load(net_file);
    logger:writeString(dump.tostring('done loading network')..'\n');
    print ('done loading network');
    -- logger:writeString(dump.tostring(net)..'\n');
    print (net);

    logger:writeString(dump.tostring('making cuda')..'\n');
    print ('making cuda');
    net = net:cuda();
    logger:writeString(dump.tostring('done')..'\n');
    print ('done');

    logger:writeString(dump.tostring('loading params')..'\n');
    print ('loading params');
    parameters, gradParameters = net:getParameters()
    logger:writeString(dump.tostring('loading done')..'\n');
    print ('loading done');
    logger:writeString(dump.tostring(optimState)..'\n');
    print (optimState)

    local data_params={file_path=data_path,
					batch_size=params.batchSize,
					mean_file=params.mean_im_path,
					std_file=params.std_im_path,
					augmentation=params.augmentation,
					limit=params.limit};

	td=data(data_params);

    if params.testAfter>0 then
    	data_params.file_path = params.val_data_path;
    	data_params.augmentation=false;
    	vd=data(data_params);
	end
    

    local losses = {};
    local losses_iter = {};

    local val_losses = {};
    local val_losses_iter = {};

    

    for i=1,params.iterations do

        local _, minibatch_loss = optimMethod(fevalScore,parameters, optimState)
        losses[#losses + 1] = minibatch_loss[1] -- append the new loss        
        losses_iter[#losses_iter +1] = i;

        if i%params.dispAfter==0 then
        	local disp_str=string.format("lr: %6s, minibatches processed: %6s, loss = %6.6f", optimState.learningRate,i, losses[#losses])
            logger:writeString(dump.tostring(disp_str)..'\n');
            print (disp_str);

            local str_score=''..losses[#losses];
            
            if str_seg=='nan' or str_score=='nan' then
                logger:writeString(dump.tostring('QUITTING')..'\n');
                print('QUITTING');
                break;
            end

            

        end


        if i%params.testAfter==0 and params.testAfter>0 then 
            net:evaluate();
            vd:getTrainingData();

            vd.training_set.data=vd.training_set.data:cuda();
			vd.training_set.label=vd.training_set.label:cuda();
			local batch_inputs=vd.training_set.data;
			local batch_targets=vd.training_set.label;
		    
		    gradParameters:zero()
		    local outputs=net:forward(batch_inputs);
		    local loss = getLoss(outputs,batch_targets);

            val_losses[#val_losses+1]=loss;
            val_losses_iter[#val_losses_iter+1]=i;

            net:training();
            disp_str=string.format("minibatches processed: %6s, val loss = %6.6f", i, val_losses[#val_losses])
            logger:writeString(dump.tostring(disp_str)..'\n');
            print(disp_str)
        end

        -- check if model needs to be saved. save it.
        -- also save losses
        if i%params.saveAfter==0 then
            local out_file_intermediate=out_file_intermediate_pre..i..'.dat';
            torch.save(out_file_intermediate,net);
            local out_file_loss_intermediate=out_file_loss_intermediate_pre..i..'.npy';
            npy4th.savenpy(out_file_loss_intermediate, torch.Tensor(losses))
            
            if params.testAfter>0 then 
                local out_file_loss_intermediate=out_file_loss_intermediate_pre..i..'_val.npy';
                npy4th.savenpy(out_file_loss_intermediate, torch.Tensor(val_losses))
            end
        end

        if i%params.dispPlotAfter==0 then
        	plotLossFigure(losses,losses_iter,val_losses,val_losses_iter,out_file_loss_plot);
        end

	end

    -- save final model
    torch.save(out_file_net,net);
    npy4th.savenpy(out_file_loss, torch.Tensor(losses))
    
    if params.testAfter>0 and #val_losses>0 then
        npy4th.savenpy(out_file_loss_val, torch.Tensor(val_losses))
    end
    plotLossFigure(losses,losses_iter,val_losses,val_losses_iter,out_file_loss_plot);
-- sha1:e99282b7d438:baa6fb60c347c8647277116d2efd6cdeedfbb012
	-- logger:close..'\n'()


end



cmd = torch.CmdLine()
cmd:text()
cmd:text('Train TPS network')
cmd:text()
cmd:text('Options')

local epoch_size=56;

-- cmd:option('-model','/home/SSD3/maheen-data/horse_project/vanilla/vanilla_scratch_bn.dat','model to load')
cmd:option('-model','/home/SSD3/maheen-data/temp/vanilla_train_bn_check/intermediate/model_all_3444.dat');
cmd:option('-outDir','/home/SSD3/maheen-data/temp/vanilla_train_horse_bn_hlr_longer','directory to write output');

cmd:option('-mean_im_path','/home/SSD3/maheen-data/data_face_network/aflw_cvpr_40_mean.png');
cmd:option('-std_im_path','/home/SSD3/maheen-data/data_face_network/aflw_cvpr_40_std.png');
cmd:option('-limit',-1,'num of training data to read');
cmd:option('-iterations',50*epoch_size,'num of iterations to run');
cmd:option('-saveAfter',2*epoch_size,'num of iterations after which to save model');
cmd:option('-batchSize',64,'batch size');
cmd:option('-testAfter',30,'num iterations after which to get validation loss');
cmd:option('-dispAfter',1,'num iterations after which to display training loss');
cmd:option('-dispPlotAfter',30,'num iterations after which to display training loss');

-- cmd:option('-val_data_path','/home/SSD3/maheen-data/horse_project/data_check/aflw/pairs_val.txt')
-- cmd:option('-data_path','/home/SSD3/maheen-data/data_face_network/aflw_cvpr_train.txt')

cmd:option('-val_data_path','/home/SSD3/maheen-data/horse_project/data_check/horse/pairs_val.txt')
cmd:option('-data_path','/home/SSD3/maheen-data/horse_project/data_check/horse/pairs.txt')
-- cmd:option('-data_path','/home/SSD3/maheen-data/data_face_network/aflw_cvpr_train.txt')


cmd:option('learningRate', 1e-3)
cmd:option('learningRateDecay',5e-6)
cmd:option('beta1', 0.9)
cmd:option('beta2', 0.999)
cmd:option('epsilon', 1e-8)
cmd:option('augmentation' , true);

cmd:option('-gpu',1,'gpu to run the training on');
cmd:text()

params = cmd:parse(arg)
main(params);






-- meta_debug_dir='/home/SSD3/maheen-data/temp/debug_aflw_40_vanilla_selected_drop_smooth_vanilla';

-- model_file='/home/SSD3/maheen-data/data_project/vanilla/vanilla_scratch_bn.dat';
-- out_file_viz=paths.concat(meta_debug_dir,'viz_data_aflw_check');
-- paths.mkdir(out_file_viz);

-- local data_dir='/home/SSD3/maheen-data/data_face_network';
-- local dataset_pre='aflw_cvpr';
-- local train_pair_file=paths.concat(data_dir,dataset_pre..'_train.txt');

-- local str_resize='40';
-- local mean_im_path=paths.concat(data_dir,dataset_pre..'_'..str_resize..'_mean.png');
-- local std_im_path=paths.concat(data_dir,dataset_pre..'_'..str_resize..'_std.png');
-- augmentation=true;

-- local data_params={file_path_horse=train_pair_file,
-- 					batch_size=10,
-- 					mean_file=mean_im_path,
-- 					std_file=std_im_path,
-- 					augmentation=augmentation};

-- td=data(data_params);
-- td:getTrainingData();

-- print (td.training_set.data:size())
-- print (td.training_set.label:size())

-- model=torch.load(model_file);
-- model:evaluate();
-- model:cuda();
-- td.training_set.data=td.training_set.data:cuda();
-- td.training_set.label=td.training_set.label:cuda();
-- results=model:forward(td.training_set.data);
-- print (results:size());


-- for i=1,results:size(1) do
-- 	local pred_curr=10*results[i];
-- 	local gt_curr=td.training_set.label[i];
-- 	print ('pred_curr');
-- 	print (pred_curr);
	
-- 	print ('gt_curr');
-- 	print (gt_curr);

-- 	local loss=getLoss(pred_curr,gt_curr);
-- 	local loss_d=getLossD(pred_curr,gt_curr);

-- 	print ('loss');
-- 	print (loss);
	
-- 	print ('loss_d');
-- 	print (loss_d);

-- end



