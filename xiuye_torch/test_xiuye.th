require 'image'
npy4th = require 'npy4th'
require 'data_horseHuman_xiuye';
require 'cunn'
require 'cudnn'
require 'nn';
require 'optim'
require 'stn'
npy4th=require 'npy4th';
require 'torchx';


function saveImageAndAnno(out_file,im,label,mean,nonormalize)
	local label=label:clone();
	label=label:t();
	
	local im= im:clone();
	im=im/255;

	if not nonormalize then
		label[{1,{}}]= (label[{1,{}}]+0.5)*im:size(2)
		label[{2,{}}]= (label[{2,{}}]+0.5)*im:size(3);
	end
	
	local colors={{255,0,0},{255,255,0},{0,0,255},{0,255,0},{255,0,255}}

	for label_idx=1,label:size(2) do
		local x=label[1][label_idx]
		local y=label[2][label_idx]
		if x>210 then
			x=210;
		end
		if x<0 then
			x=1;
		end
		if y>210 then
			y=210;
		end
		if y<0 then
			y=1;
		end
		
		im=image.drawText(im,"X",y,x,{color=colors[label_idx]})
	end
	
	image.save(out_file,im);	
end


function getLossD(pred_output,gt_output)
	local inter_oc_x=gt_output:select(2,1)-gt_output:select(2,3);
	local inter_oc_y=gt_output:select(2,2)-gt_output:select(2,4);
	local inter_oc=torch.sqrt(torch.pow(inter_oc_x,2)+torch.pow(inter_oc_y,2))+1e-6;
	inter_oc = torch.repeatTensor(inter_oc:view(inter_oc:size(1),1),1,pred_output:size(2));
	local diff = torch.cdiv((pred_output-gt_output),inter_oc);
	diff=diff/pred_output:size(1);
	return diff;
end

function getLoss(pred_output,gt_output)
	local inter_oc_x=gt_output:select(2,1)-gt_output:select(2,3);
	local inter_oc_y=gt_output:select(2,2)-gt_output:select(2,4);
	local inter_oc=torch.sqrt(torch.pow(inter_oc_x,2)+torch.pow(inter_oc_y,2))+1e-6;
	inter_oc = torch.repeatTensor(inter_oc:view(inter_oc:size(1),1),1,pred_output:size(2));
	-- print (inter_oc:size())
	local diff = torch.cdiv((pred_output-gt_output),inter_oc);
	diff=torch.sum(torch.pow(diff,2))/(2*pred_output:size(1));
	return diff;
end

function getTransformedLandMarkPoints(td,out_grids)
	t_pts_all=td.training_set_horse.label:clone();

	for i=1,out_grids:size(1) do
		local grid_curr=out_grids[i];
		local labels=td.training_set_horse.label[i];
		
		for label_idx=1,labels:size(1) do
			local label_curr=labels[{label_idx,{1,2}}];
			label_curr=label_curr:view(1,1,2);
			label_curr=torch.repeatTensor(label_curr,grid_curr:size(1),grid_curr:size(2),1);
			
			local dist=torch.sum(torch.pow(grid_curr-label_curr,2),3);
			dist=dist:view(dist:size(1),dist:size(2));
			local idx=torch.find(dist:eq(torch.min(dist)),1)[1]
			local row = math.ceil(idx/dist:size(2));
			local col = idx%dist:size(2);
			if col==0 then
				col=dist:size(2);
			end
			t_pts_all[i][label_idx][1]=row;
			t_pts_all[i][label_idx][2]=col;
		end
	end
	return t_pts_all;
end

function getTransformedImageAndPointsTrain(td,mean_im,std_im,input_size,kp_net_size)
	
	local out_grids=locnet:forward(td.training_set_horse.data:clone());	
	local trans_out=trans:forward(td.training_set_horse.data:clone());
	bil:forward({trans_out,out_grids});
	local ims = trans_2:forward(bil.output);
	
	for idx_rgb=1,3 do
		ims[{{},idx_rgb,{},{}}]=ims[{{},idx_rgb,{},{}}]+td.params.mean[idx_rgb]
	end
	
	local ims_org=ims:clone()
	ims_org = ims_org:double();
	
	local out_grids_new=torch.zeros(kp_net_size[1],kp_net_size[2],2):cuda();
	for r=1,out_grids_new:size(1) do
		for c=1,out_grids_new:size(2) do
			out_grids_new[{r,c,1}]=-1+(1/20*r);
			out_grids_new[{r,c,2}]=-1+(1/20*c);
		end
	end
	out_grids_new=out_grids_new:view(1,out_grids_new:size(1),out_grids_new:size(2),out_grids_new:size(3));
	out_grids_new=torch.repeatTensor(out_grids_new,ims:size(1),1,1,1);
	
	local trans_out=trans:forward(ims);
	bil:forward({trans_out,out_grids_new});
	ims = trans_2:forward(bil.output);

	-- subtract mean and divide by std from im
--	print("type for ims, mean_im, std_im:")
--	print(ims:type())
--	print(mean_im:type())
--	print(std_im:type())
	ims=torch.cdiv((ims-mean_im),std_im);
	
	-- transform points (assumes all 5 points are visible)
	local t_pts_all=getTransformedLandMarkPoints(td,out_grids);
	
	-- transform pts to -0.5 to 0.5
	t_pts_all[{{},{},1}]=(t_pts_all[{{},{},1}]/input_size[1])-0.5;
	t_pts_all[{{},{},2}]=(t_pts_all[{{},{},2}]/input_size[2])-0.5;
	t_pts_all=t_pts_all[{{},{},{1,2}}]:clone();
	t_pts_all=t_pts_all:resize(t_pts_all:size(1),t_pts_all:size(2)*t_pts_all:size(3))
	
	-- set td accordingly
--	return ims,t_pts_all
	return ims,t_pts_all,ims_org,out_grids;
end

function getTransformedImageAndPoints(td,mean_im,std_im,input_size,kp_net_size)
	
	
	local out_grids=locnet:forward(td.training_set_horse.data:cuda());	
	local trans_out=trans:forward(td.training_set_horse.data:cuda());
	bil:forward({trans_out,out_grids});
	local ims = trans_2:forward(bil.output);

	-- ims=ims:double();
	-- add mean to im
	-- local mean=torch.Tensor(td.params.mean):type(ims:type());
	-- print (mean);
	for idx_rgb=1,3 do
		ims[{{},idx_rgb,{},{}}]=ims[{{},idx_rgb,{},{}}]+td.params.mean[idx_rgb]
	end

	ims=ims:double();
	local ims_org=ims:clone();

	ims_new=torch.zeros(ims:size(1),ims:size(2),kp_net_size[1],kp_net_size[2]):type(ims:type());
	for im_no=1,ims:size(1) do
		ims_new[im_no]=image.scale(ims[im_no],kp_net_size[2],kp_net_size[1],"bilinear");
	end
	ims=ims_new;

	-- subtract mean and divide by std from im
	ims=torch.cdiv((ims-mean_im),std_im);
	-- print (torch.min(ims),torch.max(ims));

	-- transform points (assumes all 5 points are visible)
	local t_pts_all=getTransformedLandMarkPoints(td,out_grids);
	-- print (t_pts_all:size(),t_pts_all:type(),torch.min(t_pts_all),torch.max(t_pts_all));
	
	-- transform pts to -0.5 to 0.5
	t_pts_all[{{},{},1}]=(t_pts_all[{{},{},1}]/input_size[1])-0.5;
	t_pts_all[{{},{},2}]=(t_pts_all[{{},{},2}]/input_size[2])-0.5;
	

	t_pts_all=t_pts_all[{{},{},{1,2}}]:clone();
	t_pts_all=t_pts_all:resize(t_pts_all:size(1),t_pts_all:size(2)*t_pts_all:size(3))

	-- set td accordingly
	return ims,t_pts_all,ims_org,out_grids;
end


local fevalScore = function(x)
    if x ~= parameters then
	    parameters:copy(x)
    end
  
    td:getTrainingData();
    td.training_set_horse.data=td.training_set_horse.data:cuda();
	td.training_set_horse.label=td.training_set_horse.label:cuda();

	local ims,t_pts_all,_,_ = getTransformedImageAndPointsTrain(td,mean_im,std_im,{size_out,size_out},{size_kp_net,size_kp_net});

    local batch_inputs = ims:clone();
    -- :cuda();
    local batch_targets = t_pts_all:clone();
    -- :cuda();
    
    gradParameters:zero()
 
    local outputs=kp_net:forward(batch_inputs);
    local dloss = getLossD(outputs,batch_targets);
    local loss = getLoss(outputs,batch_targets);

    kp_net:backward(batch_inputs, dloss)
    
    return loss, gradParameters;
end



function main(params)

	print (params);
	-- local out_dir='/home/SSD3/maheen-data/horse_project/vanilla/'
	-- paths.mkdir(out_dir);
	local kp_net_file= params.kp_net_file
	-- paths.concat(out_dir,'/home/SSD3/maheen-data/horse_project/vanilla/vanilla_fcScratch.dat');

	tps_model_path = params.tps_model_path 

	mean_im_path = params.mean_im_path 
	std_im_path = params.std_im_path 

	horse_path = params.horse_path 
	human_path = params.human_path 
	humanImage = params.humanImage 
	augmentation = params.augmentation 

	out_dir = params.out_dir 
	paths.mkdir(out_dir);
	size_out = params.size_out 
	size_kp_net = params.size_kp_net 
	limit = params.limit 
	batch_size = params.batch_size 
	max_iter= params.max_iter



    local val_horse_path;
    local val_human_path
    if params.testAfter>0 then
    	val_horse_path= params.val_horse_path
    	val_human_path= params.val_human_path
    end

    paths.mkdir(out_dir);
    local out_dir_intermediate=paths.concat(out_dir,'intermediate');
    local out_dir_final=paths.concat(out_dir,'final');
    paths.mkdir(out_dir_intermediate);
    paths.mkdir(out_dir_final);
    
    local out_file_net=out_dir_final..'/'..'model_all_final.dat';

    local out_file_loss=out_dir_final..'/'..'loss_final.npy';
    
    local out_file_intermediate_pre = out_dir_intermediate..'/'..'model_all_';
    local out_file_loss_intermediate_pre = out_dir_intermediate..'/'..'loss_all_';

	mean_im = image.load(mean_im_path)*255;
	std_im = image.load(std_im_path)*255;
	mean_im = torch.repeatTensor(mean_im,batch_size,1,1,1):cuda();
	std_im = torch.repeatTensor(std_im,batch_size,1,1,1):cuda();

	locnet = torch.load(tps_model_path);
	locnet:evaluate();
	trans  =  nn.Transpose({2,3},{3,4}):cuda();
	bil = nn.BilinearSamplerBHWD():cuda();
	trans_2 = nn.Transpose({2,4},{3,4}):cuda();

	if params.limit<0 then
    	td=data({file_path_horse=params.horse_path,file_path_human=params.human_path,humanImage=params.humanImage,augmentation=params.augmentation});
    else
    	td=data({file_path_horse=params.horse_path,file_path_human=params.human_path,limit=params.limit,humanImage=params.humanImage,augmentation=params.augmentation});
    end
	td.batch_size=params.batch_size;
	td.params.input_size={size_out,size_out};


	if params.testAfter>0 then
	    if params.limit<0 then
	    	vd=data({file_path_horse=val_horse_path,file_path_human=val_human_path,humanImage=params.humanImage,augmentation=params.augmentation});
	    else
	    	vd=data({file_path_horse=val_horse_path,file_path_human=val_human_path,limit=params.limit,humanImage=params.humanImage,augmentation=params.augmentation});
	    end
	    vd.params.input_size={size_out,size_out};
	    vd.batch_size=params.batch_size;
	end

	kp_net =torch.load(kp_net_file);
	kp_net =kp_net:cuda();

	parameters, gradParameters = kp_net:getParameters()

	-- optimState = {learningRate = 0.0001, momentum = 0.9}
	-- optimMethod = optim.sgd

	optimMethod = optim.adam
	optimState={learningRate=params.learningRate,learningRateDecay=params.learningRateDecay ,beta1=params.beta1 ,beta2=params.beta2 ,epsilon=params.epsilon }

	local losses = {};
    local val_losses = {};
    
	for i=1,max_iter do
		local _, minibatch_loss = optimMethod(fevalScore,parameters, optimState)
		losses[#losses + 1] = minibatch_loss[1] -- append the new loss

		if i%params.dispAfter==0 then
            print(string.format("lr: %6s, minibatches processed: %6s, loss = %6.6f", optimState.learningRate,i, 
                losses[#losses]));

            local str_score=''..losses[#losses];
            
            if str_score=='nan' then
                print('QUITTING');
                break;
            end
        end

        if i%params.testAfter==0 and params.testAfter>0 then 
            kp_net:evaluate();

            vd:getTrainingData();
		    vd.training_set_horse.data=vd.training_set_horse.data:cuda();
			vd.training_set_horse.label=vd.training_set_horse.label:cuda();
			local ims,t_pts_all,_,_ = getTransformedImageAndPointsTrain(vd,mean_im,std_im,{size_out,size_out},{size_kp_net,size_kp_net});

		    local batch_inputs = ims:cuda();
		    local batch_targets = t_pts_all:cuda();
		    
		    -- gradParameters:zero()
		 
		    local outputs=kp_net:forward(batch_inputs);
		    local loss = getLoss(outputs,batch_targets);
            
            val_losses[#val_losses+1]=loss;
            
            kp_net:training();
            print(string.format("minibatches processed: %6s, val loss = %6.6f", i, val_losses[#val_losses]))
        end

        -- check if model needs to be saved. save it.
        -- also save losses
        if i%params.saveAfter==0 then
            local out_file_intermediate=out_file_intermediate_pre..i..'.dat';
            torch.save(out_file_intermediate,kp_net);
            local out_file_loss_intermediate=out_file_loss_intermediate_pre..i..'.npy';
            npy4th.savenpy(out_file_loss_intermediate, torch.Tensor(losses))
            
            if params.testAfter>0 then 
                local out_file_loss_intermediate=out_file_loss_intermediate_pre..i..'_val.npy';
                npy4th.savenpy(out_file_loss_intermediate, torch.Tensor(val_losses))
            end
        end

		
	end
	print ('saving',kp_net);
	torch.save(out_file_net,kp_net);
    npy4th.savenpy(out_file_loss, torch.Tensor(losses))


end

function getPointsOriginalImage(outputs,out_grids)
	
	-- local for_loss_all=torch.zeros(outputs:size()):type(outputs:type());
	local for_loss_all=torch.zeros(outputs:size()):type(outputs:type());

	local outputs=outputs:clone();
	outputs=outputs:resize(outputs:size(1),outputs:size(2)/2,2):transpose(2,3);
	
	outputs[{{},1,{}}]= (outputs[{{},1,{}}]+0.5)*out_grids:size(2)
	outputs[{{},2,{}}]= (outputs[{{},2,{}}]+0.5)*out_grids:size(3);
	
	

	for idx_im=1,outputs:size(1) do
		for label_idx=1,outputs:size(3) do

			local r_idx=math.floor(outputs[idx_im][1][label_idx]);
			local c_idx=math.floor(outputs[idx_im][2][label_idx]);
			
			if r_idx<1 then
				r_idx=1;
			end

			if c_idx<1 then
				c_idx=1;
			end

			if r_idx>out_grids:size(2) then
				r_idx=out_grids:size(2);
			end

			if c_idx>out_grids:size(3) then
				c_idx=out_grids:size(3);
			end
		
			local grid_value_r=out_grids[idx_im][r_idx][c_idx][1];
			local grid_value_c=out_grids[idx_im][r_idx][c_idx][2];

			local for_loss_r= grid_value_r/2;
			local for_loss_c= grid_value_c/2;
			for_loss_all[idx_im][(2*label_idx)-1]=for_loss_r;
			for_loss_all[idx_im][2*label_idx]=for_loss_c;
		end
	end
	return for_loss_all

end

function test(params)
	-- local out_dir='/home/SSD3/maheen-data/horse_project/vanilla/'
	-- paths.mkdir(out_dir);
	local kp_net_file= params.kp_net_file
	tps_model_path = params.tps_model_path 
	mean_im_path = params.mean_im_path 
	std_im_path = params.std_im_path 

	horse_path = params.horse_path 
	human_path = params.human_path 
	humanImage = params.humanImage 
	augmentation = params.augmentation 

	out_dir = params.out_dir 
	paths.mkdir(out_dir);
	size_out = params.size_out 
	size_kp_net = params.size_kp_net 
	limit = params.limit 
	batch_size = params.batch_size 
	max_iter= params.max_iter

    paths.mkdir(out_dir);
    local out_dir_intermediate=paths.concat(out_dir,'intermediate');
    local out_dir_final=paths.concat(out_dir,'final');
    paths.mkdir(out_dir_intermediate);
    paths.mkdir(out_dir_final);
    
    local out_file_loss=out_dir_final..'/'..'loss_val_final.npy';
    
    local out_file_loss_intermediate_pre = out_dir_intermediate..'/'..'loss_val_all_';


	val_horse_path= params.val_horse_path
	val_human_path= params.val_human_path

	locnet = torch.load(tps_model_path);

	mean_im = image.load(mean_im_path)*255;
	std_im = image.load(std_im_path)*255;
	mean_im = torch.repeatTensor(mean_im,batch_size,1,1,1);
	std_im = torch.repeatTensor(std_im,batch_size,1,1,1);
	
	mean_im = mean_im:cuda()
	std_im = mean_im:cuda()

	locnet=locnet:cuda();
	locnet:evaluate();
	trans  =  nn.Transpose({2,3},{3,4}):cuda();
	bil = nn.BilinearSamplerBHWD():cuda();
	trans_2 = nn.Transpose({2,4},{3,4}):cuda();
	print (kp_net_file);
	kp_net =torch.load(kp_net_file);
	kp_net =kp_net:cuda();

	if params.limit<0 then
    	vd=data({file_path_horse=val_horse_path,file_path_human=val_human_path});
    else
    	vd=data({file_path_horse=val_horse_path,file_path_human=val_human_path,limit=params.limit});
    end

	vd.params.input_size={size_out,size_out};
	vd.batch_size=params.batch_size;

    kp_net:evaluate();
    val_losses={};
    for i=1,1 do
	    vd:getTrainingData();
	    vd.training_set_horse.data=vd.training_set_horse.data:cuda();
		vd.training_set_horse.label=vd.training_set_horse.label:cuda();
--		function getTransformedImageAndPointsTrain(td,mean_im,std_im,input_size,kp_net_size)
		local ims,t_pts_all,ims_org,out_grids = getTransformedImageAndPointsTrain(vd,mean_im,std_im,{size_out,size_out},{size_kp_net,size_kp_net});
		
		local batch_inputs = ims:cuda();
	    local batch_targets = t_pts_all:cuda();		 
	    local outputs=kp_net:forward(batch_inputs);

	    outputs=getPointsOriginalImage(outputs,out_grids);
	
		for idx_im=1,vd.training_set_horse.data:size(1) do
		 	local out_file=paths.concat(out_dir,idx_im..'.jpg');
		 	local im_curr=vd.training_set_horse.data[idx_im];
			im_curr = im_curr:double()
			for idx_rgb=1,3 do
				im_curr[{idx_rgb,{},{}}] = im_curr[{idx_rgb,{},{}}] + vd.params.mean[idx_rgb]
			end
		 	local label_curr=outputs[idx_im]:resize(outputs:size(2)/2,2);
		 	-- print (idx_im,label_curr)
		 	saveImageAndAnno(out_file,im_curr,label_curr:double(),vd.params.mean)
		 end
	
	    
	    batch_targets = vd.training_set_horse.label:clone();
		batch_targets=batch_targets/2;
		batch_targets=batch_targets[{{},{},{1,2}}]:clone();
		batch_targets=batch_targets:resize(batch_targets:size(1),batch_targets:size(2)*batch_targets:size(3));
		batch_targets=batch_targets:cuda();
		-- print (batch_targets);
	    local loss = getLoss(outputs,batch_targets);
		
		val_losses[#val_losses+1]=loss;
        print(string.format("minibatches processed: %6s, val loss = %6.6f", i, val_losses[#val_losses]))
    
--        if i%params.saveAfter==0 then            
--            local out_file_loss_intermediate=out_file_loss_intermediate_pre..i..'.npy';
--            npy4th.savenpy(out_file_loss_intermediate, torch.Tensor(val_losses))
--        end

	end

    npy4th.savenpy(out_file_loss, torch.Tensor(val_losses))

--function saveImageAndAnno(out_file,im,label,mean,nonormalize)




end


-- td:getTrainingData();
-- td.training_set_horse.data=td.training_set_horse.data:cuda();
-- td.training_set_horse.label=td.training_set_horse.label:cuda();	
-- ims,t_pts_all,ims_org=getTransformedImageAndPoints(td,mean_im,std_im,{size_out,size_out},{size_kp_net,size_kp_net})
-- ims=ims:cuda();
-- t_pts_all=t_pts_all:cuda();
-- output=kp_net:forward(ims);
-- for idx_im=1,td.training_set_horse.data:size(1) do
-- 	local out_file=paths.concat(out_dir,idx_im..'.jpg');
-- 	local im_curr=ims_org[idx_im];
-- 	local label_curr=output[idx_im]:resize(output:size(2)/2,2);
-- 	-- print (idx_im,label_curr)
-- 	saveImageAndAnno(out_file,im_curr,label_curr:double(),td.params.mean)
-- end


cmd = torch.CmdLine()
cmd:text()
cmd:text('Train TPS network')
cmd:text()
cmd:text('Options')

--cmd:option('-kp_net_file','/home/SSD3/maheen-data/horse_project/vanilla/vanilla_fcScratch.dat','model to load')
cmd:option('-tps_model_path','/home/SSD3/maheen-data/horse_human_fiveKP/intermediate/model_all_1500.dat')

-- model_path = '/home/SSD3/maheen-data/horse_human_fiveKP/intermediate/model_all_1500.dat';
cmd:option('-mean_im_path','/home/laoreja/finetune-deep-landmark/dataset/train/horse_5_points_40_mean.png');
cmd:option('-std_im_path','/home/laoreja/finetune-deep-landmark/dataset/train/horse_5_points_40_std.png');

cmd:option('horse_path' , '/home/SSD3/maheen-data/horse_project/horse_resize/matches_5_train_fiveKP.txt');
cmd:option('human_path' , '/home/SSD3/maheen-data/horse_project/aflw/matches_5_train_fiveKP_noIm.txt');
cmd:option('humanImage' , false);
cmd:option('augmentation' , false);

cmd:option('out_dir','/home/SSD3/maheen-data/training_kp_withWarp_test_final');
cmd:option ('size_out',224);

cmd:option('size_kp_net' , 40)
cmd:option('limit' , -1)
cmd:option('batch_size' , 64)
cmd:option('max_iter',5000)


cmd:option('learningRate', 1e-5)
cmd:option('learningRateDecay', 5e-4)
cmd:option('beta1', 0.9)
cmd:option('beta2', 0.999)
cmd:option('epsilon', 1e-8)

cmd:option('-saveAfter',75,'num of iterations after which to save model');

cmd:option('-testAfter',30,'num iterations after which to get validation loss');
cmd:option('-dispAfter',1,'num iterations after which to display training loss');

cmd:option('-val_horse_path','/home/SSD3/maheen-data/horse_project/horse_resize/matches_5_val_fiveKP_single.txt')
cmd:option('-val_human_path','/home/SSD3/maheen-data/horse_project/aflw/matches_5_val_fiveKP_noIm_single.txt')

cmd:option('-gpu',1,'gpu to run the training on');
cmd:text()

-- cmd:option('-kp_net_file','/home/SSD3/maheen-data/temp/horse_human/viz_transform_aflw_val_xiuye/intermediate/model_all_100.dat','model to load')

--params = cmd:parse(arg)
--main(params);


 cmd:option('-kp_net_file','/home/SSD3/maheen-data/training_kp_withWarp/final/model_all_final.dat','model to load')
 params = cmd:parse(arg)
 test(params);




