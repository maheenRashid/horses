require 'image'
npy4th = require 'npy4th'
require 'data_aflw_optimizing';
require 'data_horseHuman_xiuye_optimizing';
require 'cunn'
require 'cudnn'
require 'nn';
require 'optim'
require 'stn'
require 'torchx';
require 'gnuplot';
dump=require 'dump';
tps_helper=require 'tps_helper';

visualize=require 'visualize';
loss_helper=require 'loss_helper';



function getOptimStateTotal(params,layer_pivot,logger)
    local optimStateTotal={}

    for layer_num=1,#parameters do
        local str=''..layer_num;
        for layer_size_idx=1,#parameters[layer_num]:size() do
            str=str..' '..parameters[layer_num]:size(layer_size_idx);
        end

        local learningRate_curr=params.learningRate;
        if layer_num<=layer_pivot[1] then
            -- if params.divisor==0 then
            --     learningRate_curr=0;
            -- else
                learningRate_curr=learningRate_curr*params.multiplierBottom;
            -- end
        elseif layer_num<=layer_pivot[2] then
            learningRate_curr=learningRate_curr*params.multiplierMid;
        else
            learningRate_curr=learningRate_curr;
        end

        local optimState_curr={learningRate=learningRate_curr,
                learningRateDecay=params.learningRateDecay ,
                beta1=params.beta1 ,
                beta2=params.beta2 ,
                epsilon=params.epsilon };

        str=str..' '..optimState_curr.learningRate;
        -- print (str);
        logger:writeString(dump.tostring(str)..'\n');
        optimStateTotal[#optimStateTotal+1]=optimState_curr;
    end
    return optimStateTotal;
end




function createIntegratedModel(tps_model_path,face_detection_model,full_size,full_model_flag)
    local tot_net;
    if not full_model_flag then
        local locnet = torch.load(tps_model_path);
        local tranet=nn.Transpose({2,3},{3,4})
        local concat=nn.ConcatTable()
        concat:add(tranet)
        concat:add(locnet)

        local net=nn.Sequential();
        net:add(concat)
        net:add(nn.BilinearSamplerBHWD())
        local spanet;

        if full_size then
            spanet=net;
            spanet:add(nn.Transpose({3,4},{2,3}));
        else
            local downGrid=nn.Sequential();
            downGrid:add(nn.Identity());

            local paranet=nn.ParallelTable();
            paranet:add(net);
            paranet:add(downGrid);

            spanet=nn.Sequential();
            spanet:add(paranet);
            spanet:add(nn.BilinearSamplerBHWD());
            spanet:add(nn.Transpose({3,4},{2,3}));
        end
        
        tot_net=nn.Sequential();
        tot_net:add(spanet);
        local kp_net=torch.load(face_detection_model);
        -- if full_model_flag then
        --     kp_net=kp_net:get(2);
        -- end
        tot_net:add(kp_net);
    else
        tot_net=torch.load(face_detection_model);
    end

    cudnn.convert(tot_net,cudnn);

    return tot_net;
end

function getDownSampleGrid(batchSize)
    local out_grids_new=torch.zeros(40,40,2);
    for r=1,out_grids_new:size(1) do
        for c=1,out_grids_new:size(2) do
            out_grids_new[{r,c,1}]=-1+(1/20*r);
            out_grids_new[{r,c,2}]=-1+(1/20*c);
        end
    end
    out_grids_new=out_grids_new:view(1,out_grids_new:size(1),out_grids_new:size(2),out_grids_new:size(3));
    out_grids_new=torch.repeatTensor(out_grids_new,batchSize,1,1,1);
    return out_grids_new;
end

function doTheForward(batch_inputs,batch_targets,saveImage,padTPS)
    
    local batch_inputs_view;
    if saveImage then
        batch_inputs_view=batch_inputs:double():clone();
    end

    local inputs;
    if ds_grid then
        inputs={batch_inputs,ds_grid:clone()}
    else
        inputs=batch_inputs;    
    end

    local midoutputs;
    local tps_output;
    local temp;
    local bil_input;
    if padTPS then
        tps_output = net:get(1):get(1):get(2):forward(inputs);
        bil_input = net:get(1):get(1):get(1):forward(inputs);
        tps_output[tps_output:le(-1)]=-1;
        tps_output[tps_output:ge(1)]=1;
        temp=net:get(1):get(2):forward{bil_input,tps_output};
        midoutputs=net:get(1):get(3):forward(temp);
    else
        midoutputs=net:get(1):forward(inputs);
    end

    local midoutputs_view;
    if saveImage then
        midoutputs_view=midoutputs:double():clone();
    end

    midoutputs=tps_helper:switchMeans(midoutputs,td.params.imagenet_mean,td.mean_im,td.std_im)
    -- print ('mean',torch.mean(midoutputs));

    local outputs=net:get(2):forward(midoutputs);
    
    local tps_layer;
    if ds_grid then
        tps_layer= net:get(1):get(1):get(1):get(1):get(2);
        tps_layer=tps_layer:get(#tps_layer);
    else
        tps_layer= net:get(1):get(1):get(2);
        tps_layer=tps_layer:get(#tps_layer);
    end

    local t_pts=tps_helper:getPointsOriginalImage(outputs,tps_layer.output)

    if saveImage then
        -- local t_pts_copy=t_pts:clone();
        local outputs_view=outputs:view(outputs:size(1),outputs:size(2)/2,2):clone();
        local t_pts_view=t_pts:view(t_pts:size(1),t_pts:size(2)/2,2):clone();
        local colors={{255,0,0},{255,255,0},{0,0,255},{0,255,0},{255,0,255}};
   
        -- visualize=Visualize();
        t_pts_view=t_pts_view:transpose(2,3)
        visualize:saveBatchImagesWithKeypoints(batch_inputs_view,t_pts_view,
                                            {saveImage,'_org.jpg'},td.params.imagenet_mean,{-1,1},colors);
        visualize:saveBatchImagesWithKeypoints(midoutputs_view,outputs_view:transpose(2,3),
                                            {saveImage,'.jpg'},td.params.imagenet_mean,{-1,1},colors);
    end

    local dloss = loss_helper:getLossD_RCNN(t_pts,batch_targets);
    local loss = loss_helper:getLoss_RCNN(t_pts,batch_targets);
    return loss,dloss,midoutputs,inputs,tps_layer,tps_output,temp,bil_input;
end


function doTheForwardEuclidean(batch_inputs,batch_targets,saveImage,padTPS)
    local batch_inputs_view;
    if saveImage then
        batch_inputs_view=batch_inputs:double():clone();
    end

    local inputs;
    if ds_grid then
        inputs={batch_inputs,ds_grid:clone()}
    else
        inputs=batch_inputs;    
    end

    -- local midoutputs=net:get(1):forward(inputs);
    local midoutputs;
    if padTPS then
        local tps_output = net:get(1):get(1):get(2):forward(inputs);
        local bil_input = net:get(1):get(1):get(1):forward(inputs);
        tps_output[tps_output:le(-1)]=-1;
        tps_output[tps_output:ge(1)]=1;
        local temp=net:get(1):get(2):forward{bil_input,tps_output};
        midoutputs=net:get(1):get(3):forward(temp);
    else
        midoutputs=net:get(1):forward(inputs);
    end

    local midoutputs_view;
    if saveImage then
        midoutputs_view=midoutputs:double():clone();
    end

    midoutputs=tps_helper:switchMeans(midoutputs,td.params.imagenet_mean,td.mean_im,td.std_im)
    -- print ('mean',torch.mean(midoutputs));

    local outputs=net:get(2):forward(midoutputs);
    
    local tps_layer;
    if ds_grid then
        tps_layer= net:get(1):get(1):get(1):get(1):get(2);
        tps_layer=tps_layer:get(#tps_layer);
    else
        tps_layer= net:get(1):get(1):get(2);
        tps_layer=tps_layer:get(#tps_layer);
    end

    local t_pts=tps_helper:getPointsOriginalImage(outputs,tps_layer.output)

    if saveImage then
        -- local t_pts_copy=t_pts:clone();
        local outputs_view=outputs:view(outputs:size(1),outputs:size(2)/2,2):clone();
        local t_pts_view=t_pts:view(t_pts:size(1),t_pts:size(2)/2,2):clone();
        local colors={{255,0,0},{255,255,0},{0,0,255},{0,255,0},{255,0,255}};
   
        -- visualize=Visualize();
        t_pts_view=t_pts_view:transpose(2,3)
        visualize:saveBatchImagesWithKeypoints(batch_inputs_view,t_pts_view,{saveImage,'_org.jpg'},td.params.imagenet_mean,{-1,1},colors);
        visualize:saveBatchImagesWithKeypoints(midoutputs_view,outputs_view:transpose(2,3),{saveImage,'.jpg'},td.params.imagenet_mean,{-1,1},colors);
    end

    -- local dloss = loss_helper:getLossD_RCNN(t_pts,batch_targets);
    local loss,loss_all = loss_helper:getLoss_Euclidean(t_pts,batch_targets);
    return loss,loss_all,midoutputs,inputs;
end

function doTheUpdate(optimMethod,optimStateTotal,padTPS,full_back_prop)
    
    td:getTrainingData();
    td.training_set.data=td.training_set.data:cuda();
    td.training_set.label=td.training_set.label:cuda();
	local batch_inputs=td.training_set.data;
	local batch_targets=td.training_set.label;
    
    net:zeroGradParameters();
    local loss,dloss,midoutputs,inputs=doTheForward(batch_inputs,batch_targets,nil,padTPS);

    local grad_mid=net:get(2):backward(midoutputs, dloss);
    -- grad_mid=grad_mid:clone();
    if full_back_prop then
        local std_rep=td.std_im:view(1,td.std_im:size(1),td.std_im:size(2),td.std_im:size(3)):clone();
        std_rep=torch.repeatTensor(std_rep,grad_mid:size(1),1,1,1):type(grad_mid:type());
        grad_mid=grad_mid:cdiv(std_rep);    
        -- net:get(2):backward(midoutputs,dloss);
        net:get(1):backward(inputs, grad_mid);
    end
    

    for layer_num =1, #parameters do
        local fevalScoreVar = function(x)
            return loss, gradParameters[layer_num]
        end
        optimMethod(fevalScoreVar, parameters[layer_num], optimStateTotal[layer_num]);
    end
    return loss;

end


function doTheUpdateDualLoss(optimMethod,optimStateTotal,padTPS,full_back_prop,affine_flag)
    
    local t=os.clock()
    td:getTrainingData();

    print ('td:getTrainingData():',os.clock()-t);

    local t=os.clock()
    td.training_set_horse.data=td.training_set_horse.data:cuda();
    td.training_set_human.label=td.training_set_human.label:cuda();
    td.training_set_horse.label=td.training_set_horse.label:cuda();
    print ('making cuda:',os.clock()-t);
    
    local t=os.clock()
    local horse_labels,human_labels,batch_inputs,_,data_idx=td:getBatchPoints(true)
    print ('td:getBatchPoints:',os.clock()-t);

    local batch_targets_final=torch.zeros(#horse_labels,
                                        td.training_set_horse.label:size(2),
                                        td.training_set_horse.label:size(3)):type(td.training_set_horse.label:type())
    for label_num=1,batch_targets_final:size(1) do
        batch_targets_final[label_num]=td.training_set_horse.label[data_idx[label_num]]:clone();
    end

    
    net:zeroGradParameters();
    local t=os.clock()
    local loss,dloss,midoutputs,inputs,tps_layer,tps_output,temp,bil_input=
                                            doTheForward(batch_inputs,batch_targets_final,nil,padTPS);
    print ('doTheForward():',os.clock()-t);

    local grad_mid=net:get(2):backward(midoutputs, dloss);

    local loss_mid;
    
    if full_back_prop then
        
        local std_rep=td.std_im:view(1,td.std_im:size(1),td.std_im:size(2),td.std_im:size(3)):clone();
        std_rep=torch.repeatTensor(std_rep,grad_mid:size(1),1,1,1):type(grad_mid:type());
        grad_mid=grad_mid:cdiv(std_rep);    
    
        if padTPS then        
            local grad_mid_bil = net:get(1):get(3):backward(temp,grad_mid);
            local grad_mid_tps = net:get(1):get(2):backward({bil_input,tps_output},grad_mid_bil);
            net:get(1):get(1):get(1):backward(inputs,grad_mid_tps[1]);
            net:get(1):get(1):get(2):backward(inputs,grad_mid_tps[2]);
        else   
            net:get(1):backward(inputs, grad_mid);
        end

        if params.dual then
            local gt_output;
            if affine_flag then
                local t=os.clock()
                local gt_params=tps_helper:getGTParams(human_labels,horse_labels);
                print ('getGTParams:',os.clock()-t);                
                -- print (tps_layer);
                gt_output=tps_layer:clone():forward(gt_params)
                -- :get(1):get(2).output;

            else
                gt_output = tps_layer:getGTOutput(human_labels,horse_labels);
            end
            -- local -- t=os.clock()
            local batch_targets = gt_output:clone();            
            -- print ('getGTParams:',os.clock()-t);                
            local lossD_mid=loss_helper:getLossD_L2(net:get(1):get(1):get(2).output,batch_targets);
            loss_mid=loss_helper:getLoss_L2(net:get(1):get(1):get(2).output,batch_targets);
            net:get(1):get(1):get(2):backward(inputs,lossD_mid);
            
        end
    end

    
    for layer_num =1, #parameters do
        local fevalScoreVar = function(x)
            return loss, gradParameters[layer_num]
        end
        optimMethod(fevalScoreVar, parameters[layer_num], optimStateTotal[layer_num]);
    end

    return loss,loss_mid;

end



function test(params)
    local data_path=params.data_path;
    local out_dir=params.outDir
    local net_file=params.model
    if params.limit<0 then
        params.limit=nil;
    end
    val_data_path= params.val_data_path

    paths.mkdir(out_dir);
    local out_dir_images=params.outDirTest;
    -- paths.concat(out_dir,'test_images');
    paths.mkdir(out_dir_images);
    local out_file_loss_val=paths.concat(out_dir_images,'loss_final_val.npy');
    local out_file_loss_val_ind=paths.concat(out_dir_images,'loss_final_val_ind.npy');
    local out_file_log=paths.concat(out_dir_images,'log_test.txt');
    local logger=torch.DiskFile(out_file_log,'w');

    logger:writeString(dump.tostring(params)..'\n');
    cutorch.setDevice(params.gpu);
    logger:writeString(dump.tostring('loading network')..'\n');
    -- print ('loading network');

    net=torch.load(params.full_model_path);
    if params.full_size then
        ds_grid=nil;
    else
        ds_grid= getDownSampleGrid(params.batchSize);
    end

    logger:writeString(dump.tostring('done loading network')..'\n');
    -- print ('done loading network');
    -- logger:writeString(dump.tostring(net)..'\n');
    print (net);

    logger:writeString(dump.tostring('making cuda')..'\n');
    -- print ('making cuda');
    net = net:cuda();
    if ds_grid then
        ds_grid = ds_grid:cuda();
    end
    net:evaluate();

    logger:writeString(dump.tostring('done')..'\n');
    -- print ('done');

    logger:writeString(dump.tostring('loading params')..'\n');
    -- print ('loading params');
    parameters, gradParameters = net:getParameters()
    logger:writeString(dump.tostring('loading done')..'\n');
    -- print ('loading done');
    logger:writeString(dump.tostring(optimState)..'\n');
    -- print (optimState)

    local data_params={file_path=val_data_path,
                    batch_size=params.batchSize,
                    mean_file=params.mean_im_path,
                    std_file=params.std_im_path,
                    augmentation=false,
                    limit=params.limit,
                    input_size={224,224},
                    imagenet_mean=true};

    
    td=data(data_params);
   
    local val_losses = {};
    local val_losses_iter = {};

    local val_losses_ind={};

    for i=1,params.iterations do

            td:getTrainingData();

            td.training_set.data=td.training_set.data:cuda();
            td.training_set.label=td.training_set.label:cuda();

            local batch_inputs=td.training_set.data;
            local batch_targets=td.training_set.label;
            
            local loss,loss_all = doTheForwardEuclidean(batch_inputs,batch_targets,paths.concat(out_dir_images,i..'_'),params.padTPS)
            for idx_ind=1,loss_all:size(1) do
                val_losses_ind[#val_losses_ind+1]=loss_all[idx_ind];
            end
            -- gradParameters:zero()
            -- local outputs=net:forward(batch_inputs);
            -- local loss = loss_helper:getLoss_RCNN(outputs,batch_targets);

            val_losses[#val_losses+1]=loss;
            val_losses_iter[#val_losses_iter+1]=i;

            -- net:training();
            disp_str=string.format("minibatches processed: %6s, val loss = %6.6f", i, val_losses[#val_losses])
            logger:writeString(dump.tostring(disp_str)..'\n');
            print(disp_str)

    end
    val_losses_ind=torch.Tensor(val_losses_ind);
    print (val_losses_ind:size())
    if val_losses_ind:size(1)>#td.lines_horse then
        val_losses_ind=val_losses_ind[{{1,#td.lines_horse}}];
    end
    print (val_losses_ind:size())
    print (params.full_model_path);
    disp_str=string.format("minibatches processed: all, val loss = %6.6f", torch.mean(val_losses_ind))
    logger:writeString(dump.tostring(disp_str)..'\n');
    print(disp_str)

    npy4th.savenpy(out_file_loss_val, torch.Tensor(val_losses))
    npy4th.savenpy(out_file_loss_val_ind, val_losses_ind)

end

function main(params) 
    torch.setnumthreads(1);
	local data_path=params.data_path;
	local out_dir=params.outDir
    local net_file=params.model
    if params.limit<0 then
    	params.limit=nil;
    end
    local val_data_path;
    local val_human_path

    -- full_back_prop=params.full_back_prop;
    -- print ('full_back_prop',full_back_prop);    
    if params.testAfter>0 then
    	val_data_path= params.val_data_path
    end
    

    paths.mkdir(out_dir);
    out_dir_debug=paths.concat(out_dir,'debug');
    print (out_dir_debug);
    paths.mkdir(out_dir_debug);
    local out_dir_intermediate=paths.concat(out_dir,'intermediate');
    local out_dir_final=paths.concat(out_dir,'final');
    paths.mkdir(out_dir_intermediate);
    paths.mkdir(out_dir_final);
    
    local out_file_net=paths.concat(out_dir_final,'model_all_final.dat');
    local out_file_loss=paths.concat(out_dir_final,'loss_final.npy');
    local out_file_loss_val=paths.concat(out_dir_final,'loss_final_val.npy');
    
    local out_file_intermediate_pre = paths.concat(out_dir_intermediate,'model_all_');
    local out_file_loss_intermediate_pre = paths.concat(out_dir_intermediate,'loss_all_');

    local out_file_loss_plot=paths.concat(out_dir_intermediate,'loss_all.png');
    local out_file_loss_mid_plot=paths.concat(out_dir_intermediate,'loss_mid.png');
    local out_file_loss_val_plot=paths.concat(out_dir_intermediate,'loss_val.png');
    local out_file_log=paths.concat(out_dir_intermediate,'log.txt');
    local logger=torch.DiskFile(out_file_log,'w');

	
    -- log = torch.DiskFile(out_file_log,'w');
    -- log:writeString(params);
    -- local str_curr;
    -- str_curr=
    logger:writeString(dump.tostring(params)..'\n');
    -- print (params);

    cutorch.setDevice(params.gpu);

    local optimState       
    local optimMethod      

	optimMethod = optim.adam
	optimState={learningRate=params.learningRate,learningRateDecay=params.learningRateDecay ,beta1=params.beta1 ,beta2=params.beta2 ,epsilon=params.epsilon }


    logger:writeString(dump.tostring('loading network')..'\n');
    print ('loading network');

    net=createIntegratedModel(params.tps_model_path,params.face_detection_model_path,params.full_size,params.full_model_flag);
    net:training();
    
    -- if not params.full_back_prop then
    --     print ('making ev');
    --     net:get(1):evaluate();
    -- end

    if params.full_size then
        ds_grid=nil;
    else
        ds_grid= getDownSampleGrid(params.batchSize);
    end
    
    -- net = torch.load(net_file);
    logger:writeString(dump.tostring('done loading network')..'\n');
    print ('done loading network');
    -- logger:writeString(dump.tostring(net)..'\n');
    print (net);

    logger:writeString(dump.tostring('making cuda')..'\n');
    print ('making cuda');
    net = net:cuda();
    if ds_grid then
        ds_grid = ds_grid:cuda();
    end

    logger:writeString(dump.tostring('done')..'\n');
    print ('done');

    logger:writeString(dump.tostring('loading params')..'\n');
    print ('loading params');
    
    parameters, gradParameters = net:parameters()
    local optimStateTotal=getOptimStateTotal(params,{10,22},logger)

    logger:writeString(dump.tostring('loading done')..'\n');
    print ('loading done');
    logger:writeString(dump.tostring(optimState)..'\n');
    -- print (optimState)


    local data_params={file_path_horse=params.horse_data_path,
                        file_path_human=params.human_data_path,
                        batch_size=params.batchSize,
                        mean_file=params.mean_im_path,
                        std_file=params.std_im_path,
                        augmentation=params.augmentation,
                        limit=params.limit,
                        input_size={224,224},
                        humanImage=false,
                        };
                        
    td=data_horseHuman(data_params);
    
    if params.testAfter>0 then
    	data_params.file_path = params.val_data_path;
    	data_params.augmentation=false;
        data_params.imagenet_mean=true;
    	vd=data(data_params);
	end
    

    local losses = {};
    local losses_mid = {};
    local losses_iter = {};

    local val_losses = {};
    local val_losses_iter = {};
    local counter=0;
    -- local visualize=Visualize();

    for i=1,params.iterations do

        if params.decreaseAfter then
            if i%params.decreaseAfter==0 and counter==0 then
                counter=counter+1;
                params.learningRate=params.learningRate/10;
                optimState.learningRate=params.learningRate;
                optimStateTotal=getOptimStateTotal(params,{10,22},logger);
            end
        end

        -- local train_loss = doTheUpdate(optimMethod,optimStateTotal,params.padTPS,params.full_back_prop);
        local t=os.clock();
        local train_loss,loss_mid = doTheUpdateDualLoss(optimMethod,optimStateTotal,params.padTPS,params.full_back_prop,params.affine);
        print (os.clock()-t);

        if not loss_mid then
            loss_mid=train_loss;
        end
        losses[#losses + 1] = train_loss;
        losses_mid[#losses_mid+1]=loss_mid;
        -- val_losses[#val_losses + 1] = loss_mid;
        -- val_losses_iter[#val_losses_iter + 1] = i;

        losses_iter[#losses_iter +1] = i;

        -- local p,gp=net:parameters();
        -- for layer_num=1,#p do
        --     if #p[layer_num]:size()>1 then
        --         -- print (layer_num,p[layer_num]:size());
        --     --     print (layer_num,'p',torch.min(p[layer_num]),torch.max(p[layer_num]));
        --         print (layer_num,'gp',torch.min(gp[layer_num]),torch.max(gp[layer_num]));
        --         -- print (layer_num,torch.norm(p[layer_num])/torch.norm(gp[layer_num]));
        --     end
        -- end

        if i%params.dispAfter==0 then
            -- local disp_str=string.format("lr: %6s, minibatches processed: %6s, loss = %6.6f", optimState.learningRate,i, losses[#losses]);    
        	local disp_str=string.format("lr: %6s, minibatches processed: %6s, loss = %6.6f, midloss = %6.6f", optimState.learningRate,i, losses[#losses],losses_mid[#losses_mid])
            logger:writeString(dump.tostring(disp_str)..'\n');
            print (disp_str);

            local str_score=''..losses[#losses];
            
            if str_seg=='nan' or str_score=='nan' then
                logger:writeString(dump.tostring('QUITTING')..'\n');
                print('QUITTING');
                break;
            end
        end


        if i%params.testAfter==0 and params.testAfter>0 then 
            -- counter=counter+1;
            net:evaluate();
            vd:getTrainingData();

            vd.training_set.data=vd.training_set.data:cuda();
			vd.training_set.label=vd.training_set.label:cuda();

			local batch_inputs=vd.training_set.data;
			local batch_targets=vd.training_set.label;
		    
            local loss = doTheForward(batch_inputs,batch_targets,nil,params.padTPS);
                -- ,paths.concat(out_dir_debug,''..i..'_'),params.padTPS);
            -- 
                -- 
            val_losses[#val_losses+1]=loss;
            val_losses_iter[#val_losses_iter+1]=i;

            net:training();
            disp_str=string.format("minibatches processed: %6s, val loss = %6.6f", i, val_losses[#val_losses])
            logger:writeString(dump.tostring(disp_str)..'\n');
            print(disp_str)
        end

        -- check if model needs to be saved. save it.
        -- also save losses
        if i%params.saveAfter==0 then
            local out_file_intermediate=out_file_intermediate_pre..i..'.dat';
            
            torch.save(out_file_intermediate,net);
            
            local out_file_loss_intermediate=out_file_loss_intermediate_pre..i..'.npy';
            npy4th.savenpy(out_file_loss_intermediate, torch.Tensor(losses))
            
            if params.testAfter>0 then 
                local out_file_loss_intermediate=out_file_loss_intermediate_pre..i..'_val.npy';
                npy4th.savenpy(out_file_loss_intermediate, torch.Tensor(val_losses))
            end
            visualize:plotLossFigure(losses,losses_iter,{},{},out_file_loss_plot);
            visualize:plotLossFigure(losses_mid,losses_iter,{},{},out_file_loss_mid_plot);
            visualize:plotLossFigure(losses,losses_iter,val_losses,val_losses_iter,out_file_loss_val_plot);
        end

        if i%params.dispPlotAfter==0 then
            -- visualize:plotLossFigure(losses,losses_iter,{},{},out_file_loss_plot);
            -- visualize:plotLossFigure(losses_mid,losses_iter,{},{},out_file_loss_mid_plot);
            visualize:plotLossFigure(losses,losses_iter,val_losses,val_losses_iter,out_file_loss_val_plot);
        end

	end

    -- save final model
    -- torch.save(out_file_net,net);

    -- npy4th.savenpy(out_file_loss, torch.Tensor(losses))
    
    -- if params.testAfter>0 and #val_losses>0 then
    --     npy4th.savenpy(out_file_loss_val, torch.Tensor(val_losses))
    -- end
    -- visualize:plotLossFigure(losses,losses_iter,{},{},out_file_loss_plot);
    -- visualize:plotLossFigure(losses_mid,losses_iter,{},{},out_file_loss_mid_plot);
    -- visualize:plotLossFigure(losses,losses_iter,val_losses,val_losses_iter,out_file_loss_val_plot);
end



cmd = torch.CmdLine()
cmd:text()
cmd:text('Train Full network')
cmd:text()
cmd:text('Options')

local epoch_size=1;
-- 7;

cmd:option('-tps_model_path','/home/SSD3/maheen-data/horse_project/full_system_sheep_data_5kp/tps/final/model_all_final.dat');
    -- 'temp');
-- cmd:option('affine',true);

    -- '/home/SSD3/maheen-data/horse_project/tps_25_varlr_fixEvaluate/final/model_all_final.dat');
-- cmd:option('-tps_model_path','/home/SSD3/maheen-data/horse_project/tps_25_varlr_fixEvaluate_rotFix/final/model_all_final.dat');

-- cmd:option('-tps_model_path','/home/SSD3/maheen-data/horse_project/models/conv5_2fc_bn_normalXavier_128_50_withTPS.dat');
cmd:option('-face_detection_model_path','/home/SSD3/maheen-data/horse_project/vanilla_train_face_big/intermediate/model_all_16762.dat');


-- cmd:option('-full_model_flag',false);
-- cmd:option('-outDir','/home/SSD3/maheen-data/horse_project/ft_horse_2loss_25_scratch_1e-2_1_10','directory to write output');

-- cmd:option('-face_detection_model_path','/home/SSD3/maheen-data/horse_project/ft_horse_2loss_25_scratch_1e-2_1_10/final/model_all_final.dat');
-- cmd:option('-full_model_flag',true);
-- cmd:option('-tps_model_path','/home/SSD3/maheen-data/horse_project/models/conv5_2fc_bn_normalXavier_128_50_withTPS.dat');
-- cmd:option('-full_model_flag',false);
-- cmd:option('-outDir','/home/SSD3/maheen-data/horse_project/ft_horse_2loss_25_scratch_1e-2_1_10_noGray/scratch_full_1e-2','directory to write output');
-- cmd:option('learningRate', 1e-2)
-- cmd:option('multiplierBottom',1/10);
-- cmd:option('multiplierMid',1);

-- cmd:option('-face_detection_model_path','/home/SSD3/maheen-data/horse_project/ft_horse_2loss_25_1e-2_100/final/model_all_final.dat');
-- cmd:option('-full_model_flag',true);
cmd:option('-outDir','/home/SSD3/maheen-data/horse_project/debug_time');
    -- full_system_horse_data_5kp/full_system');
    -- '/home/SSD3/maheen-data/horse_project/ft_horse_2loss_25_1e-2_100_withGray_fullback_padded','directory to write output');
cmd:option('-full_back_prop',true,'whether to back prop through spatial net as well');

cmd:option('padTPS',false,'no gray tps');



cmd:option('-face_detection_model_path','/home/SSD3/maheen-data/horse_project/full_system_sheep_data_5kp/full_system/final/model_all_final.dat');
cmd:option('-full_model_flag',true);
-- cmd:option('-outDir','/home/SSD3/maheen-data/horse_project/ft_horse_2loss_25_1e-2_100_withGray_halfback/resume_1e-3','directory to write output');
-- cmd:option('-full_back_prop',false,'whether to back prop through spatial net as well');
-- cmd:option('padTPS',false,'no gray tps');


-- cmd:option('-outDir','/home/SSD3/maheen-data/horse_project/ft_horse_2loss_25_1e-2_100_withGray_halfback','directory to write output');
-- cmd:option('-full_back_prop',false,'whether to back prop through spatial net as well');
-- cmd:option('padTPS',false,'no gray tps');


cmd:option('learningRate', 1e-3)
cmd:option('multiplierBottom',1/100);
cmd:option('multiplierMid',1/10);


cmd:option('-mean_im_path','/home/SSD3/maheen-data/data_face_network/aflw_cvpr_224_mean.png');
cmd:option('-std_im_path','/home/SSD3/maheen-data/data_face_network/aflw_cvpr_224_std.png');
cmd:option('-full_size',true,'true when on 224');

cmd:option('-limit',-1,'num of training data to read');
cmd:option('-iterations',5*epoch_size,'num of iterations to run');
cmd:option('-saveAfter',30*epoch_size,'num of iterations after which to save model');
cmd:option('-batchSize',64,'batch size');
cmd:option('-testAfter',30,'num iterations after which to get validation loss');
cmd:option('-dispAfter',1,'num iterations after which to display training loss');
cmd:option('-dispPlotAfter',30,'num iterations after which to display training loss');
cmd:option('-decreaseAfter',50*epoch_size,'num of iterations to run');

cmd:option('-mean_im_path','/home/SSD3/maheen-data/data_face_network/aflw_cvpr_224_mean.png');
cmd:option('-std_im_path','/home/SSD3/maheen-data/data_face_network/aflw_cvpr_224_std.png');


cmd:option('-val_data_path','/home/SSD3/maheen-data/horse_project/files_for_sheepCode/sheep_test_us_sheep_minloss.txt');
    -- 'temp');
    -- '/home/SSD3/maheen-data/horse_project/data_check/horse/matches_5_val_allKP_minLoss_clean.txt')
-- cmd:option('-val_human_data_path','/home/SSD3/maheen-data/horse_project/data_check/aflw/matches_5_val_allKP_minLoss_noIm_clean_padded.txt')
cmd:option('-horse_data_path','/home/SSD3/maheen-data/horse_project/files_for_sheepCode/sheep_train_us_sheep_minloss.txt');
cmd:option('-human_data_path','/home/SSD3/maheen-data/horse_project/files_for_sheepCode/sheep_train_us_face_noIm_minloss.txt');

    -- 'temp');
    -- '/home/SSD3/maheen-data/horse_project/data_check/horse/matches_5_train_allKP_minLoss_clean_padded.txt')
    -- 'temp');
    -- '/home/SSD3/maheen-data/horse_project/data_check/aflw/matches_5_train_allKP_minLoss_noIm_clean_padded.txt')


cmd:option('dual',true);


cmd:option('learningRateDecay',5e-6)
cmd:option('beta1', 0.9)
cmd:option('beta2', 0.999)
cmd:option('epsilon', 1e-8)
cmd:option('augmentation' , true);

cmd:option('-gpu',1,'gpu to run the training on');


params = cmd:parse(arg)
main(params);

-- cmd:option('-full_model_path',params.face_detection_model_path);
--     -- paths.concat(params.outDir,'final/model_all_final.dat'),'full model path for testing');
--     -- ,'temp');
-- cmd:option('-iterations',2,'num of iterations to run');
-- cmd:option('-batchSize',100,'batch size');
-- cmd:option('-outDirTest',paths.concat(params.outDir,'test_images'));
--     -- 'temp');
-- params = cmd:parse(arg)
-- print (params.full_model_path);
-- test(params)