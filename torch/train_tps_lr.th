require 'image'
npy4th = require 'npy4th'
require 'data_horseHuman_xiuye';
require 'cunn'
require 'cudnn'
require 'nn';
require 'optim'
require 'stn'
npy4th=require 'npy4th';
require 'torchx';
require 'gnuplot';
dump=require 'dump';
tps_helper=require 'tps_helper';
visualize=require 'visualize';
loss_helper=require 'loss_helper';


function getLossD(pred_output,gt_output)
	local lossD=pred_output-gt_output
	lossD=torch.mul(lossD,2);
	return lossD;
end

function getLoss(pred_output,gt_output)
	local loss=torch.pow(pred_output-gt_output,2);
	loss=torch.mean(loss);
	return loss;
end

local fevalScore = function(x)
    if x ~= parameters then
	    parameters:copy(x)
    end
    
    td:getTrainingData();
    td.training_set_horse.data=td.training_set_horse.data:cuda();
	td.training_set_human.label=td.training_set_human.label:cuda();
	td.training_set_horse.label=td.training_set_horse.label:cuda();

    local horse_labels,human_labels,batch_inputs,_,_=td:getBatchPoints()
    local gt_output=tps:getGTOutput(human_labels,horse_labels);
    local batch_targets = gt_output:clone();
    
    gradParameters:zero()
    local outputs=net:forward(batch_inputs);
    local dloss = getLossD(outputs,batch_targets);
    local loss = getLoss(outputs,batch_targets);

    net:backward(batch_inputs, dloss)

    return loss, gradParameters;
end

function main(params) 
    local out_dir=params.outDir
    if params.limit<0 then
        params.limit=nil;
    end

    paths.mkdir(out_dir);
    
    if params.debug then
        local out_dir_debug=paths.concat(out_dir,'debug');
        paths.mkdir(out_dir_debug);
    end


    local out_dir_intermediate=paths.concat(out_dir,'intermediate');
    local out_dir_final=paths.concat(out_dir,'final');
    paths.mkdir(out_dir_intermediate);
    paths.mkdir(out_dir_final);
    
    local out_file_net=paths.concat(out_dir_final,'model_all_final.dat');
    local out_file_loss=paths.concat(out_dir_final,'loss_final.npy');
    local out_file_loss_val=paths.concat(out_dir_final,'loss_final_val.npy');
    
    local out_file_intermediate_pre = paths.concat(out_dir_intermediate,'model_all_');
    local out_file_loss_intermediate_pre = paths.concat(out_dir_intermediate,'loss_all_');

    local out_file_loss_plot=paths.concat(out_dir_intermediate,'loss_all.png');
    local out_file_log=paths.concat(out_dir_intermediate,'log.txt');
    local logger=torch.DiskFile(out_file_log,'w');

    logger:writeString(dump.tostring(params)..'\n');
    print (params);

    cutorch.setDevice(params.gpu);


    local num_ctrl_pts=params.num_ctrl_pts
    local size_out=params.size_out;

    local optimState       
    local optimMethod      

    optimMethod = optim.adam
    optimState={learningRate=params.learningRate,learningRateDecay=params.learningRateDecay ,beta1=params.beta1 ,beta2=params.beta2 ,epsilon=params.epsilon }


    logger:writeString(dump.tostring('loading network')..'\n');
    print ('loading network');

    net = torch.load(params.model);
    -- ds_net=setUpGTNets();
    tps=nn.TPSGridGeneratorBHWD(num_ctrl_pts,size_out,size_out);
    net:add(tps);
    
    logger:writeString(dump.tostring('done loading network')..'\n');
    print ('done loading network');
    print (net);
    

    logger:writeString(dump.tostring('making cuda')..'\n');
    print ('making cuda');
    net = net:cuda();
    tps=tps:cuda();
    net:training();

    logger:writeString(dump.tostring('done')..'\n');
    print ('done');

    logger:writeString(dump.tostring('loading params')..'\n');
    print ('loading params');
    parameters, gradParameters = net:getParameters()
    logger:writeString(dump.tostring('loading done')..'\n');
    print ('loading done');
    logger:writeString(dump.tostring(optimState)..'\n');
    print (optimState)



    local data_params={file_path_horse=params.horse_data_path,
                        file_path_human=params.human_data_path,
                        augmentation=params.augmentation,
                        humanImage=false,
                        limit=params.limit};

    td=data_horseHuman(data_params);
    td.params.input_size={size_out,size_out};
    td.batch_size=params.batchSize;

    if params.testAfter>0 then
        data_params.file_path_horse = params.val_horse_data_path;
        data_params.file_path_human = params.val_human_data_path;
        data_params.augmentation = false;
        vd = data_horseHuman(data_params);
        vd.params.input_size = {size_out,size_out};
        vd.batch_size = params.batchSize;        
    end
    
    local losses = {};
    local losses_iter = {};

    local val_losses = {};
    local val_losses_iter = {};

    
    for i=1,params.iterations do

        local _, minibatch_loss = optimMethod(fevalScore,parameters, optimState)
        if minibatch_loss[1]>1 then
            minibatch_loss[1]=0;
        end
        losses[#losses + 1] = minibatch_loss[1] -- append the new loss        
        losses_iter[#losses_iter +1] = i;

        if i%params.dispAfter==0 then
            local disp_str=string.format("lr: %6s, minibatches processed: %6s, loss = %6.6f", optimState.learningRate,i, losses[#losses])
            logger:writeString(dump.tostring(disp_str)..'\n');
            print (disp_str);

            local str_score=''..losses[#losses];
            
            if str_seg=='nan' or str_score=='nan' then
                logger:writeString(dump.tostring('QUITTING')..'\n');
                print('QUITTING');
                break;
            end

        end


        if i%params.testAfter==0 and params.testAfter>0 then 
            net:evaluate();
            vd:getTrainingData();
            vd.training_set_horse.data=vd.training_set_horse.data:cuda();
            vd.training_set_human.label=vd.training_set_human.label:cuda();
            vd.training_set_horse.label=vd.training_set_horse.label:cuda();

            local horse_labels,human_labels,batch_inputs,_,_=vd:getBatchPoints()
            local gt_output=tps:getGTOutput(human_labels,horse_labels);
            local batch_targets = gt_output:clone();
            local outputs=net:forward(batch_inputs);
            local loss = getLoss(outputs,batch_targets);
            if loss >1 then
                loss =0;
            end
            val_losses[#val_losses+1]=loss;
            val_losses_iter[#val_losses_iter+1]=i;

            net:training();
            disp_str=string.format("minibatches processed: %6s, val loss = %6.6f", i, val_losses[#val_losses])
            logger:writeString(dump.tostring(disp_str)..'\n');
            print(disp_str)
        end

        -- check if model needs to be saved. save it.
        -- also save losses
        if i%params.saveAfter==0 then
            local out_file_intermediate=out_file_intermediate_pre..i..'.dat';
            torch.save(out_file_intermediate,net);
            local out_file_loss_intermediate=out_file_loss_intermediate_pre..i..'.npy';
            npy4th.savenpy(out_file_loss_intermediate, torch.Tensor(losses))
            
            if params.testAfter>0 then 
                local out_file_loss_intermediate=out_file_loss_intermediate_pre..i..'_val.npy';
                npy4th.savenpy(out_file_loss_intermediate, torch.Tensor(val_losses))
            end
        end

        if i%params.dispPlotAfter==0 then
            visualize:plotLossFigure(losses,losses_iter,val_losses,val_losses_iter,out_file_loss_plot);
        end

    end

    -- save final model
    torch.save(out_file_net,net);
    npy4th.savenpy(out_file_loss, torch.Tensor(losses))
    
    if params.testAfter>0 and #val_losses>0 then
        npy4th.savenpy(out_file_loss_val, torch.Tensor(val_losses))
    end
    visualize:plotLossFigure(losses,losses_iter,val_losses,val_losses_iter,out_file_loss_plot);

end



cmd = torch.CmdLine()
cmd:text()
cmd:text('Train Full network')
cmd:text()
cmd:text('Options')

local epoch_size=273;


cmd:option('-outDir','/home/SSD3/maheen-data/horse_project/tps_bn_clean_1e-3','directory to write output');
cmd:option('-num_ctrl_pts',36,'num of training data to read');
cmd:option('-size_out',224,'num of training data to read');

cmd:option('-model','/home/SSD3/maheen-data/horse_project/models/conv5_2fc_bn_normalXavier.dat','model to load')

cmd:option('-limit',-1,'num of training data to read');
cmd:option('-iterations',20*epoch_size,'num of iterations to run');
cmd:option('-saveAfter',2*epoch_size,'num of iterations after which to save model');
cmd:option('-batchSize',64,'batch size');
cmd:option('-testAfter',30,'num iterations after which to get validation loss');
cmd:option('-dispAfter',1,'num iterations after which to display training loss');
cmd:option('-dispPlotAfter',30,'num iterations after which to display training loss');

cmd:option('-val_horse_data_path','/home/SSD3/maheen-data/horse_project/data_check/horse/matches_5_val_allKP_minLoss_clean.txt')
cmd:option('-val_human_data_path','/home/SSD3/maheen-data/horse_project/data_check/aflw/matches_5_val_allKP_minLoss_noIm_clean.txt')
cmd:option('-horse_data_path','/home/SSD3/maheen-data/horse_project/data_check/horse/matches_5_train_allKP_minLoss_clean_full.txt')
cmd:option('-human_data_path','/home/SSD3/maheen-data/horse_project/data_check/aflw/matches_5_train_allKP_minLoss_noIm_clean_full.txt')

cmd:option('learningRate', 1e-3)
cmd:option('learningRateDecay',5e-6)
cmd:option('beta1', 0.9)
cmd:option('beta2', 0.999)
cmd:option('epsilon', 1e-8)
cmd:option('augmentation' , true);
cmd:option('-gpu',1,'gpu to run the training on');
cmd:option('-debug',false,'debug mode');

params = cmd:parse(arg)
main(params);
