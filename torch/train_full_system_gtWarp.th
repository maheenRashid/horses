require 'image'
npy4th = require 'npy4th'
require 'data_aflw';
require 'data_horseHuman_xiuye';
require 'cunn'
require 'cudnn'
require 'nn';
require 'optim'
require 'stn'
npy4th=require 'npy4th';
require 'torchx';
require 'gnuplot';
dump=require 'dump';
tps_helper=require 'tps_helper';
visualize=require 'visualize';
loss_helper=require 'loss_helper';

function getDownSampleGrid(batchSize)
    local out_grids_new=torch.zeros(40,40,2);
    for r=1,out_grids_new:size(1) do
        for c=1,out_grids_new:size(2) do
            out_grids_new[{r,c,1}]=-1+(1/20*r);
            out_grids_new[{r,c,2}]=-1+(1/20*c);
        end
    end
    out_grids_new=out_grids_new:view(1,out_grids_new:size(1),out_grids_new:size(2),out_grids_new:size(3));
    out_grids_new=torch.repeatTensor(out_grids_new,batchSize,1,1,1);
    return out_grids_new;
end


function setUpGTNets(full_size)
    local tranet=nn.Transpose({2,3},{3,4})
    local gt_net=nn.Sequential();
    local parnet=nn.ParallelTable();
    
    parnet:add(tranet);
    parnet:add(nn.Identity());
    gt_net:add(parnet);
    gt_net:add(nn.BilinearSamplerBHWD());

    local ds_net;
    if full_size then
        gt_net:add(nn.Transpose({3,4},{2,3}));
        ds_net=gt_net;
    else
        ds_net=nn.Sequential();
        local ds_paranet=nn.ParallelTable();
        
        ds_paranet:add(gt_net);
        ds_paranet:add(nn.Identity())
        ds_net:add(ds_paranet);
        ds_net:add(nn.BilinearSamplerBHWD());
        ds_net:add(nn.Transpose({3,4},{2,3}));
    end

    return ds_net
end

local fevalScore = function(x)
    if x ~= parameters then
	    parameters:copy(x)
    end

    local batch_inputs,batch_targets,human_labels,horse_labels = getInputsAndTarget(td);
 
    local loss,dloss,midoutputs=doTheForward(batch_inputs,batch_targets,human_labels,horse_labels,nil,params.full_size)
    net:backward(midoutputs, dloss);
    
    return loss, gradParameters;
end


function getInputsAndTarget(curr_d)
    curr_d:getTrainingData();
    curr_d.training_set_horse.data=curr_d.training_set_horse.data:cuda();
    curr_d.training_set_human.label=curr_d.training_set_human.label:cuda();
    curr_d.training_set_horse.label=curr_d.training_set_horse.label:cuda();
    local horse_labels,human_labels,batch_inputs,_,data_idx=curr_d:getBatchPoints(true)
    local batch_targets=torch.zeros(#horse_labels,curr_d.training_set_horse.label:size(2),
            curr_d.training_set_horse.label:size(3)):type(curr_d.training_set_horse.label:type());
    for idx_curr=1,#data_idx do
        batch_targets[idx_curr]=curr_d.training_set_horse.label[data_idx[idx_curr]];
    end
    return batch_inputs,batch_targets,human_labels,horse_labels;
end

function doTheForward(batch_inputs,batch_targets,human_labels,horse_labels,saveImage,full_size)
    if gradParameters then
        gradParameters:zero()
    end
    local gt_output=tps:getGTOutput(human_labels,horse_labels);
    local ds_grid=getDownSampleGrid(batch_inputs:size(1));
    local midoutputs;
    if full_size then
        midoutputs=ds_net:forward{batch_inputs,gt_output};
    else
        midoutputs=ds_net:forward{{batch_inputs,gt_output},ds_grid:cuda()};
    end
    

    local midoutputs_view;
    local batch_inputs_view;
    if saveImage then
        midoutputs_view=midoutputs:clone():double();
        batch_inputs_view=batch_inputs:clone():double();
    end

    midoutputs=tps_helper:switchMeans(midoutputs,td.params.mean,mean_im,std_im);

    local outputs=net:forward(midoutputs);

    local t_pts=tps_helper:getPointsOriginalImage(outputs,gt_output)

    if saveImage then
        local outputs_view=outputs:view(outputs:size(1),outputs:size(2)/2,2):clone();
        local t_pts_view=t_pts:view(t_pts:size(1),t_pts:size(2)/2,2):clone();
        local colors={{255,0,0},{255,255,0},{0,0,255},{0,255,0},{255,0,255}};

        t_pts_view=t_pts_view:transpose(2,3)
        visualize:saveBatchImagesWithKeypoints(batch_inputs_view,t_pts_view,{saveImage,'_org.jpg'},td.params.mean,{-1,1},colors);        
        visualize:saveBatchImagesWithKeypoints(midoutputs_view,outputs_view:transpose(2,3),{saveImage,'.jpg'},td.params.mean,{-1,1},colors);
    end

    local dloss = loss_helper:getLossD_RCNN(t_pts,batch_targets);
    local loss = loss_helper:getLoss_RCNN(t_pts,batch_targets);
    return loss,dloss,midoutputs;

end

function doTheForwardEuclidean(batch_inputs,batch_targets,human_labels,horse_labels,saveImage,full_size)
    if gradParameters then
        gradParameters:zero()
    end
    local gt_output=tps:getGTOutput(human_labels,horse_labels);
    local ds_grid=getDownSampleGrid(batch_inputs:size(1));
    local midoutputs;
    if full_size then
        midoutputs=ds_net:forward{batch_inputs,gt_output}
    else
        midoutputs=ds_net:forward{{batch_inputs,gt_output},ds_grid:cuda()};
    end
    

    local midoutputs_view;
    local batch_inputs_view;
    if saveImage then
        midoutputs_view=midoutputs:clone():double();
        batch_inputs_view=batch_inputs:clone():double();
    end

    midoutputs=tps_helper:switchMeans(midoutputs,td.params.mean,mean_im,std_im);

    local outputs=net:forward(midoutputs);

    local t_pts=tps_helper:getPointsOriginalImage(outputs,gt_output)

    if saveImage then
        local outputs_view=outputs:view(outputs:size(1),outputs:size(2)/2,2):clone();
        local t_pts_view=t_pts:view(t_pts:size(1),t_pts:size(2)/2,2):clone();
        local colors={{255,0,0},{255,255,0},{0,0,255},{0,255,0},{255,0,255}};
   
        t_pts_view=t_pts_view:transpose(2,3)
        
        for im_num=1,t_pts:size(1) do 
            local out_file_gt=saveImage..im_num..'_gt_pts.npy';
            local out_file_pred=saveImage..im_num..'_pred_pts.npy';

            local pred_output=t_pts[im_num]:clone():double();
            local gt_output=batch_targets[im_num]:clone():double();
            pred_output=pred_output:view(pred_output:size(1)/2,2);
            npy4th.savenpy(out_file_gt,gt_output);
            npy4th.savenpy(out_file_pred,pred_output);

        end

        local binary=batch_targets[{{},{},3}]:clone();
        visualize:saveBatchImagesWithKeypointsSensitive(batch_inputs_view,t_pts_view,{saveImage,'_org.jpg'},td.params.mean,{-1,1},colors,pointSize,binary);
        visualize:saveBatchImagesWithKeypointsSensitive(midoutputs_view,outputs_view:transpose(2,3),{saveImage,'.jpg'},td.params.mean,{-1,1},colors,pointSize,binary);

        visualize:saveBatchImagesWithKeypointsSensitive(batch_inputs_view,t_pts_view,{saveImage,'_org_nokp.jpg'},nil,{-1,1},colors,-1,binary);
        visualize:saveBatchImagesWithKeypointsSensitive(midoutputs_view,outputs_view:transpose(2,3),{saveImage,'_nokp.jpg'},nil,{-1,1},colors,-1,binary);

    end

    local loss,loss_all = loss_helper:getLoss_Euclidean(t_pts,batch_targets);
    return loss,loss_all,midoutputs;

end


function test(params) 

    local out_dir=params.outDir
    if params.limit<0 then
        params.limit=nil;
    end

    paths.mkdir(out_dir);
    local out_dir_images=paths.concat(out_dir,'test_images');
    paths.mkdir(out_dir_images);
    local out_file_loss_val=paths.concat(out_dir_images,'loss_final_val.npy');
    local out_file_loss_val_ind=paths.concat(out_dir_images,'loss_final_val_ind.npy');

    local out_file_log=paths.concat(out_dir_images,'log_test.txt');
    local logger=torch.DiskFile(out_file_log,'w');

    logger:writeString(dump.tostring(params)..'\n');
    print (params);

    cutorch.setDevice(params.gpu);


    local num_ctrl_pts=36;
    local size_out=224;

    logger:writeString(dump.tostring('loading network')..'\n');
    print ('loading network');

    net = torch.load(params.face_detection_model_path);
    ds_net=setUpGTNets(params.full_size);
    tps=nn.TPSGridGeneratorBHWD(num_ctrl_pts,size_out,size_out);
    
    logger:writeString(dump.tostring('done loading network')..'\n');
    print ('done loading network');
    
    print (net);
    print (ds_net)

    logger:writeString(dump.tostring('making cuda')..'\n');
    -- print ('making cuda');
    net = net:cuda();
    ds_net=ds_net:cuda();
    tps=tps:cuda();

    
    logger:writeString(dump.tostring('done')..'\n');
    print ('done');

    net:evaluate();

    local data_params={file_path_horse=params.val_horse_data_path,
                        file_path_human=params.val_human_data_path,
                        humanImage=false,
                        limit=params.limit,
                        augmentation=false};

    td = data_horseHuman(data_params);
    td.params.input_size = {size_out,size_out};
    td.batch_size = params.batchSize;        
    
    mean_im = image.load(params.mean_im_path)*255;
    std_im = image.load(params.std_im_path)*255;
    
    mean_im=mean_im:cuda();
    std_im=std_im:cuda();

    local val_losses = {};
    local val_losses_iter = {};
    local val_losses_ind={};

    for i=1,params.iterations do
            
            local batch_inputs,batch_targets,human_labels,horse_labels = getInputsAndTarget(td);
 
            local loss,loss_all = doTheForwardEuclidean(batch_inputs,batch_targets,human_labels,horse_labels,
                                        paths.concat(out_dir_images,i..'_'),params.full_size);
            
            for idx_ind=1,loss_all:size(1) do
                val_losses_ind[#val_losses_ind+1]=loss_all[idx_ind];
            end

            val_losses[#val_losses+1]=loss;
            val_losses_iter[#val_losses_iter+1]=i;

            
            disp_str=string.format("minibatches processed: %6s, val loss = %6.6f", i, val_losses[#val_losses])
            logger:writeString(dump.tostring(disp_str)..'\n');
            print(disp_str)

    end
    
    val_losses_ind=torch.Tensor(val_losses_ind);
    print (val_losses_ind:size())
    if val_losses_ind:size(1)>#td.lines_horse then
        val_losses_ind=val_losses_ind[{{1,#td.lines_horse}}];
    end
    print (val_losses_ind:size())

    disp_str=string.format("minibatches processed: all, val loss = %6.6f", torch.mean(val_losses_ind))
    logger:writeString(dump.tostring(disp_str)..'\n');
    print(disp_str)

    npy4th.savenpy(out_file_loss_val, torch.Tensor(val_losses))
    npy4th.savenpy(out_file_loss_val_ind, val_losses_ind)

end

function main(params) 

	local out_dir=params.outDir
    if params.limit<0 then
    	params.limit=nil;
    end



    paths.mkdir(out_dir);
    
    if params.debug then
        local out_dir_debug=paths.concat(out_dir,'debug');
        paths.mkdir(out_dir_debug);
    end


    local out_dir_intermediate=paths.concat(out_dir,'intermediate');
    local out_dir_final=paths.concat(out_dir,'final');
    paths.mkdir(out_dir_intermediate);
    paths.mkdir(out_dir_final);
    
    local out_file_net=paths.concat(out_dir_final,'model_all_final.dat');
    local out_file_loss=paths.concat(out_dir_final,'loss_final.npy');
    local out_file_loss_val=paths.concat(out_dir_final,'loss_final_val.npy');
    
    local out_file_intermediate_pre = paths.concat(out_dir_intermediate,'model_all_');
    local out_file_loss_intermediate_pre = paths.concat(out_dir_intermediate,'loss_all_');

    local out_file_loss_plot=paths.concat(out_dir_intermediate,'loss_all.png');
    local out_file_log=paths.concat(out_dir_intermediate,'log.txt');
    local logger=torch.DiskFile(out_file_log,'w');

    logger:writeString(dump.tostring(params)..'\n');
    print (params);

    cutorch.setDevice(params.gpu);


    local num_ctrl_pts=36;
    local size_out=224;

    local optimState       
    local optimMethod      

	optimMethod = optim.adam
	optimState={learningRate=params.learningRate,learningRateDecay=params.learningRateDecay ,beta1=params.beta1 ,beta2=params.beta2 ,epsilon=params.epsilon }


    logger:writeString(dump.tostring('loading network')..'\n');
    print ('loading network');

    net = torch.load(params.face_detection_model_path);
    ds_net=setUpGTNets(params.full_size);
    tps=nn.TPSGridGeneratorBHWD(num_ctrl_pts,size_out,size_out);
    
    logger:writeString(dump.tostring('done loading network')..'\n');
    print ('done loading network');
    print (net);
    print (ds_net)

    logger:writeString(dump.tostring('making cuda')..'\n');
    print ('making cuda');
    net = net:cuda();
    ds_net=ds_net:cuda();
    tps=tps:cuda();

    logger:writeString(dump.tostring('done')..'\n');
    print ('done');

    logger:writeString(dump.tostring('loading params')..'\n');
    print ('loading params');
    parameters, gradParameters = net:getParameters()
    logger:writeString(dump.tostring('loading done')..'\n');
    print ('loading done');
    logger:writeString(dump.tostring(optimState)..'\n');
    print (optimState)



    local data_params={file_path_horse=params.horse_data_path,
                        file_path_human=params.human_data_path,
                        augmentation=params.augmentation,
                        humanImage=false,
                        limit=params.limit};

    td=data_horseHuman(data_params);
    td.params.input_size={size_out,size_out};
    td.batch_size=params.batchSize;

    if params.testAfter>0 then
    	data_params.file_path_horse = params.val_horse_data_path;
        data_params.file_path_human = params.val_human_data_path;
    	data_params.augmentation = false;
    	vd = data_horseHuman(data_params);
        vd.params.input_size = {size_out,size_out};
        vd.batch_size = params.batchSize;        
	end
    
    mean_im = image.load(params.mean_im_path)*255;
    std_im = image.load(params.std_im_path)*255;
    
    mean_im=mean_im:cuda();
    std_im=std_im:cuda();


    local losses = {};
    local losses_iter = {};

    local val_losses = {};
    local val_losses_iter = {};

    local counter=0;
    for i=1,params.iterations do

        if params.decreaseAfter then
            if i%params.decreaseAfter==0 and counter==0 then
                counter=counter+1;
                params.learningRate=params.learningRate/10;
                optimState.learningRate=params.learningRate;
            end
        end

        local _, minibatch_loss = optimMethod(fevalScore,parameters, optimState)
        losses[#losses + 1] = minibatch_loss[1] -- append the new loss        
        losses_iter[#losses_iter +1] = i;

        if i%params.dispAfter==0 then
        	local disp_str=string.format("lr: %6s, minibatches processed: %6s, loss = %6.6f", optimState.learningRate,i, losses[#losses])
            logger:writeString(dump.tostring(disp_str)..'\n');
            print (disp_str);

            local str_score=''..losses[#losses];
            
            if str_seg=='nan' or str_score=='nan' then
                logger:writeString(dump.tostring('QUITTING')..'\n');
                print('QUITTING');
                break;
            end

        end


        if i%params.testAfter==0 and params.testAfter>0 then 
            net:evaluate();
            
            local batch_inputs,batch_targets,human_labels,horse_labels = getInputsAndTarget(vd);
 
            local loss = doTheForward(batch_inputs,batch_targets,human_labels,horse_labels,nil,params.full_size);

            val_losses[#val_losses+1]=loss;
            val_losses_iter[#val_losses_iter+1]=i;

            net:training();
            disp_str=string.format("minibatches processed: %6s, val loss = %6.6f", i, val_losses[#val_losses])
            logger:writeString(dump.tostring(disp_str)..'\n');
            print(disp_str)
        end

        -- check if model needs to be saved. save it.
        -- also save losses
        if i%params.saveAfter==0 then
            local out_file_intermediate=out_file_intermediate_pre..i..'.dat';
            torch.save(out_file_intermediate,net);
            local out_file_loss_intermediate=out_file_loss_intermediate_pre..i..'.npy';
            npy4th.savenpy(out_file_loss_intermediate, torch.Tensor(losses))
            
            if params.testAfter>0 then 
                local out_file_loss_intermediate=out_file_loss_intermediate_pre..i..'_val.npy';
                npy4th.savenpy(out_file_loss_intermediate, torch.Tensor(val_losses))
            end
        end

        if i%params.dispPlotAfter==0 then
        	visualize:plotLossFigure(losses,losses_iter,val_losses,val_losses_iter,out_file_loss_plot);
        end

	end

    -- save final model
    torch.save(out_file_net,net);
    npy4th.savenpy(out_file_loss, torch.Tensor(losses))
    
    if params.testAfter>0 and #val_losses>0 then
        npy4th.savenpy(out_file_loss_val, torch.Tensor(val_losses))
    end
    visualize:plotLossFigure(losses,losses_iter,val_losses,val_losses_iter,out_file_loss_plot);

end



cmd = torch.CmdLine()
cmd:text()
cmd:text('Train Full network')
cmd:text()
cmd:text('Options')

local epoch_size=56;

cmd:option('-face_detection_model_path','/home/SSD3/maheen-data/horse_project/vanilla_train_face_big/intermediate/model_all_16762.dat');
-- cmd:option('-face_detection_model_path','/home/SSD3/maheen-data/horse_project/ft_horse_allKp_gtWarp_halfBack_1e-3/final/model_all_final.dat');
-- cmd:option('-outDir','/home/SSD3/maheen-data/horse_project/gtWarp_fullSize_padded','directory to write output');
cmd:option('-outDir','/home/SSD3/maheen-data/horse_project/full_system_small_data/matches_5_3531_horse_minloss_gtwarp','directory to write output');

-- cmd:option('-mean_im_path','/home/SSD3/maheen-data/data_face_network/aflw_cvpr_40_mean.png');
-- cmd:option('-std_im_path','/home/SSD3/maheen-data/data_face_network/aflw_cvpr_40_std.png');

cmd:option('-full_size',true);
cmd:option('-mean_im_path','/home/SSD3/maheen-data/data_face_network/aflw_cvpr_224_mean.png');
cmd:option('-std_im_path','/home/SSD3/maheen-data/data_face_network/aflw_cvpr_224_std.png');

cmd:option('-limit',-1,'num of training data to read');
cmd:option('-iterations',150*epoch_size,'num of iterations to run');
cmd:option('-saveAfter',30*epoch_size,'num of iterations after which to save model');
cmd:option('-batchSize',64,'batch size');
cmd:option('-testAfter',30,'num iterations after which to get validation loss');
cmd:option('-dispAfter',1,'num iterations after which to display training loss');
cmd:option('-dispPlotAfter',30,'num iterations after which to display training loss');

cmd:option('-val_horse_data_path','/home/SSD3/maheen-data/horse_project/data_check/horse/matches_5_val_allKP_minLoss_clean_noDuplicates.txt')
cmd:option('-val_human_data_path','/home/SSD3/maheen-data/horse_project/data_check/aflw/matches_5_val_allKP_minLoss_noIm_clean_noDuplicates.txt')

cmd:option('-horse_data_path','/home/SSD3/maheen-data/horse_project/neighbor_data/small_datasets/matches_5_3531_horse_minloss.txt')
cmd:option('-human_data_path','/home/SSD3/maheen-data/horse_project/neighbor_data/small_datasets/matches_5_3531_face_noIm_minloss.txt')

cmd:option('learningRate', 1e-2)
cmd:option('learningRateDecay',5e-6)
cmd:option('beta1', 0.9)
cmd:option('beta2', 0.999)
cmd:option('epsilon', 1e-8)
cmd:option('augmentation' , true);
cmd:option('-gpu',1,'gpu to run the training on');
cmd:option('-debug',false,'debug mode');

cmd:option('decreaseAfter',50*epoch_size);
params = cmd:parse(arg)
-- main(params);

-- cmd:option('-face_detection_model_path',paths.concat(params.outDir,'intermediate/model_all_1680.dat'));
-- cmd:option('-outDir',paths.concat(params.outDir,'1680'),'directory to write output');

-- cmd:option('-face_detection_model_path',paths.concat(params.outDir,'intermediate/model_all_1064.dat'));
-- cmd:option('-outDir',paths.concat(params.outDir,'1064'),'directory to write output');

cmd:option('-face_detection_model_path','/media/ext_disk/maheen-data/vision1/full_system_small_data/matches_5_3531_horse_minloss_gtwarp/final/model_all_final.dat');
    -- paths.concat(params.outDir,'final/model_all_final.dat'));
cmd:option('-outDir','/home/SSD3/maheen-data/horse_project/cvpr_figs_noDuplicates/gt_warp/test_images','directory to write output');
-- cmd:option('-outDir',paths.concat(params.outDir,'test_images'),'directory to write output');

-- cmd:option('-val_horse_data_path',params.horse_data_path)
-- cmd:option('-val_human_data_path',params.human_data_path)
cmd:option('-iterations',2,'num of iterations to run');
cmd:option('-batchSize',100,'batch size');
params = cmd:parse(arg)
test(params);
