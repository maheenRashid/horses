require 'image'
npy4th = require 'npy4th'
require 'data_horseHuman_xiuye';
require 'cunn'
require 'cudnn'
require 'nn';
require 'optim'
require 'stn'
npy4th=require 'npy4th';
require 'torchx';


function getLossD(pred_output,gt_output)
	local lossD=pred_output-gt_output
	lossD=torch.mul(lossD,2);
	return lossD;
end

function getLoss(pred_output,gt_output)
	local loss=torch.pow(pred_output-gt_output,2);
	local loss_no_mean=loss:clone();
	loss=torch.mean(loss);
	return loss,loss_no_mean;
end

local fevalScore = function(x)
    if x ~= parameters then
	    parameters:copy(x)
    end
    
    td:getTrainingData();
    td.training_set_horse.data=td.training_set_horse.data:cuda();
	td.training_set_human.label=td.training_set_human.label:cuda();
	td.training_set_horse.label=td.training_set_horse.label:cuda();

    local horse_labels,human_labels,batch_inputs,_,data_idx=td:getBatchPoints(true)
    local gt_output=tps:getGTOutput(human_labels,horse_labels);
    -- local batch_inputs = td.training_set_horse.data:clone();
    local batch_targets = gt_output:clone();
    
    gradParameters:zero()
    -- print (batch_inputs:size());
 	local outputs=net:forward(batch_inputs);
    local dloss = getLossD(outputs,batch_targets);
    local loss,loss_no_mean = getLoss(outputs,batch_targets);


    net:backward(batch_inputs, dloss)

    if loss>100 then
    	for idx=1,loss_no_mean:size(1) do
    		print (torch.mean(loss_no_mean[idx]),td.training_set_horse.files[data_idx[idx]])
    	end
    end

    
    return loss, gradParameters;
end

-- function getPointsOriginalImage(outputs,out_grids,scaleFactor)
	
-- 	local for_loss_all=torch.zeros(outputs:size()):type(outputs:type());
-- 	local outputs=outputs:clone();
-- 	-- outputs=outputs:resize(outputs:size(1),outputs:size(2)/2,2):transpose(2,3);
-- 	outputs[{{},1,{}}]= (outputs[{{},1,{}}]+scaleFactor)*out_grids:size(2);
-- 	outputs[{{},2,{}}]= (outputs[{{},2,{}}]+scaleFactor)*out_grids:size(3);
-- 	outputs=torch.round(outputs);
-- 	for idx_im=1,outputs:size(1) do
-- 		for label_idx=1,outputs:size(3) do

-- 			local r_idx=outputs[idx_im][1][label_idx];
-- 			local c_idx=outputs[idx_im][2][label_idx];
			
-- 			if r_idx<1 then
-- 				r_idx=1;
-- 			end

-- 			if c_idx<1 then
-- 				c_idx=1;
-- 			end

-- 			if r_idx>out_grids:size(2) then
-- 				r_idx=out_grids:size(2);
-- 			end

-- 			if c_idx>out_grids:size(3) then
-- 				c_idx=out_grids:size(3);
-- 			end
		
-- 			local grid_value_r=out_grids[idx_im][r_idx][c_idx][1];
-- 			local grid_value_c=out_grids[idx_im][r_idx][c_idx][2];

-- 			local for_loss_r= grid_value_r*scaleFactor;
-- 			local for_loss_c= grid_value_c*scaleFactor;
-- 			for_loss_all[idx_im][(2*label_idx)-1]=for_loss_r;
-- 			for_loss_all[idx_im][2*label_idx]=for_loss_c;
-- 		end
-- 	end
-- 	return for_loss_all

-- end

function main(params) 

	local horse_path=params.horse_path;
	local human_path=params.human_path;
	local out_dir=params.outDir
    local net_file=params.model
    local num_ctrl_pts=params.num_ctrl_pts;
    local size_out=params.size_out;
    -- 36;
	-- max_iter=60;
	-- size_out=224;

    -- local pos_file= params.pos_file;
    -- local neg_file= params.neg_file;
    local val_horse_path;
    local val_human_path
    if params.testAfter>0 then
    	val_horse_path= params.val_horse_path
    	val_human_path= params.val_human_path
    end

    paths.mkdir(out_dir);
    local out_dir_intermediate=paths.concat(out_dir,'intermediate');
    local out_dir_final=paths.concat(out_dir,'final');
    paths.mkdir(out_dir_intermediate);
    paths.mkdir(out_dir_final);
    
    local out_file_net=out_dir_final..'/'..'model_all_final.dat';

    local out_file_loss=out_dir_final..'/'..'loss_final.npy';
    
    local out_file_intermediate_pre = out_dir_intermediate..'/'..'model_all_';
    local out_file_loss_intermediate_pre = out_dir_intermediate..'/'..'loss_all_';

    print (params);

    local opt = {}         
    opt.optimization = 'sgd'
    opt.batch_size=params.batchSize;
    opt.testAfter=params.testAfter;
    opt.iterations=params.iterations;
    opt.saveAfter=params.saveAfter;
    opt.dispAfter=params.dispAfter;
    cutorch.setDevice(params.gpu);

    local optimState       
    local optimMethod      

	optimMethod = optim.adam
	optimState={learningRate=params.learningRate,learningRateDecay=params.learningRateDecay ,beta1=params.beta1 ,beta2=params.beta2 ,epsilon=params.epsilon }

	-- optimMethod = optim.sgd
	-- optimState = {learningRate = params.learningRate, momentum = 0.9, weightDecay = 0}
	-- optimState={learningRate=params.learningRate,weigthDecay=params.learningRateDecay ,beta1=params.beta1 ,beta2=params.beta2 ,epsilon=params.epsilon }


    print ('loading network');

    net = torch.load(net_file);
    net:add(nn.TPSGridGeneratorBHWD(num_ctrl_pts,size_out,size_out));

    print ('done loading network');
    print (net);

    print ('making cuda');
    net = net:cuda();
    print ('done');

    print ('loading params');
    parameters, gradParameters = net:getParameters()
    print ('loading done');
    print (optimState)

    if params.limit<0 then
    	td=data({file_path_horse=horse_path,file_path_human=human_path,augmentation=params.augmentation,humanImage=params.humanImage});
    else
    	td=data({file_path_horse=horse_path,file_path_human=human_path,limit=params.limit,augmentation=params.augmentation,humanImage=params.humanImage});
    end
    td.params.input_size={size_out,size_out};
    td.batch_size=params.batchSize;
    
    if opt.testAfter>0 then
	    if params.limit<0 then
	    	vd=data({file_path_horse=val_horse_path,file_path_human=val_human_path,augmentation=false,humanImage=params.humanImage});
	    else
	    	vd=data({file_path_horse=val_horse_path,file_path_human=val_human_path,limit=params.limit,augmentation=false,humanImage=params.humanImage});
	    end
	    vd.params.input_size={size_out,size_out};
	    vd.batch_size=params.batchSize;
	end
    

    tps=nn.TPSGridGeneratorBHWD(num_ctrl_pts,size_out,size_out);
    tps=tps:cuda();
    
    -- vd=data({file_path_positive=val_pos_file,file_path_negative=val_neg_file});

    local counter = 0

    local losses = {};
    local val_losses = {};
    
    for i=1,opt.iterations do
    	-- if (i%375==0) then
    	-- 	optimState.learningRate=optimState.learningRate/10;
    	-- end

        local _, minibatch_loss = optimMethod(fevalScore,parameters, optimState)
        losses[#losses + 1] = minibatch_loss[1] -- append the new loss        


        if i%opt.dispAfter==0 then
            print(string.format("lr: %6s, minibatches processed: %6s, loss = %6.6f", optimState.learningRate,i, 
                losses[#losses]));

            local str_score=''..losses[#losses];
            
            if str_seg=='nan' or str_score=='nan' then
                print('QUITTING');
                break;
            end
        end


        if i%opt.testAfter==0 and opt.testAfter>0 then 
            net:evaluate();
            vd:getTrainingData();
		    vd.training_set_horse.data=vd.training_set_horse.data:cuda();
			vd.training_set_human.label=vd.training_set_human.label:cuda();
			vd.training_set_horse.label=vd.training_set_horse.label:cuda();

		    local horse_labels,human_labels,batch_inputs,_,data_idx=vd:getBatchPoints(true)
		    
		    local gt_output=tps:getGTOutput(human_labels,horse_labels);
		    local batch_targets = gt_output:clone();
		    local outputs=net:forward(batch_inputs);
		    local loss,loss_no_mean= getLoss(outputs,batch_targets);

            
            val_losses[#val_losses+1]=loss;
            -- val_losses_score[#val_losses_score+1]=score_loss_val;

            net:training();
            print(string.format("minibatches processed: %6s, val loss = %6.6f", i, val_losses[#val_losses]))
        end

        -- check if model needs to be saved. save it.
        -- also save losses
        if i%opt.saveAfter==0 then
            local out_file_intermediate=out_file_intermediate_pre..i..'.dat';
            torch.save(out_file_intermediate,net);
            local out_file_loss_intermediate=out_file_loss_intermediate_pre..i..'.npy';
            npy4th.savenpy(out_file_loss_intermediate, torch.Tensor(losses))
            
            if opt.testAfter>0 then 
                local out_file_loss_intermediate=out_file_loss_intermediate_pre..i..'_val.npy';
                npy4th.savenpy(out_file_loss_intermediate, torch.Tensor(val_losses))
            end
        end
	end

    -- save final model
    torch.save(out_file_net,net);
    npy4th.savenpy(out_file_loss, torch.Tensor(losses))

-- sha1:e99282b7d438:baa6fb60c347c8647277116d2efd6cdeedfbb012



end




cmd = torch.CmdLine()
cmd:text()
cmd:text('Train TPS network')
cmd:text()
cmd:text('Options')
cmd:option('-model','/home/SSD3/maheen-data/horse_project/models/conv5_2fc_closeToZero.dat','model to load')
cmd:option('-outDir','/home/SSD3/maheen-data/horse_project/tps_train_allKP_adam_dummy','directory to write output');
cmd:option('-num_ctrl_pts',36,'num of training data to read');
cmd:option('-limit',-1,'num of training data to read');
cmd:option('-size_out',224,'num of training data to read');
cmd:option('-iterations',8,'num of iterations to run');
-- cmd:option('-learningRate',0.00001,'starting learning rate to use');
cmd:option('-saveAfter',300,'num of iterations after which to save model');
cmd:option('-batchSize',64,'batch size');
cmd:option('-testAfter',30,'num iterations after which to get validation loss');
cmd:option('-dispAfter',1,'num iterations after which to display training loss');

-- cmd:option('-val_horse_path','/home/SSD3/maheen-data/horse_project/horse_resize/matches_5_val_fiveKP.txt')
-- cmd:option('-val_human_path','/home/SSD3/maheen-data/horse_project/aflw/matches_5_val_fiveKP_noIm.txt')

cmd:option('-val_horse_path','/home/SSD3/maheen-data/horse_project/data_resize/horse/matches_5_val_allKP.txt')
cmd:option('-val_human_path','/home/SSD3/maheen-data/horse_project/data_resize/aflw/matches_5_val_allKP_noIm.txt')

cmd:option('-horse_path','/home/SSD3/maheen-data/horse_project/data_resize/horse/matches_5_train_allKP.txt')
cmd:option('-human_path','/home/SSD3/maheen-data/horse_project/data_resize/aflw/matches_5_train_allKP_noIm.txt')

cmd:option('learningRate', 1e-5)
cmd:option('learningRateDecay',5e-6)
cmd:option('beta1', 0.9)
cmd:option('beta2', 0.999)
cmd:option('epsilon', 1e-8)
cmd:option('humanImage' , false);
cmd:option('augmentation' , false);

-- cmd:option('-tps_model_path','/home/SSD3/maheen-data/horse_human_fiveKP_tps_adam/final/model_all_final.dat')
-- cmd:option('-horse_path','/home/SSD3/maheen-data/horse_project/horse/matches_5.txt','horse train data file');
-- cmd:option('-human_path','/home/SSD3/maheen-data/horse_project/aflw/matches_noIm_5.txt','human train data file');
-- cmd:option('-val_horse_path','/home/SSD3/maheen-data/horse_project/horse/matches_5.txt','horse train data file');
-- cmd:option('-val_human_path','/home/SSD3/maheen-data/horse_project/aflw/matches_noIm_5.txt','human train data file');
cmd:option('-gpu',1,'gpu to run the training on');
cmd:text()

params = cmd:parse(arg)
main(params);

