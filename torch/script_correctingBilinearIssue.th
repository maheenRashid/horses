require 'image'
npy4th = require 'npy4th'
require 'data_aflw';
require 'cunn'
require 'cudnn'
require 'nn';
require 'optim'
require 'stn'
npy4th=require 'npy4th';
require 'torchx';
require 'gnuplot';
dump=require 'dump';
tps_helper=require 'tps_helper';
visualize=require 'visualize';
loss_helper=require 'loss_helper';


function doTheForward(batch_inputs,batch_targets,saveImage)
    
    local batch_inputs_view;
    if saveImage then
        batch_inputs_view=batch_inputs:double():clone();
    end

    local inputs;
    if ds_grid then
        inputs={batch_inputs,ds_grid:clone()}
    else
        inputs=batch_inputs;    
    end
    

	local rotation_ims={};
    for angle_idx=1,#td.params.angles do
        local rot = torch.ones(3,224,224);
        rot=image.rotate(rot,math.rad(td.params.angles[angle_idx]),"simple");
        image.save(saveImage..'check_'..angle_idx..'.png',rot);
    end

    -- local layer_curr=net:get(1):get(1):get(1);
    print (td.lines_horse)
    -- -- print (layer_curr);
    -- local net_curr=net:get(1):get(1):get(2);
    -- net_curr:insert(layer_curr,1);
    -- print (net_curr);

    -- print (net:get(1):get(1):get(1));
    local tps_output = net:get(1):get(1):get(2):forward(inputs);
    local bil_input = net:get(1):get(1):get(1):forward(inputs);
    -- print (tps_output:size())
    -- print (net:get(1):get(2));
    -- local tps_output=net:get(1):get(1).output[2];
    tps_output[tps_output:le(-1)]=-1;
    tps_output[tps_output:ge(1)]=1;

    -- print (net:get(1):get(2));
    local temp=net:get(1):get(2):forward{bil_input,tps_output};
    local midoutputs=net:get(1):get(3):forward(temp);

    local midoutputs_view;
    if saveImage then
        midoutputs_view=midoutputs:double():clone();
    end


    -- local bilinear_output=net:get(1):get(2).output;


    -- tps_output
    -- print (bilinear_output:size(),torch.min(bilinear_output),torch.max(bilinear_output));
    -- print (tps_output:size(),torch.min(tps_output),torch.max(tps_output));

    midoutputs=tps_helper:switchMeans(midoutputs,td.params.imagenet_mean,td.mean_im,td.std_im)
    -- print ('mean',torch.mean(midoutputs));

    local outputs=net:get(2):forward(midoutputs);
    
    local tps_layer;
    if ds_grid then
        tps_layer= net:get(1):get(1):get(1):get(1):get(2);
        tps_layer=tps_layer:get(#tps_layer);
    else
        tps_layer= net:get(1):get(1):get(2);
        tps_layer=tps_layer:get(#tps_layer);
    end

    local t_pts=tps_helper:getPointsOriginalImage(outputs,tps_layer.output)

    if saveImage then
        -- local t_pts_copy=t_pts:clone();
        local outputs_view=outputs:view(outputs:size(1),outputs:size(2)/2,2):clone();
        local t_pts_view=t_pts:view(t_pts:size(1),t_pts:size(2)/2,2):clone();
        local colors={{255,0,0},{255,255,0},{0,0,255},{0,255,0},{255,0,255}};
   
        -- visualize=Visualize();
        t_pts_view=t_pts_view:transpose(2,3)
        visualize:saveBatchImagesWithKeypoints(batch_inputs_view,t_pts_view,{saveImage,'_org.jpg'},td.params.imagenet_mean,{-1,1},colors);
        visualize:saveBatchImagesWithKeypoints(midoutputs_view,outputs_view:transpose(2,3),{saveImage,'.jpg'},td.params.imagenet_mean,{-1,1},colors);
    end

    local dloss = loss_helper:getLossD_RCNN(t_pts,batch_targets);
    local loss = loss_helper:getLoss_RCNN(t_pts,batch_targets);
    return loss,dloss,midoutputs,inputs;
end


function main(params) 

	local data_path=params.data_path;
	local out_dir=params.outDir
    local net_file=params.model
    if params.limit<0 then
    	params.limit=nil;
    end
    local val_data_path;
    local val_human_path

    full_back_prop=params.full_back_prop;
    print ('full_back_prop',full_back_prop);    
    if params.testAfter>0 then
    	val_data_path= params.val_data_path
    end
    

    paths.mkdir(out_dir);
    out_dir_debug=paths.concat(out_dir,'debug');
    print (out_dir_debug);
    paths.mkdir(out_dir_debug);
    local out_dir_intermediate=paths.concat(out_dir,'intermediate');
    local out_dir_final=paths.concat(out_dir,'final');
    paths.mkdir(out_dir_intermediate);
    paths.mkdir(out_dir_final);
    
    local out_file_net=paths.concat(out_dir_final,'model_all_final.dat');
    local out_file_loss=paths.concat(out_dir_final,'loss_final.npy');
    local out_file_loss_val=paths.concat(out_dir_final,'loss_final_val.npy');
    
    local out_file_intermediate_pre = paths.concat(out_dir_intermediate,'model_all_');
    local out_file_loss_intermediate_pre = paths.concat(out_dir_intermediate,'loss_all_');

    local out_file_loss_plot=paths.concat(out_dir_intermediate,'loss_all.png');
    local out_file_log=paths.concat(out_dir_intermediate,'log.txt');
    local logger=torch.DiskFile(out_file_log,'w');

	
    -- log = torch.DiskFile(out_file_log,'w');
    -- log:writeString(params);
    -- local str_curr;
    -- str_curr=
    logger:writeString(dump.tostring(params)..'\n');
    print (params);

    cutorch.setDevice(params.gpu);

    local optimState       
    local optimMethod      

	optimMethod = optim.adam
	optimState={learningRate=params.learningRate,learningRateDecay=params.learningRateDecay ,beta1=params.beta1 ,beta2=params.beta2 ,epsilon=params.epsilon }


    logger:writeString(dump.tostring('loading network')..'\n');
    print ('loading network');

    net=torch.load(params.face_detection_model_path);
    net=net:cuda();
    -- createIntegratedModel(params.tps_model_path,params.face_detection_model_path,params.full_size,params.full_model_flag);
    net:evaluate();
    
    -- if not params.full_back_prop then
    --     print ('making ev');
    --     net:get(1):evaluate();
    -- end

    if params.full_size then
        ds_grid=nil;
    else
        ds_grid= getDownSampleGrid(params.batchSize);
    end
    
    -- net = torch.load(net_file);
    logger:writeString(dump.tostring('done loading network')..'\n');
    print ('done loading network');
    -- logger:writeString(dump.tostring(net)..'\n');
    print (net);

    logger:writeString(dump.tostring('making cuda')..'\n');
    print ('making cuda');
    net = net:cuda();
    if ds_grid then
        ds_grid = ds_grid:cuda();
    end

    logger:writeString(dump.tostring('done')..'\n');
    print ('done');

    logger:writeString(dump.tostring('loading params')..'\n');
    print ('loading params');
    
    parameters, gradParameters = net:parameters()
    -- local optimStateTotal=getOptimStateTotal(params,{10,22},logger)

    logger:writeString(dump.tostring('loading done')..'\n');
    print ('loading done');
    logger:writeString(dump.tostring(optimState)..'\n');
    -- print (optimState)

    local data_params={file_path=data_path,
					batch_size=params.batchSize,
					mean_file=params.mean_im_path,
					std_file=params.std_im_path,
					augmentation=params.augmentation,
					limit=params.limit,
                    input_size={224,224},
                    imagenet_mean=true};

	td=data(data_params);

    if params.testAfter>0 then
    	data_params.file_path = params.val_data_path;
    	data_params.augmentation=false;
    	vd=data(data_params);
	end
	td.params.angles={-10,10};


    local losses = {};
    local losses_iter = {};

    local val_losses = {};
    local val_losses_iter = {};

    -- local visualize=Visualize();

    for i=1,params.iterations do

        -- if params.decreaseAfter then
        --     if i%params.decreaseAfter==0 then
        --         params.learningRate=params.learningRate/10;
        --         optimState.learningRate=params.learningRate;
        --         optimStateTotal=getOptimStateTotal(params,{10,22},logger);
        --     end
        -- end

        td:getTrainingData();

        td.training_set.data=td.training_set.data:cuda();
		td.training_set.label=td.training_set.label:cuda();

		local batch_inputs=td.training_set.data;
		local batch_targets=td.training_set.label;
	    
        local train_loss = doTheForward(batch_inputs,batch_targets,paths.concat(out_dir_debug,''..i..'_'));
        
        losses[#losses + 1] = train_loss;

        losses_iter[#losses_iter +1] = i;

        if i%params.dispAfter==0 then
        	local disp_str=string.format("lr: %6s, minibatches processed: %6s, loss = %6.6f", optimState.learningRate,i, losses[#losses])
            logger:writeString(dump.tostring(disp_str)..'\n');
            print (disp_str);

            local str_score=''..losses[#losses];
            
            if str_seg=='nan' or str_score=='nan' then
                logger:writeString(dump.tostring('QUITTING')..'\n');
                print('QUITTING');
                break;
            end
        end


        if i%params.testAfter==0 and params.testAfter>0 then 
            net:evaluate();
            vd:getTrainingData();

            vd.training_set.data=vd.training_set.data:cuda();
			vd.training_set.label=vd.training_set.label:cuda();

			local batch_inputs=vd.training_set.data;
			local batch_targets=vd.training_set.label;
		    
            local loss = doTheForward(batch_inputs,batch_targets)
            val_losses[#val_losses+1]=loss;
            val_losses_iter[#val_losses_iter+1]=i;

            net:training();
            disp_str=string.format("minibatches processed: %6s, val loss = %6.6f", i, val_losses[#val_losses])
            logger:writeString(dump.tostring(disp_str)..'\n');
            print(disp_str)
        end



        if i%params.dispPlotAfter==0 then
        	visualize:plotLossFigure(losses,losses_iter,val_losses,val_losses_iter,out_file_loss_plot);
        end

	end

   
    visualize:plotLossFigure(losses,losses_iter,val_losses,val_losses_iter,out_file_loss_plot);
end




cmd = torch.CmdLine()
cmd:text()
cmd:text('Train Full network')
cmd:text()
cmd:text('Options')

local epoch_size=56;

cmd:option('-outDir','/home/SSD3/maheen-data/horse_project/ft_horse_allKp_full_big_clean_fullBack_25_1e-2_100_100_evaluateFix/debug_bilinear','directory to write output');

cmd:option('-full_model_flag',true);
cmd:option('-face_detection_model_path','/home/SSD3/maheen-data/horse_project/ft_horse_allKp_full_big_clean_fullBack_25_1e-2_100_100_evaluateFix/final/model_all_final.dat');

cmd:option('-mean_im_path','/home/SSD3/maheen-data/data_face_network/aflw_cvpr_224_mean.png');
cmd:option('-std_im_path','/home/SSD3/maheen-data/data_face_network/aflw_cvpr_224_std.png');
cmd:option('-full_size',true,'whether to downscale between nets');

cmd:option('-limit',1,'num of training data to read');
cmd:option('-iterations',5,'num of iterations to run');
cmd:option('-saveAfter',20*epoch_size,'num of iterations after which to save model');
cmd:option('-batchSize',10,'batch size');
cmd:option('-testAfter',30,'num iterations after which to get validation loss');
cmd:option('-dispAfter',1,'num iterations after which to display training loss');
cmd:option('-dispPlotAfter',30,'num iterations after which to display training loss');
cmd:option('-decreaseAfter',50*epoch_size,'num of iterations to run');

cmd:option('-val_data_path','/home/SSD3/maheen-data/horse_project/data_check/horse/matches_5_val_allKP_minLoss_clean.txt')
cmd:option('-data_path','/home/SSD3/maheen-data/horse_project/data_check/horse/matches_5_train_allKP_minLoss_clean.txt')

cmd:option('learningRate', 1e-3)
cmd:option('multiplierBottom',1/100);
cmd:option('multiplierMid',1/100);

cmd:option('learningRateDecay',5e-6)
cmd:option('beta1', 0.9)
cmd:option('beta2', 0.999)
cmd:option('epsilon', 1e-8)
cmd:option('augmentation' , true);

cmd:option('-gpu',1,'gpu to run the training on');

cmd:option('-full_back_prop',true,'whether to back prop through spatial net as well');
params = cmd:parse(arg)
main(params);
