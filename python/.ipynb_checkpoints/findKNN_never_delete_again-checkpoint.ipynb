{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import json\n",
    "import os;\n",
    "\n",
    "N_lms = 5\n",
    "lms = ['no', 'le', 're', 'rm', 'lm']\n",
    "\n",
    "def calc_distance(p1, p2):\n",
    "\treturn math.sqrt((p1[0]-p2[0])**2 + (p1[1]-p2[1])**2)\n",
    "def calc_angle(l_c1, l_c2, l_12):\n",
    "\treturn (l_c1**2 + l_c2**2 - l_12**2) / (2*l_c1*l_c2)\n",
    "\n",
    "\n",
    "# !ATTENTION lms in .txt are already in right order   \n",
    "\n",
    "def get_pattern(exist_list):\n",
    "    pattern = ''\n",
    "    for i in xrange(len(lms)):\n",
    "        pattern += 'T' if exist_list[i] else 'F'\n",
    "    return pattern\n",
    "\n",
    "def generate_json(list_path, json_path):\n",
    "    ignore = 0\n",
    "    faces_per_img = {}\n",
    "    \n",
    "    with open(list_path) as fd:\n",
    "        contents = fd.readlines()\n",
    "        print 'contents len: ', len(contents)\n",
    "\n",
    "    parts = [line.split() for line in contents]\n",
    "    image_info = {}\n",
    "\n",
    "    exist_pattern = {}\n",
    "\n",
    "    for li in parts:\n",
    "        im = {}\n",
    "        im['path'] = li[0]\n",
    "        if im['path'] not in faces_per_img:\n",
    "            faces_per_img[im['path']] = 0\n",
    "        else:\n",
    "            faces_per_img[im['path']] += 1\n",
    "            \n",
    "        im_id = im['path']+'_'+str(faces_per_img[im['path']])\n",
    "\n",
    "        im['bbox'] = [int(ele) if '.' not in ele else float(ele) for ele in li[1:5]]\n",
    "        im['le'] = [float(ele) for ele in li[5:7]]\n",
    "        im['re'] = [float(ele) for ele in li[8:10]]\n",
    "        im['no'] = [float(ele) for ele in li[11:13]]\n",
    "        im['lm'] = [float(ele) for ele in li[14:16]]\n",
    "        im['rm'] = [float(ele) for ele in li[17:19]]\n",
    "\n",
    "        im['existence'] = []\n",
    "        im['existence'].append(int(float(li[13])) == 1)\n",
    "        im['existence'].append(int(float(li[7])) == 1)\n",
    "        im['existence'].append(int(float(li[10])) == 1)\n",
    "        im['existence'].append(int(float(li[19])) == 1)    \n",
    "        im['existence'].append(int(float(li[16])) == 1)\n",
    "        \n",
    "\n",
    "#         print im['pattern'], im['existence']\n",
    "        im['pattern'] = get_pattern(im['existence'])\n",
    "        if im['pattern'] not in exist_pattern:\n",
    "            exist_pattern[im['pattern']] = [im_id]\n",
    "        else:\n",
    "            exist_pattern[im['pattern']].append(im_id)\n",
    "\n",
    "        for i in xrange(N_lms):\n",
    "            for j in xrange(i+1, N_lms):\n",
    "                if im['existence'][i] and im['existence'][j]:\n",
    "                    im[lms[i]+lms[j]] = calc_distance(im[lms[i]], im[lms[j]])\n",
    "\n",
    "        if im['existence'][0] == False or (im['existence'][1] == False and im['existence'][2] == False):\n",
    "            ignore += 1\n",
    "            continue\n",
    "        else:\n",
    "            im['angles'] = {}\n",
    "            for i in xrange(1, N_lms):\n",
    "                j = i % 4 + 1\n",
    "\n",
    "                if im['existence'][i] and im['existence'][j]:\n",
    "                    i, j = sorted([i, j])\n",
    "                    im['angles'][lms[i]+'n'+lms[j]] = calc_angle(im['no'+lms[i]], im['no'+lms[j]], im[lms[i]+lms[j]])\n",
    "                else:\n",
    "                    pass\n",
    "                \n",
    "            if im['pattern'].startswith('TTT'):\n",
    "                im['mid_eye'] = (0.5 * (im['le'][0]+im['re'][0]), 0.5 * (im['le'][1]+im['re'][1]))\n",
    "                im['comp_angle'] = math.atan2(im['no'][1] - im['mid_eye'][1], im['no'][0] - im['mid_eye'][0])\n",
    "            elif im['pattern'].startswith('TTF') or im['pattern'].startswith('TFT'):\n",
    "                exist_eye = im['le'] if im['pattern'][1] == 'T' else im['re']\n",
    "                im['comp_angle'] = math.atan2(im['no'][1] - exist_eye[1], im['no'][0] - exist_eye[0])\n",
    "\n",
    "                \n",
    "\n",
    "        image_info[im_id] = im\n",
    "\n",
    "    image_info_file = open(json_path, \"w\")\n",
    "    json.dump(image_info, image_info_file)\n",
    "    image_info_file.close() \n",
    "    print 'ignore num: ', ignore\n",
    "    return exist_pattern    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findKNN(horse_image_info, human_image_info, horse_group, human_group, K, html_filename, list_filename, writeBbox=True):\n",
    "    \n",
    "    file_names = []\n",
    "    captions = []\n",
    "\n",
    "    horse_angle_lt_zero = set()\n",
    "    human_angle_lt_zero = set()\n",
    "    \n",
    "\n",
    "    for horse_id in horse_group:\n",
    "        horse_im = horse_image_info[horse_id]\n",
    "\n",
    "        file_names_row = [horse_im['path']]\n",
    "        captions_row = [os.path.split(horse_id)[-1]]\n",
    "        dist = []\n",
    "\n",
    "        for human_id in human_group:\n",
    "            human_im = human_image_info[human_id]\n",
    "            tmp_dist = math.fabs(human_im['comp_angle'] - horse_im['comp_angle'])\n",
    "            dist.append((human_id, tmp_dist))\n",
    "\n",
    "            if human_im['comp_angle'] < 0:\n",
    "                human_angle_lt_zero.add(human_id)\n",
    "            if horse_im['comp_angle'] < 0:\n",
    "                horse_angle_lt_zero.add(horse_id)\n",
    "\n",
    "\n",
    "        dist = sorted(dist, key=lambda d:d[1])[:K]\n",
    "\n",
    "        for item in dist:\n",
    "            tmp_human = human_image_info[item[0]]\n",
    "            tmp_img_name = os.path.split(tmp_human['path'])[-1]\n",
    "            bbox = tmp_human['bbox']\n",
    "#             img = cv2.imread(tmp_human['path'])\n",
    "#             cv2.rectangle(img, (int(bbox[0]), int(bbox[2])), (int(bbox[1]), int(bbox[3])), (0,0,255), thickness = (img.shape[0]/400+1))\n",
    "#             tmp_path = os.path.join(tmp_dir, tmp_img_name)\n",
    "#             cv2.imwrite(tmp_path, img)\n",
    "\n",
    "#             file_names_row.append(tmp_path)\n",
    "            file_names_row.append(tmp_human['path'])\n",
    "            if writeBbox:\n",
    "                file_names_row.extend([str(bbox[0]), str(bbox[1]), str(bbox[2]), str(bbox[3])])\n",
    "            captions_row.append(tmp_img_name)\n",
    "\n",
    "        file_names.append(file_names_row)\n",
    "        captions.append(captions_row)\n",
    "\n",
    "    # process path\n",
    "    if not writeBbox:\n",
    "        SIZE = 200\n",
    "        idxes = np.random.choice(range(len(file_names)), size=SIZE)\n",
    "        file_names_sample = [file_names[i] for i in range(len(file_names)) if i in idxes ]\n",
    "        captions_sample = [captions[i] for i in range(len(captions)) if i in idxes ]\n",
    "\n",
    "\n",
    "        for i in xrange(len(file_names_sample)):\n",
    "            for j in xrange(len(file_names_sample[i])):\n",
    "                file_names_sample[i][j] = file_names_sample[i][j].replace('/home/SSD3/laoreja-data', '..')\n",
    "                file_names_sample[i][j] = file_names_sample[i][j].replace('/home/laoreja/data', '..')\n",
    "\n",
    "    %cd ~/data/\n",
    "    from writeHTML import writeHTML\n",
    "    %cd ~/data/knn_res_new\n",
    "    if not writeBbox:\n",
    "        writeHTML(html_filename, file_names_sample, captions_sample, height=200, width=200) \n",
    "    \n",
    "    if writeBbox:\n",
    "        # write list\n",
    "        contents = []\n",
    "        for line in file_names:\n",
    "            line = ' '.join(line)\n",
    "            line += '\\n'\n",
    "            contents.append(line)\n",
    "            \n",
    "        with open(list_filename, 'w') as fd:\n",
    "            fd.writelines(contents)\n",
    "            \n",
    "    %cd ~/new-deep-landmark/\n",
    "    return file_names, horse_angle_lt_zero, human_angle_lt_zero\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_three_class(exist_groups):\n",
    "    TTT = []\n",
    "    TFT = []\n",
    "    TTF = []\n",
    "    for pattern, li in exist_groups.items():\n",
    "        if pattern.startswith('TTT'):\n",
    "            TTT.extend(li)\n",
    "        elif pattern.startswith('TFT'):\n",
    "            TFT.extend(li)\n",
    "        elif pattern.startswith('TTF'):\n",
    "            TTF.extend(li)\n",
    "    return TTT, TFT, TTF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/maheenrashid/new-deep-landmark\n",
      "contents len:  31529\n",
      "ignore num:  433\n",
      "31096\n",
      "contents len:  200\n",
      "ignore num:  3\n",
      "197\n",
      "/home/SSD3/laoreja-data\n",
      "/home/SSD3/laoreja-data/knn_res_new\n",
      "/home/maheenrashid/new-deep-landmark\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-d4c99af60163>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m filenames, _, _ = findKNN(horse_val_image_info, aflw_train_image_info, \n\u001b[1;32m---> 49\u001b[1;33m                           horse_groups, human_groups, 5, '/home/SSD3/maheen-data/temp/all_points_val.html', '/home/SSD3/maheen-data/temp/knn_all_points_val_list.txt', False)\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;31m# filenames, _, _ = findKNN(horse_val_image_info, aflw_train_image_info,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-7dd16c7482c0>\u001b[0m in \u001b[0;36mfindKNN\u001b[1;34m(horse_image_info, human_image_info, horse_group, human_group, K, html_filename, list_filename, writeBbox)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mwriteBbox\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[0mSIZE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0midxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mSIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[0mfile_names_sample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfile_names\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0midxes\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[0mcaptions_sample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcaptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcaptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0midxes\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: global name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "%cd ~/new-deep-landmark\n",
    "import numpy as np;\n",
    "aflw_train_exist = generate_json('/home/laoreja/finetune-deep-landmark/dataset/train/aflw_cvpr_trainImageList.txt',\n",
    "                                   'aflw_cvpr_train.json')\n",
    "# horse_train_exist = generate_json('/home/laoreja/finetune-deep-landmark/dataset/train/trainImageList_5_points.txt',\n",
    "#                                  'horse_5_points_train.json')\n",
    "with open('aflw_cvpr_train.json', 'r') as image_info_fd:\n",
    "    aflw_train_image_info = json.load(image_info_fd)\n",
    "# with open('horse_5_points_train.json', 'r') as image_info_fd:\n",
    "#     horse_train_image_info = json.load(image_info_fd)\n",
    "    \n",
    "hu_TTT, hu_TFT, hu_TTF = get_three_class(aflw_train_exist)\n",
    "# ho_TTT, ho_TFT, ho_TTF = get_three_class(horse_train_exist)\n",
    "    \n",
    "# horse_groups = ho_TTT\n",
    "# horse_groups.extend(ho_TFT)\n",
    "# horse_groups.extend(ho_TTF)\n",
    "# print len(horse_groups)\n",
    "\n",
    "human_groups = hu_TTT\n",
    "human_groups.extend(hu_TFT)\n",
    "human_groups.extend(hu_TTF)\n",
    "print len(human_groups)\n",
    "    \n",
    "# filenames, horse_angle_lt_zero, human_angle_lt_zero = findKNN(horse_train_image_info, aflw_train_image_info, \n",
    "#                                                               horse_groups, human_groups, 5, '5_points_train.html', 'knn_5_points_train_list.txt', True)\n",
    "# filenames, _, _ = findKNN(horse_train_image_info, aflw_train_image_info, \n",
    "#                           horse_groups, human_groups, 5, '5_points_train.html', 'knn_5_points_train_list.txt', False)\n",
    "\n",
    "\n",
    "# horse_val_exist = generate_json('/home/laoreja/finetune-deep-landmark/dataset/train/valImageList_5_points.txt',\n",
    "#                                'horse_5_points_val.json')\n",
    "\n",
    "out_file_json='horse_all_points_val_2.json'\n",
    "horse_val_exist=generate_json('/home/laoreja/new-deep-landmark/dataset/train/valImageList_2.txt',out_file_json)\n",
    "\n",
    "with open(out_file_json, 'r') as image_info_fd:\n",
    "    horse_val_image_info = json.load(image_info_fd)\n",
    "    \n",
    "ho_TTT, ho_TFT, ho_TTF = get_three_class(horse_val_exist)\n",
    "horse_groups = ho_TTT\n",
    "horse_groups.extend(ho_TFT)\n",
    "horse_groups.extend(ho_TTF)\n",
    "print len(horse_groups)\n",
    "\n",
    "filenames, horse_angle_lt_zero, human_angle_lt_zero = findKNN(horse_val_image_info, aflw_train_image_info, \n",
    "                          horse_groups, human_groups, 5, '/home/SSD3/maheen-data/temp/all_points_val.html', '/home/SSD3/maheen-data/temp/knn_all_points_val_list.txt', True)\n",
    "\n",
    "filenames, _, _ = findKNN(horse_val_image_info, aflw_train_image_info, \n",
    "                          horse_groups, human_groups, 5, '/home/SSD3/maheen-data/temp/all_points_val.html', '/home/SSD3/maheen-data/temp/knn_all_points_val_list.txt', False)\n",
    "\n",
    "# filenames, _, _ = findKNN(horse_val_image_info, aflw_train_image_info, \n",
    "#                           horse_groups, human_groups, 5, '5_points_val.html', 'knn_5_points_val_list.txt', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contents len:  21529\n",
      "ignore num:  433\n",
      "contents len:  4170\n",
      "ignore num:  49\n",
      "4121\n",
      "21096\n",
      "/home/SSD3/laoreja-data\n",
      "/home/SSD3/laoreja-data/knn_res_new\n",
      "/home/laoreja/new-deep-landmark\n",
      "/home/SSD3/laoreja-data\n",
      "/home/SSD3/laoreja-data/knn_res_new\n",
      "/home/laoreja/new-deep-landmark\n"
     ]
    }
   ],
   "source": [
    "\n",
    "aflw_train_exist = generate_json('/home/laoreja/new-deep-landmark/dataset/train/aflw_trainImageList.txt',\n",
    "                                    'aflw_train.json')\n",
    "horse_train_exist = generate_json('/home/laoreja/finetune-deep-landmark/dataset/train/horse_final_trainImageList.txt',\n",
    "                                     'final_horse_train.json')\n",
    "with open('aflw_train.json', 'r') as image_info_fd:\n",
    "    aflw_train_image_info = json.load(image_info_fd)\n",
    "with open('final_horse_train.json', 'r') as image_info_fd:\n",
    "    horse_train_image_info = json.load(image_info_fd)\n",
    "    \n",
    "hu_TTT, hu_TFT, hu_TTF = get_three_class(aflw_train_exist)\n",
    "ho_TTT, ho_TFT, ho_TTF = get_three_class(horse_train_exist)\n",
    "    \n",
    "horse_groups = ho_TTT\n",
    "horse_groups.extend(ho_TFT)\n",
    "horse_groups.extend(ho_TTF)\n",
    "print len(horse_groups)\n",
    "\n",
    "human_groups = hu_TTT\n",
    "human_groups.extend(hu_TFT)\n",
    "human_groups.extend(hu_TTF)\n",
    "print len(human_groups)\n",
    "    \n",
    "filenames, horse_angle_lt_zero, human_angle_lt_zero = findKNN(horse_train_image_info, aflw_train_image_info, horse_groups, human_groups, 5, 'final_train.html', 'knn_final_train_list.txt', True)\n",
    "filenames, _, _ = findKNN(horse_train_image_info, aflw_train_image_info, horse_groups, human_groups, 5, 'final_train.html', 'knn_final_train_list.txt', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/SSD3/laoreja-data/finetune-landmark-data/dataset/train\n"
     ]
    }
   ],
   "source": [
    "%pwd\n",
    "%cd ~/new-deep-landmark/dataset/train/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contents len:  31529\n",
      "ignore num:  433\n",
      "contents len:  3600\n",
      "ignore num:  41\n",
      "contents len:  3600\n",
      "ignore num:  41\n",
      "3559\n",
      "31096\n"
     ]
    }
   ],
   "source": [
    "# combine the aflw training dataset with the cvpr paper's training dataset\n",
    "aflw_cvpr_contents = []\n",
    "combined_list_path = '/home/laoreja/new-deep-landmark/dataset/train/aflw_cvpr_trainImageList.txt'\n",
    "combined_json = 'aflw_cvpr_train.json'\n",
    "\n",
    "with open('/home/laoreja/new-deep-landmark/dataset/train/aflw_trainImageList.txt','r') as fd:\n",
    "    aflw_cvpr_contents.extend(fd.readlines())\n",
    "with open('/home/laoreja/deep-landmark-master/dataset/train/trainImageList_horse_format.txt', 'r') as fd:\n",
    "    aflw_cvpr_contents.extend(fd.readlines())\n",
    "with open(combined_list_path, 'w') as fd:\n",
    "    fd.writelines(aflw_cvpr_contents)\n",
    "    \n",
    "aflw_cvpr_train_exist = generate_json(combined_list_path,\n",
    "                                    combined_json)\n",
    "horse_train_exist = generate_json('/home/laoreja/finetune-deep-landmark/dataset/train/trainImageList_2.txt',\n",
    "                                     'horse_train.json')\n",
    "\n",
    "with open(combined_json, 'r') as image_info_fd:\n",
    "    aflw_cvpr_train_image_info = json.load(image_info_fd)\n",
    "    \n",
    "horse_train_exist = generate_json('/home/laoreja/finetune-deep-landmark/dataset/train/trainImageList_2.txt',\n",
    "                                     'horse_train.json')    \n",
    "with open('horse_train.json', 'r') as image_info_fd:\n",
    "    horse_train_image_info = json.load(image_info_fd)\n",
    "    \n",
    "ho_TTT, ho_TFT, ho_TTF = get_three_class(horse_train_exist)\n",
    "    \n",
    "horse_groups = ho_TTT\n",
    "horse_groups.extend(ho_TFT)\n",
    "horse_groups.extend(ho_TTF)\n",
    "print len(horse_groups)    \n",
    "    \n",
    "hu_new_TTT, hu_new_TFT, hu_new_TTF = get_three_class(aflw_cvpr_train_exist)\n",
    "human_new_groups = hu_new_TTT\n",
    "human_new_groups.extend(hu_new_TFT)\n",
    "human_new_groups.extend(hu_new_TTF)\n",
    "print len(human_new_groups)\n",
    "\n",
    "# findKNN(horse_image_info, human_image_info, horse_group, human_group, K, html_filename, list_filename, writeBbox=True):\n",
    "\n",
    "new_filenames, new_horse_angle_lt_zero, new_human_angle_lt_zero = findKNN(horse_train_image_info, aflw_cvpr_train_image_info, horse_groups, human_new_groups, 5, 'aflw_cvpr_train.html', 'knn_aflw_cvpr_train_list.txt', True)\n",
    "new_filenames, new_horse_angle_lt_zero, new_human_angle_lt_zero = findKNN(horse_train_image_info, aflw_cvpr_train_image_info, horse_groups, human_new_groups, 5, 'aflw_cvpr_train.html', '', False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'human_image_info' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-723b5a3e9e5c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhuman_image_info\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m'/home/SSD3/laoreja-data/aflwd/aflw/data/flickr/3/image00035.jpg_0'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhuman_image_info\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'human_image_info' is not defined"
     ]
    }
   ],
   "source": [
    "print len(human_image_info)\n",
    "print '/home/SSD3/laoreja-data/aflwd/aflw/data/flickr/3/image00035.jpg_0' in human_image_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_three_class(exist_groups):\n",
    "    TTT = []\n",
    "    TFT = []\n",
    "    TTF = []\n",
    "    for pattern, li in exist_groups.items():\n",
    "        if pattern.startswith('TTT'):\n",
    "            TTT.extend(li)\n",
    "        elif pattern.startswith('TFT'):\n",
    "            TFT.extend(li)\n",
    "        elif pattern.startswith('TTF'):\n",
    "            TTF.extend(li)\n",
    "    return TTT, TFT, TTF\n",
    "\n",
    "hu_TTT, hu_TFT, hu_TTF = get_three_class(aflw_existence)\n",
    "ho_TTT, ho_TFT, ho_TTF = get_three_class(horse_existence)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18487\n"
     ]
    }
   ],
   "source": [
    "print len(horse_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "horse_whole_list_filename = 'horseImageList.txt'\n",
    "horse_img_lists = [\n",
    "'/home/laoreja/finetune-deep-landmark/dataset/train/valImageList_2.txt',\n",
    "'/home/laoreja/finetune-deep-landmark/dataset/train/testImageList_2.txt',\n",
    "'/home/laoreja/finetune-deep-landmark/dataset/train/trainImageList_2.txt',]\n",
    "\n",
    "contents = []\n",
    "for li in horse_img_lists:\n",
    "    with open(li, 'r') as fd:\n",
    "        contents.extend(fd.readlines())\n",
    "print len(contents)\n",
    "\n",
    "with open(horse_whole_list_filename, 'w') as fd:\n",
    "    fd.writelines(contents)\n",
    "\n",
    "# generate_json('/home/laoreja/data/aflwd/aflw/data/aflwImageList.txt', 'aflw_info.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "horse_whole_list_filename = 'horseImageList_5_points.txt'\n",
    "horse_img_lists = [\n",
    "'/home/laoreja/finetune-deep-landmark/dataset/train/valImageList_5_points.txt',\n",
    "'/home/laoreja/finetune-deep-landmark/dataset/train/trainImageList_5_points.txt',]\n",
    "\n",
    "contents = []\n",
    "for li in horse_img_lists:\n",
    "    with open(li, 'r') as fd:\n",
    "        contents.extend(fd.readlines())\n",
    "print len(contents)\n",
    "\n",
    "with open(horse_whole_list_filename, 'w') as fd:\n",
    "    fd.writelines(contents)\n",
    "\n",
    "# generate_json('/home/laoreja/data/aflwd/aflw/data/aflwImageList.txt', 'aflw_info.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "aflw_existence = generate_json('/home/laoreja/data/aflwd/aflw/data/aflwImageList.txt', 'aflw_info.json')\n",
    "horse_whole_list_filename = 'horseImageList.txt'\n",
    "horse_existence = generate_json(horse_whole_list_filename, 'horse_info.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('aflw_info.json', 'r') as image_info_fd:\n",
    "    human_image_info = json.load(image_info_fd)\n",
    "print 'aflw image info len:', len(human_image_info)\n",
    "\n",
    "for k, v in human_image_info.items():\n",
    "    print k\n",
    "    pprint(v)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('horse_info.json', 'r') as image_info_fd:\n",
    "    horse_image_info = json.load(image_info_fd)\n",
    "print 'horse image info len:', len(horse_image_info)\n",
    "\n",
    "for k, v in horse_image_info.items():\n",
    "    print k\n",
    "    pprint(v)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_mid_tan(horse_im, human_im):\n",
    "#     human_mid_eye = (0.5 * (human_im['le'][0]+human_im['re'][0]), 0.5 * (human_im['le'][1]+human_im['re'][1]))\n",
    "#     horse_mid_eye = (0.5 * (horse_im['le'][0]+horse_im['re'][0]), 0.5 * (horse_im['le'][1]+horse_im['re'][1]))    \n",
    "    \n",
    "#     human_angle = math.atan2(human_im['no'][1] - human_mid_eye[1], human_im['no'][0] - human_mid_eye[0])    \n",
    "#     horse_angle = math.atan2(horse_im['no'][1] - horse_mid_eye[1], horse_im['no'][0] - horse_mid_eye[0])\n",
    "    return math.fabs(human_angle - horse_angle), human_angle, horse_angle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for idx in horse_existence['TTTTT']:\n",
    "    if not horse_image_info[idx]['pattern'].startswith('TTT'):\n",
    "        print idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k, v in aflw_existence.items():\n",
    "    print k\n",
    "    print len(v)\n",
    "    if k.startswith('TTT'):\n",
    "        for vv in v:\n",
    "            print human_image_info[vv]['pattern']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for h_id in hu_TTT:\n",
    "    try:\n",
    "#         human_image_info[h_id]['pattern']\n",
    "        a = human_image_info[h_id]['me_no_angle']\n",
    "    except KeyError, e:\n",
    "        print h_id\n",
    "        print human_image_info[h_id]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "tmp_dir = '/home/laoreja/data/trash/rect/'\n",
    "if not os.path.exists(tmp_dir):\n",
    "    os.makedirs(tmp_dir)\n",
    "\n",
    "K = 5\n",
    "\n",
    "print 'no, le, re exist'\n",
    "\n",
    "file_names = []\n",
    "captions = []\n",
    "\n",
    "horse_angle_lt_zero = set()\n",
    "human_angle_lt_zero = set()\n",
    "\n",
    "for horse_id in ho_TTT:\n",
    "    horse_im = horse_image_info[horse_id]\n",
    "\n",
    "    file_names_row = [horse_im['path']]\n",
    "    captions_row = [os.path.split(horse_id)[-1]]\n",
    "    dist = []\n",
    "        \n",
    "    for human_id in hu_TTT:\n",
    "        human_im = human_image_info[human_id]\n",
    "        tmp_dist = math.fabs(human_im['me_no_angle'] - horse_im['me_no_angle'])\n",
    "        dist.append((human_id, tmp_dist))\n",
    "        \n",
    "        if human_im['me_no_angle'] < 0:\n",
    "            human_angle_lt_zero.add(human_id)\n",
    "        if horse_im['me_no_angle'] < 0:\n",
    "            horse_angle_lt_zero.add(horse_id)\n",
    "                \n",
    "        \n",
    "    dist = sorted(dist, key=lambda d:d[1])[:K]\n",
    "    for item in dist:\n",
    "\n",
    "        tmp_human = human_image_info[item[0]]\n",
    "        tmp_img_name = os.path.split(tmp_human['path'])[-1]\n",
    "        bbox = tmp_human['bbox']\n",
    "        img = cv2.imread(tmp_human['path'])\n",
    "        cv2.rectangle(img, (int(bbox[0]), int(bbox[2])), (int(bbox[1]), int(bbox[3])), (0,0,255), thickness = (img.shape[0]/400+1))\n",
    "        tmp_path = os.path.join(tmp_dir, tmp_img_name)\n",
    "        cv2.imwrite(tmp_path, img)\n",
    "        \n",
    "        file_names_row.append(tmp_path)\n",
    "        captions_row.append(tmp_img_name)\n",
    "    \n",
    "    file_names.append(file_names_row)\n",
    "    captions.append(captions_row)\n",
    "        \n",
    "# process path\n",
    "SIZE = 200\n",
    "idxes = np.random.choice(range(len(file_names)), size=SIZE)\n",
    "file_names = [file_names[i] for i in range(len(file_names)) if i in idxes ]\n",
    "captions = [captions[i] for i in range(len(captions)) if i in idxes ]\n",
    "\n",
    "print len(file_names)\n",
    "print len(captions)\n",
    "\n",
    "\n",
    "for i in xrange(len(file_names)):\n",
    "    for j in xrange(len(file_names[i])):\n",
    "        file_names[i][j] = file_names[i][j].replace('/home/SSD3/laoreja-data', '..')\n",
    "        file_names[i][j] = file_names[i][j].replace('/home/laoreja/data', '..')\n",
    "                    \n",
    "%cd ~/data\n",
    "from writeHTML import writeHTML\n",
    "writeHTML('knn_res_new/NO_LE_RE.html', file_names, captions, height=200, width=200)\n",
    "        \n",
    "\n",
    "print 'no, one eye exist'\n",
    "\n",
    "\n",
    "side_file_names = []\n",
    "side_captions = []\n",
    "\n",
    "# horse_angle_lt_zero = set()\n",
    "# human_angle_lt_zero = set()\n",
    "\n",
    "for horse_id in ho_TFT:\n",
    "    horse_im = horse_image_info[horse_id]\n",
    "\n",
    "    side_file_names_row = [horse_im['path']]\n",
    "    side_captions_row = [os.path.split(horse_id)[-1]]\n",
    "    dist = []\n",
    "        \n",
    "    for human_id in hu_TFT:\n",
    "        human_im = human_image_info[human_id]\n",
    "        tmp_dist = math.fabs(human_im['ee_no_angle'] - horse_im['ee_no_angle'])\n",
    "        dist.append((human_id, tmp_dist))\n",
    "        \n",
    "        if human_im['ee_no_angle'] < 0:\n",
    "            human_angle_lt_zero.add(human_id)\n",
    "        if horse_im['ee_no_angle'] < 0:\n",
    "            horse_angle_lt_zero.add(horse_id)\n",
    "                \n",
    "        \n",
    "    dist = sorted(dist, key=lambda d:d[1])[:K]\n",
    "    for item in dist:\n",
    "        side_file_names_row.append(human_image_info[item[0]]['path'])\n",
    "        side_captions_row.append(os.path.split(human_image_info[item[0]]['path'])[-1])\n",
    "        \n",
    "        \n",
    "for horse_id in ho_TTF:\n",
    "    horse_im = horse_image_info[horse_id]\n",
    "\n",
    "    side_file_names_row = [horse_im['path']]\n",
    "    side_captions_row = [os.path.split(horse_id)[-1]]\n",
    "    dist = []\n",
    "        \n",
    "    for human_id in hu_TTF:\n",
    "        human_im = human_image_info[human_id]\n",
    "        tmp_dist = math.fabs(human_im['ee_no_angle'] - horse_im['ee_no_angle'])\n",
    "        dist.append((human_id, tmp_dist))\n",
    "        \n",
    "        if human_im['ee_no_angle'] < 0:\n",
    "            human_angle_lt_zero.add(human_id)\n",
    "        if horse_im['ee_no_angle'] < 0:\n",
    "            horse_angle_lt_zero.add(horse_id)\n",
    "                \n",
    "        \n",
    "    dist = sorted(dist, key=lambda d:d[1])[:K]\n",
    "    for item in dist:\n",
    "        tmp_human = human_image_info[item[0]]\n",
    "        tmp_img_name = os.path.split(tmp_human['path'])[-1]\n",
    "        bbox = tmp_human['bbox']\n",
    "        img = cv2.imread(tmp_human['path'])\n",
    "        cv2.rectangle(img, (int(bbox[0]), int(bbox[2])), (int(bbox[1]), int(bbox[3])), (0,0,255), thickness = (img.shape[0]/400+1))\n",
    "        tmp_path = os.path.join(tmp_dir, tmp_img_name)\n",
    "        cv2.imwrite(tmp_path, img)\n",
    "        \n",
    "        side_file_names_row.append(tmp_path)\n",
    "        side_captions_row.append(tmp_img_name)\n",
    "\n",
    "        \n",
    "    side_file_names.append(side_file_names_row)\n",
    "    side_captions.append(side_captions_row)\n",
    "        \n",
    "# process path\n",
    "\n",
    "\n",
    "idxes = np.random.choice(range(len(side_file_names)), size=SIZE)\n",
    "side_file_names = [side_file_names[i] for i in range(len(side_file_names)) if i in idxes]\n",
    "side_captions = [side_captions[i] for i in range(len(side_captions)) if i in idxes ]\n",
    "\n",
    "\n",
    "print len(side_file_names)\n",
    "print len(side_captions)    \n",
    "\n",
    "for i in xrange(len(side_file_names)):\n",
    "    for j in xrange(len(side_file_names[i])):\n",
    "        side_file_names[i][j] = side_file_names[i][j].replace('/home/SSD3/laoreja-data', '..')\n",
    "        side_file_names[i][j] = side_file_names[i][j].replace('/home/laoreja/data', '..')\n",
    "\n",
    "%cd ~/data\n",
    "from writeHTML import writeHTML\n",
    "writeHTML('knn_res_new/NO_ONE_EYE.html', side_file_names, side_captions, height=200, width=200)        \n",
    "\n",
    "        \n",
    "print 'human angle less than zero: ', len(human_angle_lt_zero)\n",
    "pprint(human_angle_lt_zero)\n",
    "print 'horse angle less than zero: ', len(horse_angle_lt_zero)\n",
    "pprint(horse_angle_lt_zero)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(file_name)\n",
    "print file_name[0]\n",
    "print len(captions)\n",
    "print captions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
