{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "import util;\n",
    "import os;\n",
    "import visualize;\n",
    "import numpy as np;\n",
    "import time;\n",
    "\n",
    "def getNumNeighbors(file_horse):\n",
    "    lines=util.readLinesFromFile(file_horse);\n",
    "    lines=np.array(lines);\n",
    "    uni_lines=np.unique(lines);\n",
    "    counts=np.zeros(uni_lines.shape);\n",
    "    for idx_uni_curr,uni_curr in enumerate(uni_lines):\n",
    "        if idx_uni_curr%500==0:\n",
    "            print idx_uni_curr;\n",
    "        counts[idx_uni_curr]=np.sum(lines==uni_curr);\n",
    "    return uni_lines,counts;\n",
    "\n",
    "def writeSmallDatasetFile(out_file_pre,horse_data,num_neighbor,\n",
    "                          num_data,in_file_horse,in_file_face,in_file_face_noIm,post_tags=None):\n",
    "    if post_tags is None:\n",
    "        post_tags=['_horse.txt','_face.txt','_face_noIm.txt'];\n",
    "        \n",
    "    in_files=[in_file_horse,in_file_face,in_file_face_noIm];\n",
    "    \n",
    "    data_org=util.readLinesFromFile(in_file_horse);\n",
    "    data_org=np.array(data_org);\n",
    "    idx_keep_all=[];\n",
    "    print horse_data.shape\n",
    "    horse_data=horse_data[:num_data];\n",
    "    for horse_curr in horse_data:\n",
    "        idx_curr=np.where(data_org==horse_curr)[0];\n",
    "        idx_curr=np.sort(idx_curr)\n",
    "        idx_keep=idx_curr[:num_neighbor];\n",
    "        idx_keep_all=idx_keep_all+list(idx_keep);\n",
    "#         print num_data,idx_keep\n",
    "        \n",
    "    idx_keep_all=np.array(idx_keep_all);\n",
    "    print idx_keep_all.shape\n",
    "    files_to_return=[];\n",
    "    for idx_in_file,in_file in enumerate(in_files):\n",
    "        out_file_curr=out_file_pre+post_tags[idx_in_file];\n",
    "        if idx_in_file==0:\n",
    "            data_keep=data_org[idx_keep_all];\n",
    "        else:\n",
    "            data_curr=util.readLinesFromFile(in_file);\n",
    "            data_curr=np.array(data_curr);\n",
    "            data_keep=data_curr[idx_keep_all];\n",
    "        util.writeFile(out_file_curr,data_keep);\n",
    "        files_to_return.append(out_file_curr);\n",
    "    \n",
    "    return files_to_return;\n",
    "        \n",
    "def main():\n",
    "    dir_server='/home/SSD3/maheen-data';\n",
    "    click_str='http://vision1.idav.ucdavis.edu:1000';\n",
    "    \n",
    "    out_dir_meta_data='/home/SSD3/maheen-data/horse_project/data_check';\n",
    "    dir_neighbors='/home/SSD3/maheen-data/horse_project/neighbor_data';\n",
    "    matches_file=os.path.join(dir_neighbors,'horse_trainImageList_2_data_100_neigbors.txt');\n",
    "    \n",
    "    out_dir_debug=os.path.join(dir_neighbors,'debug');\n",
    "    out_dir_breakdowns=os.path.join(dir_neighbors,'small_datasets');\n",
    "    file_pre='matches';\n",
    "    util.mkdir(out_dir_breakdowns);\n",
    "    util.mkdir(out_dir_debug);\n",
    "    \n",
    "    out_file_counts=os.path.join(out_dir_debug,'counts.npz');\n",
    "    out_file_dist=os.path.join(out_dir_debug,'counts_dist.png');\n",
    "    \n",
    "    out_dir_meta_horse = os.path.join(out_dir_meta_data,'horse');\n",
    "    out_dir_meta_face = os.path.join(out_dir_meta_data,'aflw');\n",
    "    \n",
    "    num_neighbors=100;\n",
    "    out_file_face=os.path.join(out_dir_meta_face,'matches_'+str(num_neighbors)+'_train_allKP.txt');\n",
    "    out_file_face_noIm=os.path.join(out_dir_meta_face,'matches_'+str(num_neighbors)+'_train_allKP_noIm.txt');\n",
    "    out_file_horse=os.path.join(out_dir_meta_horse,'matches_'+str(num_neighbors)+'_train_allKP.txt');\n",
    "    \n",
    "    old_horse_file='/home/SSD3/maheen-data/horse_project/data_check/horse/matches_5_train_allKP_minLoss_clean_full.txt';\n",
    "    old_data=util.readLinesFromFile(old_horse_file);\n",
    "    old_data=np.array(old_data);\n",
    "    old_data=np.unique(old_data);\n",
    "\n",
    "#     new_data,counts=getNumNeighbors(out_file_horse);\n",
    "#     np.savez(out_file_counts,new_data=new_data,counts=counts);\n",
    "    \n",
    "    data=np.load(out_file_counts);\n",
    "    print data.files;\n",
    "    new_data=data['new_data'];\n",
    "    counts=data['counts'];\n",
    "    num_data=range(500,len(old_data),500);\n",
    "    num_data[-1]=len(old_data);\n",
    "    \n",
    "    num_neighbors=range(5,25,5);\n",
    "    np.random.shuffle(old_data);\n",
    "    for num_neighbor in num_neighbors:\n",
    "        for num_data_curr in num_data:\n",
    "            file_curr=file_pre+'_'+str(num_neighbor)+'_'+str(num_data_curr);\n",
    "            out_file_pre=os.path.join(out_dir_breakdowns,file_curr);\n",
    "            files=writeSmallDatasetFile(out_file_pre,old_data,num_neighbor,num_data_curr,out_file_horse,\\\n",
    "                                        out_file_face,out_file_face_noIm);\n",
    "            for file_curr in files:\n",
    "                print file_curr;\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "def writeMinLossFile(out_file_pre,post_tags,minloss_post,old_horse_file,old_human_file,old_human_file_noIm):\n",
    "    new_files=[out_file_pre+post_tag_curr for post_tag_curr in post_tags];\n",
    "    \n",
    "    old_data=util.readLinesFromFile(old_horse_file);\n",
    "    old_data=np.array(old_data);\n",
    "    \n",
    "    new_data=util.readLinesFromFile(new_files[0]);\n",
    "    new_data=np.array(new_data);\n",
    "    new_data_uni=np.unique(new_data);\n",
    "    bin_keep=np.in1d(old_data,new_data_uni);\n",
    "#     print bin_keep.shape,sum(bin_keep);\n",
    "    old_files=[old_horse_file,old_human_file,old_human_file_noIm];\n",
    "    new_files_write=[file_curr[:file_curr.rindex('.')]+minloss_post for file_curr in new_files];\n",
    "    for old_file_curr,new_file_curr,new_file_org in zip(old_files,new_files_write,new_files):\n",
    "        data_curr=util.readLinesFromFile(old_file_curr);\n",
    "        data_curr=np.array(data_curr);\n",
    "        data_keep=data_curr[bin_keep];\n",
    "        print old_file_curr,new_file_curr,len(data_keep);\n",
    "        print data_keep[0];\n",
    "        new_file_org_data=util.readLinesFromFile(new_file_org);\n",
    "        new_file_org_data=np.array(new_file_org_data);\n",
    "        bin_check=np.in1d(data_keep,new_file_org_data);\n",
    "        print sum(bin_check),data_keep.shape[0];\n",
    "        assert sum(bin_check)==data_keep.shape[0];\n",
    "#         util.writeFile(new_file_curr,data_keep);\n",
    "    \n",
    "def getCommandFaceTest(path_to_th,out_dir,file_curr,batch_size=100):\n",
    "    command=['th',path_to_th];\n",
    "    command=command+['-val_data_path',file_curr];\n",
    "    command=command+['-outDir',out_dir];\n",
    "    command=command+['-batchSize',str(batch_size)];\n",
    "    amount_data=len(util.readLinesFromFile(file_curr));\n",
    "    num_iterations=amount_data/batch_size;\n",
    "    if amount_data%batch_size!=0:\n",
    "        num_iterations=num_iterations+1;\n",
    "    command=command+['-iterations',str(num_iterations)];\n",
    "    command=' '.join(command);\n",
    "    return command;\n",
    "\n",
    "def getCommandFaceTrain(path_to_th,out_dir,file_curr):\n",
    "    command=['th',path_to_th];\n",
    "    command=command+['-data_path',file_curr];\n",
    "    command=command+['-outDir',out_dir];\n",
    "    command=' '.join(command);\n",
    "    return command;\n",
    "\n",
    "def getCommandFull2LossTrain(path_to_th,out_dir,horse_data_path,human_data_path,tps_model_path):\n",
    "    command=['th',path_to_th];\n",
    "    command=command+['-outDir',out_dir];\n",
    "    command=command+['-horse_data_path',horse_data_path];\n",
    "    command=command+['-human_data_path',human_data_path];\n",
    "    command=command+['-tps_model_path',tps_model_path];\n",
    "    command=' '.join(command);\n",
    "    return command;\n",
    "\n",
    "\n",
    "def writeMinLossFileLossData(out_file_pre,post_tags,minloss_post,loss_file):\n",
    "    new_files=[out_file_pre+post_tag_curr for post_tag_curr in post_tags];\n",
    "    horse_data=util.readLinesFromFile(new_files[0]);\n",
    "    horse_data=np.array(horse_data);\n",
    "    horse_data_uni=np.unique(horse_data);\n",
    "    face_data=util.readLinesFromFile(new_files[1]);\n",
    "    face_data_noIm=util.readLinesFromFile(new_files[2]);\n",
    "    assert len(face_data)==len(face_data_noIm);\n",
    "    \n",
    "    loss_all=np.load(loss_file);\n",
    "    loss_all=loss_all[:len(face_data)];\n",
    "    assert loss_all.shape[0]==len(face_data);\n",
    "    \n",
    "    new_data=[[],[],[]];\n",
    "    for idx_curr,horse_curr in enumerate(horse_data_uni):\n",
    "        idx_rel=np.where(horse_data==horse_curr)[0];\n",
    "        loss_rel=loss_all[idx_rel];\n",
    "        min_idx=np.argmin(loss_rel);\n",
    "        min_idx_big=idx_rel[min_idx];\n",
    "        assert loss_rel[min_idx]==loss_all[min_idx_big];\n",
    "        new_data[0].append(horse_curr);\n",
    "        new_data[1].append(face_data[min_idx_big]);\n",
    "        new_data[2].append(face_data_noIm[min_idx_big]);\n",
    "  \n",
    "    new_files_out=[new_file_curr[:new_file_curr.rindex('.')]+minloss_post for new_file_curr in new_files];\n",
    "    for new_file_to_write,data_to_write in zip(new_files_out,new_data):\n",
    "        print new_file_to_write,len(data_to_write);\n",
    "        util.writeFile(new_file_to_write,data_to_write);\n",
    "\n",
    "def getFilePres():\n",
    "    num_neighbors=range(5,10,5);\n",
    "    num_data=range(500,3500,500);\n",
    "    num_data=num_data+[3531]\n",
    "    post_tags='_horse_minloss';\n",
    "    file_pre='matches';\n",
    "    file_pres=[file_pre+'_'+str(num_neighbors[0])+'_'+str(num_data_curr)+post_tags for num_data_curr in num_data];\n",
    "    return file_pres,num_data\n",
    "\n",
    "def getLogFileLoss(log_file):\n",
    "    data_curr=util.readLinesFromFile(log_file);\n",
    "    data_curr=data_curr[-1];\n",
    "    data_curr=data_curr.split(' ')[-1].strip('\"');\n",
    "    data_curr=float(data_curr);\n",
    "    return data_curr;\n",
    "\n",
    "def getMinLoss(dir_meta,loss_dir_pre,loss_dir_posts,log_file):\n",
    "    loss_all=[];\n",
    "    for loss_dir_post in loss_dir_posts:\n",
    "        file_curr=os.path.join(dir_meta,loss_dir_pre+loss_dir_post,log_file);\n",
    "        if os.path.exists(file_curr):\n",
    "            loss=getLogFileLoss(file_curr);\n",
    "        else:\n",
    "            loss=float('inf');\n",
    "        loss_all.append(loss);\n",
    "        \n",
    "    loss_all=np.array(loss_all);\n",
    "    min_idx=np.argmin(loss_all);\n",
    "    min_loss=loss_all[min_idx];\n",
    "    return min_loss,loss_dir_posts[min_idx];\n",
    "\n",
    "\n",
    "dir_server='/home/SSD3/maheen-data';\n",
    "click_str='http://vision1.idav.ucdavis.edu:1000';\n",
    "print 'hello'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main_4():\n",
    "    num_neighbors=range(5,25,5);\n",
    "    num_data=range(500,3500,500);\n",
    "    num_data=num_data+[3531]\n",
    "    post_tags=['_horse.txt','_face.txt','_face_noIm.txt'];\n",
    "    \n",
    "    dir_neighbors='/home/SSD3/maheen-data/horse_project/neighbor_data';\n",
    "    out_dir_breakdowns=os.path.join(dir_neighbors,'small_datasets');\n",
    "    file_pre='matches';\n",
    "    minloss_post='_minloss.txt';\n",
    "    commands_all=[];\n",
    "    files_human=[];\n",
    "    \n",
    "    out_dir_min_loss=os.path.join(dir_neighbors,'min_loss_calculation');\n",
    "    \n",
    "    for num_neighbor in num_neighbors:\n",
    "        for num_data_curr in num_data:\n",
    "            file_curr=file_pre+'_'+str(num_neighbor)+'_'+str(num_data_curr);\n",
    "            out_file_pre=os.path.join(out_dir_breakdowns,file_curr); \n",
    "            loss_dir=os.path.join(out_dir_min_loss,out_file_pre+post_tags[1][:post_tags[1].rindex('.')]);\n",
    "            loss_file=os.path.join(loss_dir,'test_images','loss_final_val_ind.npy');\n",
    "            if os.path.exists(loss_file):\n",
    "                writeMinLossFileLossData(out_file_pre,post_tags,minloss_post,loss_file);\n",
    "            else:\n",
    "                pass;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tps_script():\n",
    "    num_neighbors=[5]\n",
    "#     range(5,25,5);\n",
    "    num_data=range(1000,3500,500);\n",
    "    num_data=num_data+[3531]\n",
    "    post_tags=['_horse.txt','_face.txt','_face_noIm.txt'];\n",
    "    dir_neighbors='/home/SSD3/maheen-data/horse_project/neighbor_data';\n",
    "    out_dir_exp='/home/SSD3/maheen-data/horse_project/tps_small_data_1e-3_dec_5_eye';\n",
    "    util.mkdir(out_dir_exp);\n",
    "    out_file_commands=os.path.join(out_dir_exp,'commands_all.sh');\n",
    "    out_dir_breakdowns=os.path.join(dir_neighbors,'small_datasets');\n",
    "    file_pre='matches';\n",
    "    path_to_doc='/home/maheenrashid/Downloads/horses/torch/train_tps_var_lr.th'\n",
    "\n",
    "    commands_all=[];\n",
    "    for num_data_curr in num_data:\n",
    "        for num_neighbor in num_neighbors:\n",
    "            file_curr=file_pre+'_'+str(num_neighbor)+'_'+str(num_data_curr);\n",
    "            out_file_pre=os.path.join(out_dir_breakdowns,file_curr);\n",
    "            horse_data_path=out_file_pre+post_tags[0];\n",
    "            human_data_path=out_file_pre+post_tags[2];\n",
    "            out_dir_curr=os.path.join(out_dir_exp,file_curr);\n",
    "            command_curr=['th',path_to_doc];\n",
    "            command_curr.extend(['-outDir',out_dir_curr]);\n",
    "            command_curr.extend(['-horse_data_path',horse_data_path]);\n",
    "            command_curr.extend(['-human_data_path',human_data_path]);\n",
    "            command_curr=' '.join(command_curr);\n",
    "            commands_all.append(command_curr);\n",
    "    commands_all=commands_all[::-1]\n",
    "    util.writeFile(out_file_commands,commands_all);\n",
    "    print len(commands_all);\n",
    "    print out_file_commands\n",
    "    \n",
    "tps_script();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main_3():\n",
    "    old_horse_file='/home/SSD3/maheen-data/horse_project/data_check/horse/matches_5_train_allKP_minLoss_clean.txt';\n",
    "    old_human_file='/home/SSD3/maheen-data/horse_project/data_check/aflw/matches_5_train_allKP_minLoss_clean.txt'\n",
    "    old_human_file_noIm='/home/SSD3/maheen-data/horse_project/data_check/aflw/matches_5_train_allKP_minLoss_noIm_clean.txt';\n",
    "    \n",
    "    num_neighbors=range(5,25,5);\n",
    "    num_data=range(500,3500,500);\n",
    "    num_data=num_data+[3531]\n",
    "    post_tags=['_horse.txt','_face.txt','_face_noIm.txt'];\n",
    "    \n",
    "    dir_neighbors='/home/SSD3/maheen-data/horse_project/neighbor_data';\n",
    "    out_dir_breakdowns=os.path.join(dir_neighbors,'small_datasets');\n",
    "    file_pre='matches';\n",
    "    minloss_post='_minloss.txt';\n",
    "    commands_all=[];\n",
    "    files_human=[];\n",
    "\n",
    "    for num_neighbor in num_neighbors:\n",
    "        for num_data_curr in num_data:\n",
    "            file_curr=file_pre+'_'+str(num_neighbor)+'_'+str(num_data_curr);\n",
    "            out_file_pre=os.path.join(out_dir_breakdowns,file_curr);\n",
    "            out_file_curr=out_file_pre+post_tags[1];\n",
    "            files_human.append(out_file_curr);\n",
    "    \n",
    "    path_to_th='/home/maheenrashid/Downloads/horses/torch/train_face_system.th';\n",
    "    out_dir_meta=os.path.join(dir_neighbors,'min_loss_calculation');\n",
    "    util.mkdir(out_dir_meta);\n",
    "    out_file_sh=os.path.join(out_dir_meta,'commands')\n",
    "    commands_all=[];\n",
    "    for file_curr in files_human:\n",
    "        out_dir=os.path.join(out_dir_meta,file_curr[:file_curr.rindex('.')]);\n",
    "        command_curr=getCommandFaceTest(path_to_th,out_dir,file_curr);\n",
    "        commands_all.append(command_curr);\n",
    "#         print command_curr;\n",
    "    num_files=2;\n",
    "    range_data=range(0,len(commands_all),len(commands_all)/num_files);\n",
    "    \n",
    "    if range_data[-1]!=len(commands_all):\n",
    "        range_data.append(len(commands_all));\n",
    "        \n",
    "    for num_file in range(num_files):\n",
    "        out_file_sh_curr=out_file_sh+'_'+str(num_file)+'.sh';\n",
    "        data_curr=commands_all[range_data[num_file]:range_data[num_file+1]];\n",
    "        \n",
    "        util.writeFile(out_file_sh_curr,data_curr);\n",
    "        print out_file_sh_curr;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bl_script():\n",
    "    num_neighbors=range(5,10,5);\n",
    "    num_data=range(500,3500,500);\n",
    "    num_data=num_data+[3531]\n",
    "    print num_data\n",
    "    post_tags=['_horse.txt','_face.txt','_face_noIm.txt'];\n",
    "    \n",
    "    dir_neighbors='/home/SSD3/maheen-data/horse_project/neighbor_data';\n",
    "    out_dir_breakdowns=os.path.join(dir_neighbors,'small_datasets');\n",
    "    file_pre='matches';\n",
    "    minloss_post='_minloss.txt';\n",
    "    num_files=2;\n",
    "    \n",
    "    out_file_pres=[];\n",
    "    for num_neighbor in num_neighbors:\n",
    "        for num_data_curr in num_data:\n",
    "            file_curr=file_pre+'_'+str(num_neighbor)+'_'+str(num_data_curr);\n",
    "            out_file_pre=os.path.join(out_dir_breakdowns,file_curr);\n",
    "            out_file_pres.append(out_file_pre);\n",
    "    \n",
    "    train_data_paths=[file_curr+post_tags[0][:post_tags[0].rindex('.')]+minloss_post for file_curr in out_file_pres];\n",
    "    out_dir_meta='/home/SSD3/maheen-data/horse_project/face_baselines_small_data';\n",
    "    util.mkdir(out_dir_meta);\n",
    "    \n",
    "    commands_all=[];\n",
    "    \n",
    "    path_to_th='/home/maheenrashid/Downloads/horses/torch/train_face_system.th';\n",
    "    out_file_sh=os.path.join(out_dir_meta,'commands')\n",
    "    \n",
    "    commands_all=[];\n",
    "    for file_curr in train_data_paths:\n",
    "        file_only=os.path.split(file_curr)[1];\n",
    "        out_dir=os.path.join(out_dir_meta,file_only[:file_only.rindex('.')]);\n",
    "#         print file_curr,len(util.readLinesFromFile(file_curr))\n",
    "        command_curr=getCommandFaceTrain(path_to_th,out_dir,file_curr);\n",
    "        print command_curr\n",
    "        commands_all.append(command_curr);\n",
    "\n",
    "    range_data=range(0,len(commands_all),len(commands_all)/num_files);\n",
    "\n",
    "    if range_data[-1]!=len(commands_all):\n",
    "        range_data[-1]=(len(commands_all));\n",
    "    print range_data;\n",
    "    for num_file in range(num_files):\n",
    "        out_file_sh_curr=out_file_sh+'_'+str(num_file)+'.sh';\n",
    "        data_curr=commands_all[range_data[num_file]:range_data[num_file+1]];\n",
    "        \n",
    "        util.writeFile(out_file_sh_curr,data_curr);\n",
    "        print out_file_sh_curr,len(data_curr);\n",
    "\n",
    "bl_script();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def full_system_script():\n",
    "    num_neighbors=range(5,10,5);\n",
    "#     num_data=range(500,3500,500);\n",
    "#     num_data=num_data+[3531]\n",
    "    num_data=[500,3531,1000,3000,1500,2500,2000];\n",
    "    print num_data\n",
    "    post_tags=['_horse.txt','_face.txt','_face_noIm.txt'];\n",
    "    \n",
    "    dir_neighbors='/home/SSD3/maheen-data/horse_project/neighbor_data';\n",
    "    out_dir_breakdowns=os.path.join(dir_neighbors,'small_datasets');\n",
    "    file_pre='matches';\n",
    "    minloss_post='_minloss.txt';\n",
    "    num_files=2;\n",
    "    \n",
    "    out_file_pres=[];\n",
    "    for num_neighbor in num_neighbors:\n",
    "        for num_data_curr in num_data:\n",
    "            file_curr=file_pre+'_'+str(num_neighbor)+'_'+str(num_data_curr);\n",
    "            out_file_pre=os.path.join(out_dir_breakdowns,file_curr);\n",
    "            out_file_pres.append(out_file_pre);\n",
    "    \n",
    "    horse_train_data_paths=[file_curr+post_tags[0][:post_tags[0].rindex('.')]+minloss_post\\\n",
    "                            for file_curr in out_file_pres];\n",
    "    human_train_data_paths=[file_curr+post_tags[2][:post_tags[2].rindex('.')]+minloss_post\\\n",
    "                            for file_curr in out_file_pres];\n",
    "    dirs_tps=[os.path.split(file_curr)[1] for file_curr in out_file_pres];\n",
    "    \n",
    "    out_dir_meta='/home/SSD3/maheen-data/horse_project/full_system_small_data_eye_1e-2_10_100';\n",
    "    out_dir_tps='/home/SSD3/maheen-data/horse_project/tps_small_data_1e-3_dec_5_eye';\n",
    "    util.mkdir(out_dir_meta);\n",
    "    \n",
    "    commands_all=[];\n",
    "    \n",
    "    path_to_th='/home/maheenrashid/Downloads/horses/torch/train_full_system_2loss.th';\n",
    "    out_file_sh=os.path.join(out_dir_meta,'commands_fixed')\n",
    "    \n",
    "    commands_all=[];\n",
    "    for dir_tps,file_curr,file_human in zip(dirs_tps,horse_train_data_paths,human_train_data_paths):\n",
    "        file_only=os.path.split(file_curr)[1];\n",
    "        out_dir=os.path.join(out_dir_meta,file_only[:file_only.rindex('.')]);\n",
    "#         print file_curr,len(util.readLinesFromFile(file_curr))\n",
    "#         tps_model_path=os.path.join(tps_dir,)\n",
    "        tps_model_path=os.path.join(out_dir_tps,dir_tps,'final/model_all_final.dat');\n",
    "#         print dir_tps\n",
    "#         print tps_model_path\n",
    "        command_curr=getCommandFull2LossTrain(path_to_th,out_dir,file_curr,file_human,tps_model_path);\n",
    "        \n",
    "        print os.path.exists(tps_model_path);\n",
    "        assert os.path.exists(file_curr);\n",
    "        assert os.path.exists(file_human);\n",
    "#         print command_curr\n",
    "        commands_all.append(command_curr);\n",
    "\n",
    "    range_data=range(0,len(commands_all),len(commands_all)/num_files);\n",
    "\n",
    "    if range_data[-1]!=len(commands_all):\n",
    "        range_data[-1]=len(commands_all);\n",
    "        \n",
    "    print range_data;\n",
    "    \n",
    "    for num_file in range(num_files):\n",
    "        out_file_sh_curr=out_file_sh+'_'+str(num_file)+'.sh';\n",
    "        data_curr=commands_all[range_data[num_file]:range_data[num_file+1]];\n",
    "        \n",
    "        util.writeFile(out_file_sh_curr,data_curr);\n",
    "        print out_file_sh_curr,len(data_curr);\n",
    "\n",
    "full_system_script();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def makeComparativeGraph():\n",
    "    num_neighbors=range(5,10,5);\n",
    "    num_data=range(500,3500,500);\n",
    "    num_data=num_data+[3531]\n",
    "#     print num_data\n",
    "    post_tags='_horse_minloss';\n",
    "    \n",
    "#     dir_neighbors='/home/SSD3/maheen-data/horse_project/neighbor_data';\n",
    "#     out_dir_breakdowns=os.path.join(dir_neighbors,'small_datasets');\n",
    "    file_pre='matches';\n",
    "#     minloss_post='_minloss.txt';\n",
    "#     num_files=2;\n",
    "    out_dir_meta=['/home/SSD3/maheen-data/horse_project/full_system_small_data',\\\n",
    "                  '/home/SSD3/maheen-data/horse_project/face_baselines_small_data'];\n",
    "    \n",
    "    out_file_pres=[];\n",
    "    for num_neighbor in num_neighbors:\n",
    "        for num_data_curr in num_data:\n",
    "            file_curr=file_pre+'_'+str(num_neighbor)+'_'+str(num_data_curr);\n",
    "#             out_file_pre=os.path.join(out_dir_breakdowns,file_curr);\n",
    "            out_file_pres.append(file_curr);\n",
    "    log_files=[os.path.join(file_curr+post_tags,'test_images','log_test.txt') for file_curr in out_file_pres];\n",
    "    losses_all=[];\n",
    "    for dir_curr in out_dir_meta:\n",
    "        losses=[];\n",
    "        for file_curr in log_files:\n",
    "            file_curr=os.path.join(dir_curr,file_curr);\n",
    "            lines=util.readLinesFromFile(file_curr);\n",
    "            line_last=lines[-1];\n",
    "            line_last=line_last.split(' ')[-1].strip('\"');\n",
    "            line_last=float(line_last);\n",
    "            losses.append(line_last);\n",
    "        losses_all.append(losses);\n",
    "    \n",
    "    out_file=os.path.join(out_dir_meta[0],'comparison.png');\n",
    "    \n",
    "    xAndYs=[(num_data,losses_all[0]),(num_data,losses_all[1])]\n",
    "    legend_entries=['Ours','Baseline']\n",
    "    visualize.plotSimple(xAndYs,out_file,title='',xlabel='Training Data',\\\n",
    "                         ylabel='Average Euclidean Distance',legend_entries=legend_entries);\n",
    "    print out_file.replace(dir_server,click_str)\n",
    "\n",
    "#         file_curr=\n",
    "#     print log_files;\n",
    "#     horse_train_data_paths=[file_curr+post_tags[0][:post_tags[0].rindex('.')]+minloss_post\\\n",
    "#                             for file_curr in out_file_pres];\n",
    "#     human_train_data_paths=[file_curr+post_tags[2][:post_tags[2].rindex('.')]+minloss_post\\\n",
    "#                             for file_curr in out_file_pres];\n",
    "    \n",
    "\n",
    "#     util.mkdir(out_dir_meta);\n",
    "    \n",
    "\n",
    "makeComparativeGraph();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def scripts_for_min():\n",
    "    increments=range(1680,10000,1680);\n",
    "    print increments;\n",
    "#     out_dir_meta='/home/SSD3/maheen-data/horse_project/full_system_small_data'\n",
    "#     path_to_th='/home/maheenrashid/Downloads/horses/torch/train_full_system_2loss.th';\n",
    "#     model_command='-full_model_path';\n",
    "\n",
    "    out_dir_meta='/home/SSD3/maheen-data/horse_project/face_baselines_small_data'\n",
    "    path_to_th='/home/maheenrashid/Downloads/horses/torch/train_face_system.th';\n",
    "    model_command='-model';\n",
    "\n",
    "    out_dir_command='-outDirTest';\n",
    "    out_file_sh=os.path.join(out_dir_meta,'command_test_mid.sh');\n",
    "    \n",
    "    file_pres,_=getFilePres();\n",
    "    \n",
    "    dirs=[os.path.join(out_dir_meta,dir_curr) for dir_curr in file_pres];\n",
    "    commands_all=[];\n",
    "    for dir_meta in dirs:\n",
    "        model_paths=[os.path.join('intermediate','model_all_'+str(num)+'.dat') for num in increments];\n",
    "\n",
    "        model_paths=[os.path.join(dir_meta,model_path_curr) for model_path_curr in model_paths];\n",
    "        out_dirs=[os.path.join(dir_meta,'test_images_'+str(num)) for num in increments]\n",
    "        for model_path,out_dir in zip(model_paths,out_dirs):\n",
    "            assert os.path.exists(model_path)\n",
    "            command_curr=['th',path_to_th,model_command,model_path,out_dir_command,out_dir]\n",
    "            command_curr=' '.join(command_curr);\n",
    "            commands_all.append(command_curr);\n",
    "            \n",
    "    print len(commands_all);\n",
    "    print commands_all[0]\n",
    "    print out_file_sh\n",
    "    util.writeFile(out_file_sh,commands_all);\n",
    "    \n",
    "    \n",
    "scripts_for_min();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def script_getMinLoss():\n",
    "    dir_metas=['/home/SSD3/maheen-data/horse_project/full_system_small_data',\n",
    "               '/home/SSD3/maheen-data/horse_project/face_baselines_small_data'];\n",
    "    file_pres,num_data=getFilePres();\n",
    "    loss_dir_pre='test_images_';\n",
    "    loss_dir_posts=[str(num_curr) for num_curr in range(1680,9000,1680)];\n",
    "    log_file='log_test.txt';\n",
    "#     num_data=\n",
    "    losses_all=[];\n",
    "    losses_all_end=[];\n",
    "    min_loss_iter_all=[];\n",
    "    for dir_meta in dir_metas:\n",
    "        loss_curr=[];\n",
    "        loss_end=[];\n",
    "        min_loss_iter=[];\n",
    "        for file_pre in file_pres:\n",
    "            min_loss,min_loss_post=getMinLoss(os.path.join(dir_meta,file_pre),loss_dir_pre,loss_dir_posts,log_file);\n",
    "#             print loss_dir_posts\n",
    "            loss_end_curr,min_loss_post_end=getMinLoss(os.path.join(dir_meta,file_pre),loss_dir_pre,['8400'],log_file);\n",
    "            loss_curr.append(min_loss);\n",
    "            loss_end.append(loss_end_curr);\n",
    "            min_loss_iter.append(min_loss_post);\n",
    "            print min_loss,min_loss_post,loss_end_curr,min_loss_post_end;\n",
    "\n",
    "        losses_all.append(loss_curr);\n",
    "        losses_all_end.append(loss_end)\n",
    "        min_loss_iter_all.append(min_loss_iter);\n",
    "        \n",
    "    out_file=os.path.join(dir_metas[0],'comparison_best.png');\n",
    "    print len(file_pres),len(losses_all[0]),len(losses_all[1]),len(num_data)\n",
    "    xAndYs=[(num_data,losses_all[0]),(num_data,losses_all[1]),\\\n",
    "           (num_data,losses_all_end[0]),(num_data,losses_all_end[1])];\n",
    "    legend_entries=['Ours Best','Baseline Best','Ours 8400','Baseline 8400'];\n",
    "    visualize.plotSimple(xAndYs,out_file,title='',xlabel='Training Data',\\\n",
    "                         ylabel='Average Euclidean Distance',legend_entries=legend_entries);\n",
    "    print out_file.replace(dir_server,click_str)\n",
    "    \n",
    "    \n",
    "print 'hello'\n",
    "script_getMinLoss();\n",
    "print 'hello post'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dir_curr='/home/SSD3/maheen-data/temp/checkDualRotation';\n",
    "visualize.writeHTMLForFolder(dir_curr);\n",
    "print os.path.join(dir_curr,os.path.split(dir_curr)[1]+'.html').replace(dir_server,click_str);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path_to_th='/home/maheenrashid/Downloads/horses/torch/train_full_system_2loss.th';\n",
    "tps_model_path='';\n",
    "out_dir_meta='/home/SSD3/maheen-data/horse_project/full_system_2loss_search';\n",
    "util.mkdir(out_dir_meta);\n",
    "out_file_sh=os.path.join(out_dir_meta,'commands');\n",
    "# outDir='';\n",
    "learningRate=1e-2;\n",
    "multiplier_pairs=[(1/100.0,1/100.0),(1/10.0,1/10.0),(1/10.0,1/100.0)];\n",
    "horse_data_path='/home/SSD3/maheen-data/horse_project/neighbor_data/small_datasets/matches_5_3531_horse_minloss.txt';\n",
    "human_data_path='/home/SSD3/maheen-data/horse_project/neighbor_data/small_datasets/matches_5_3531_face_noIm_minloss.txt';\n",
    "tps_model_path='/home/SSD3/maheen-data/horse_project/tps_small_data_1e-3_dec_5/matches_5_3531/final/model_all_final.dat';\n",
    "\n",
    "# th /home/maheenrashid/Downloads/horses/torch/train_full_system_2loss.th \n",
    "# -outDir /home/SSD3/maheen-data/horse_project/full_system_small_data/matches_5_3531_horse_minloss -horse_data_path  -human_data_path \n",
    "\n",
    "commands_all=[];\n",
    "for multiplierMid,multiplierBottom in multiplier_pairs:\n",
    "    out_dir_curr=os.path.join(out_dir_meta,'test_'+\\\n",
    "                              '_'.join([str(num) for num in [learningRate,multiplierMid,multiplierBottom]]));\n",
    "    command_curr=['th',path_to_th];\n",
    "    command_curr.extend(['-outDir',out_dir_curr]);\n",
    "    command_curr.extend(['learningRate',str(learningRate)]);\n",
    "    command_curr.extend(['multiplierBottom',str(multiplierBottom)]);\n",
    "    command_curr.extend(['multiplierMid',str(multiplierMid)]);\n",
    "    command_curr.extend(['-horse_data_path',horse_data_path]);\n",
    "    command_curr.extend(['-human_data_path',human_data_path]);\n",
    "    command_curr.extend(['-tps_model_path',tps_model_path]);\n",
    "    command_curr=' '.join(command_curr);\n",
    "    print command_curr;\n",
    "    commands_all.append(command_curr);\n",
    "    \n",
    "num_files=2;\n",
    "range_data=range(0,len(commands_all),len(commands_all)/num_files);\n",
    "\n",
    "if range_data[-1]!=len(commands_all):\n",
    "    range_data.append(len(commands_all));\n",
    "\n",
    "for num_file in range(num_files):\n",
    "    out_file_sh_curr=out_file_sh+'_'+str(num_file)+'.sh';\n",
    "    data_curr=commands_all[range_data[num_file]:range_data[num_file+1]];\n",
    "\n",
    "    util.writeFile(out_file_sh_curr,data_curr);\n",
    "    print out_file_sh_curr;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random;\n",
    "dir_meta='/home/SSD3/maheen-data/horse_project/sheep_data'\n",
    "in_file='sheep.txt';\n",
    "test_num=100;\n",
    "out_file_train=os.path.join(dir_meta,'trainImageList.txt');\n",
    "out_file_test=os.path.join(dir_meta,'testImageList.txt')\n",
    "lines=util.readLinesFromFile(os.path.join(dir_meta,in_file));\n",
    "train_num=len(lines)-test_num;\n",
    "random.shuffle(lines);\n",
    "train_split=lines[:train_num];\n",
    "test_split=lines[train_num:];\n",
    "util.writeFile(out_file_train,train_split);\n",
    "util.writeFile(out_file_test,test_split);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/SSD3/maheen-data/horse_project/data_check/sheep/matches_5_sheep_test_allKP.txt True\n",
      "/home/SSD3/maheen-data/horse_project/data_check/aflw/matches_5_sheep_test_allKP.txt True\n",
      "/home/SSD3/maheen-data/horse_project/data_check/aflw/matches_5_sheep_test_allKP_noIm.txt True\n",
      "/home/SSD3/maheen-data/horse_project/sheep_models/tps/test_face/loss_final_val_ind.npy True\n",
      "done\n",
      "/home/SSD3/maheen-data/horse_project/data_check/sheep/matches_5_sheep_test_allKP_minloss.txt 99\n",
      "/home/SSD3/maheen-data/horse_project/data_check/aflw/matches_5_sheep_test_allKP_minloss.txt 99\n",
      "/home/SSD3/maheen-data/horse_project/data_check/aflw/matches_5_sheep_test_allKP_noIm_minloss.txt 99\n",
      "/home/SSD3/maheen-data/horse_project/data_check/sheep/matches_5_sheep_train_allKP.txt True\n",
      "/home/SSD3/maheen-data/horse_project/data_check/aflw/matches_5_sheep_train_allKP.txt True\n",
      "/home/SSD3/maheen-data/horse_project/data_check/aflw/matches_5_sheep_train_allKP_noIm.txt True\n",
      "/home/SSD3/maheen-data/horse_project/sheep_models/tps/train_face/loss_final_val_ind.npy True\n",
      "done\n",
      "/home/SSD3/maheen-data/horse_project/data_check/sheep/matches_5_sheep_train_allKP_minloss.txt 432\n",
      "/home/SSD3/maheen-data/horse_project/data_check/aflw/matches_5_sheep_train_allKP_minloss.txt 432\n",
      "/home/SSD3/maheen-data/horse_project/data_check/aflw/matches_5_sheep_train_allKP_noIm_minloss.txt 432\n"
     ]
    }
   ],
   "source": [
    "def writeMinLossFilesSimple(new_files,minloss_post,loss_file):\n",
    "#     new_files=[out_file_pre+post_tag_curr for post_tag_curr in post_tags];\n",
    "    horse_data=util.readLinesFromFile(new_files[0]);\n",
    "    horse_data=np.array(horse_data);\n",
    "    horse_data_uni=np.unique(horse_data);\n",
    "    face_data=util.readLinesFromFile(new_files[1]);\n",
    "    face_data_noIm=util.readLinesFromFile(new_files[2]);\n",
    "    assert len(face_data)==len(face_data_noIm);\n",
    "    \n",
    "    loss_all=np.load(loss_file);\n",
    "    loss_all=loss_all[:len(face_data)];\n",
    "    assert loss_all.shape[0]==len(face_data);\n",
    "    \n",
    "    new_data=[[],[],[]];\n",
    "    for idx_curr,horse_curr in enumerate(horse_data_uni):\n",
    "        idx_rel=np.where(horse_data==horse_curr)[0];\n",
    "        loss_rel=loss_all[idx_rel];\n",
    "        min_idx=np.argmin(loss_rel);\n",
    "        min_idx_big=idx_rel[min_idx];\n",
    "        assert loss_rel[min_idx]==loss_all[min_idx_big];\n",
    "        new_data[0].append(horse_curr);\n",
    "        new_data[1].append(face_data[min_idx_big]);\n",
    "        new_data[2].append(face_data_noIm[min_idx_big]);\n",
    "  \n",
    "    new_files_out=[new_file_curr[:new_file_curr.rindex('.')]+minloss_post for new_file_curr in new_files];\n",
    "    for new_file_to_write,data_to_write in zip(new_files_out,new_data):\n",
    "        print new_file_to_write,len(data_to_write);\n",
    "        util.writeFile(new_file_to_write,data_to_write);\n",
    "\n",
    "\n",
    "def createSheepFiles():\n",
    "    dir_meta='/home/SSD3/maheen-data/horse_project/data_check'\n",
    "    dirs=[os.path.join(dir_meta,'sheep'),os.path.join(dir_meta,'aflw')];\n",
    "    files=['matches_5_sheep_test_allKP','matches_5_sheep_train_allKP']\n",
    "    post_tags=[['.txt'],['.txt','_noIm.txt']];\n",
    "    loss_dir_post=['test_face','train_face'];\n",
    "    loss_dir='/home/SSD3/maheen-data/horse_project/sheep_models/tps'\n",
    "    minloss_post='_minloss.txt'\n",
    "    for idx_file_curr,file_curr in enumerate(files):\n",
    "        files_in=[];\n",
    "        for idx_dir_curr,dir_curr in enumerate(dirs):\n",
    "            for post_tag in post_tags[idx_dir_curr]:\n",
    "                files_in.append(os.path.join(dir_curr,file_curr+post_tag));\n",
    "\n",
    "        for file_curr in files_in:\n",
    "            print file_curr,os.path.exists(file_curr);\n",
    "\n",
    "        loss_dir_curr=os.path.join(loss_dir,loss_dir_post[idx_file_curr]);\n",
    "        loss_file=os.path.join(loss_dir_curr,'loss_final_val_ind.npy');\n",
    "        print loss_file,os.path.exists(loss_file)\n",
    "        print 'done'\n",
    "        writeMinLossFilesSimple(files_in,minloss_post,loss_file)\n",
    "\n",
    "        createSheepFiles();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th /home/maheenrashid/Downloads/horses/torch/train_full_system_2loss.th -outDir /home/SSD3/maheen-data/horse_project/sheep_models/full_system_2loss -tps_model_path /home/SSD3/maheen-data/horse_project/sheep_models/tps/final/model_all_final.dat -val_data_path /home/SSD3/maheen-data/horse_project/data_check/sheep/matches_5_sheep_test_allKP_minloss.txt -human_data_path /home/SSD3/maheen-data/horse_project/data_check/aflw/matches_5_sheep_train_allKP_noIm_minloss.txt -horse_data_path /home/SSD3/maheen-data/horse_project/data_check/sheep/matches_5_sheep_train_allKP_minloss.txt\n"
     ]
    }
   ],
   "source": [
    "out_dir_meta='/home/SSD3/maheen-data/horse_project/sheep_models'\n",
    "val_data_path=['-val_data_path',\\\n",
    "               '/home/SSD3/maheen-data/horse_project/data_check/sheep/matches_5_sheep_test_allKP_minloss.txt'];\n",
    "horse_data_path=['-horse_data_path',\\\n",
    "               '/home/SSD3/maheen-data/horse_project/data_check/sheep/matches_5_sheep_train_allKP_minloss.txt'];\n",
    "human_data_path=['-human_data_path',\\\n",
    "               '/home/SSD3/maheen-data/horse_project/data_check/aflw/matches_5_sheep_train_allKP_noIm_minloss.txt'];\n",
    "tps_model_path=['-tps_model_path',os.path.join(out_dir_meta,'tps','final','model_all_final.dat')];\n",
    "out_dir=['-outDir',os.path.join(out_dir_meta,'full_system_2loss')];\n",
    "path_to_th='/home/maheenrashid/Downloads/horses/torch/train_full_system_2loss.th';\n",
    "th=['th',path_to_th];\n",
    "command=th+out_dir+tps_model_path+val_data_path+human_data_path+horse_data_path;\n",
    "command=' '.join(command);\n",
    "print command;\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
