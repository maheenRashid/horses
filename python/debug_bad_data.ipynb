{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import util;\n",
    "import visualize;\n",
    "import os;\n",
    "\n",
    "\n",
    "def parseElaborateLogFile(error_log_file):\n",
    "    error_data=util.readLinesFromFile(error_log_file);\n",
    "    loss_string='loss ='\n",
    "    loss_lines=[(idx,line_curr) for idx,line_curr in enumerate(error_data) if loss_string in line_curr];\n",
    "    loss_tuple_info=[];\n",
    "    for idx in range(len(loss_lines)-1):\n",
    "        start_idx=loss_lines[idx][0];\n",
    "        end_idx=loss_lines[idx+1][0];\n",
    "        data_lines=error_data[start_idx+1:end_idx];\n",
    "        data_lines=[line_curr.strip() for line_curr in data_lines]\n",
    "        loss_curr=loss_lines[idx+1][1];\n",
    "        loss_curr=loss_curr[loss_curr.index(loss_string):];\n",
    "        loss_curr=float(loss_curr[len(loss_string):]);\n",
    "        loss_tuple_info.append((start_idx,end_idx,loss_curr,data_lines));\n",
    "    \n",
    "    return loss_tuple_info;\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found!\n",
      "0.326455\n",
      "found!\n",
      "0.327254\n",
      "found!\n",
      "0.115577\n",
      "found!\n",
      "0.713713\n"
     ]
    }
   ],
   "source": [
    "error_log_file='/home/SSD3/maheen-data/horse_project/tps_train_allKP_adam_dummy/log_elaborate.txt'\n",
    "to_search=\\\n",
    "['/home/SSD3/maheen-data/horse_project/data_check/horse/im/horses_pascal_selected/2009_004662.jpg /home/SSD3/maheen-data/horse_project/data_check/horse/npy/horses_pascal_selected/2009_004662.npy',\n",
    "'/home/SSD3/maheen-data/horse_project/data_check/horse/im/imagenet_n02374451/n02374451_11539.jpg /home/SSD3/maheen-data/horse_project/data_check/horse/npy/imagenet_n02374451/n02374451_11539.npy',\n",
    "'/home/SSD3/maheen-data/horse_project/data_check/horse/im/imagenet_n02374451/n02374451_16786.jpg /home/SSD3/maheen-data/horse_project/data_check/horse/npy/imagenet_n02374451/n02374451_16786.npy',\n",
    "'/home/SSD3/maheen-data/horse_project/data_check/horse/im/imagenet_n02374451/n02374451_4338.jpg /home/SSD3/maheen-data/horse_project/data_check/horse/npy/imagenet_n02374451/n02374451_4338.npy']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "to_search=[file_curr[:file_curr.index(' ')] for file_curr in to_search];\n",
    "to_search=[file_curr.replace('data_check','data_resize') for file_curr in to_search];\n",
    "loss_tuple_info = parseElaborateLogFile(error_log_file); \n",
    "\n",
    "for idx in range(len(loss_tuple_info)):\n",
    "    data_curr=loss_tuple_info[idx][3];\n",
    "    for to_search_curr in to_search:\n",
    "        if to_search_curr in data_curr:\n",
    "            print 'found!'\n",
    "            print loss_tuple_info[idx+1][2];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def saveImageAndAnno(im_path,npy_path,out_path):\n",
    "    im=cv2.imread(im_path,1);\n",
    "    # im=cv2.res tiize(im,(224,224));\n",
    "    label=np.load(npy_path).astype(np.int);\n",
    "    x=label[:,0];\n",
    "    y=label[:,1];\n",
    "    color=(0,0,255);\n",
    "    colors=[(0,0,255),(0,255,0),(255,0,0),(255,255,0),(0,255,255)]\n",
    "    for label_idx in range(len(x)):\n",
    "        if label[label_idx,2]>0:\n",
    "            cv2.circle(im,(x[label_idx],y[label_idx]),5,colors[label_idx],-1);\n",
    "    cv2.imwrite(out_path,im);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[490, 491, 492, 493, 494]\n",
      "/home/SSD3/maheen-data/temp/tps_debug_problem/out_file_horse.txt\n",
      "/home/SSD3/maheen-data/temp/tps_debug_problem/out_file_human.txt\n"
     ]
    }
   ],
   "source": [
    "import cv2;\n",
    "import numpy as np;\n",
    "line_to_find='/home/SSD3/maheen-data/horse_project/data_resize/horse/im/imagenet_n02374451/n02374451_12462.jpg';\n",
    "\n",
    "input_data_file='/home/SSD3/maheen-data/horse_project/data_resize/horse/matches_5_train_allKP.txt';\n",
    "input_human_file='/home/SSD3/maheen-data/horse_project/data_resize/aflw/matches_5_train_allKP.txt';\n",
    "\n",
    "input_data=util.readLinesFromFile(input_data_file);\n",
    "data_im=[file_curr[:file_curr.index(' ')] for file_curr in input_data];\n",
    "idx_lines=[idx_line for idx_line,line_curr in enumerate(data_im) if line_curr==line_to_find];\n",
    "print idx_lines;\n",
    "\n",
    "input_human=util.readLinesFromFile(input_human_file);\n",
    "input_human=[file_curr.split(' ') for file_curr in input_human];\n",
    "input_human_rel=[input_human[idx_curr] for idx_curr in idx_lines];\n",
    "input_horse_rel=[input_data[idx_curr].split(' ') for idx_curr in idx_lines];\n",
    "assert len(input_horse_rel)==len(input_human_rel)\n",
    "\n",
    "out_dir='/home/SSD3/maheen-data/temp/tps_debug_problem';\n",
    "util.mkdir(out_dir)\n",
    "\n",
    "combo_arr=input_human_rel+input_horse_rel;\n",
    "for i in range(len(combo_arr)):\n",
    "    out_file_curr=os.path.join(out_dir,str(i)+'.jpg');\n",
    "    saveImageAndAnno(combo_arr[i][0],combo_arr[i][1],out_file_curr);\n",
    "\n",
    "visualize.writeHTMLForFolder(out_dir);\n",
    "\n",
    "out_file_horse=os.path.join(out_dir,'out_file_horse.txt')\n",
    "out_file_human=os.path.join(out_dir,'out_file_human.txt')\n",
    "util.writeFile(out_file_horse,[' '.join(line_curr) for line_curr in input_horse_rel])\n",
    "util.writeFile(out_file_human,[' '.join(line_curr) for line_curr in input_human_rel])\n",
    "print out_file_horse;\n",
    "print out_file_human;\n",
    "\n",
    "visualize.writeHTMLForFolder(os.path.join(out_dir,'test_viz'))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getValidPoints(horse_file,human_file):\n",
    "    horse_pts=np.load(horse_file);\n",
    "    human_pts=np.load(human_file);\n",
    "    check_pts=np.logical_and(horse_pts[:,2]>0,human_pts[:,2]>0);\n",
    "    valid_horse=horse_pts[check_pts,:];\n",
    "    valid_human=human_pts[check_pts,:];\n",
    "    assert valid_horse.shape[0]==valid_human.shape[0];\n",
    "    assert np.all(valid_horse[:,2]>0);\n",
    "    assert np.all(valid_human[:,2]>0)\n",
    "\n",
    "    return valid_horse,valid_human;\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getPointLineDistanceOld(line_pt_1,line_pt_2,pt):\n",
    "    assert line_pt_1.size==2;\n",
    "    assert line_pt_2.size==2;\n",
    "    assert pt.size==2;\n",
    "    diffs=line_pt_1-line_pt_2;\n",
    "    epsilon=1e-8;\n",
    "    if np.abs(diffs[0])<epsilon:\n",
    "        distance=np.abs(pt[0]-line_pt_1[0]);\n",
    "    elif np.abs(diffs[1])<epsilon:\n",
    "        distance=np.abs(pt[1]-line_pt_1[1]);\n",
    "    else:\n",
    "        m=diffs[0]/diffs[1];\n",
    "        a=-1*m;\n",
    "        b=1;\n",
    "        c=m*line_pt_1[1]-line_pt_1[0];\n",
    "        distance=np.abs(a*pt[1]+b*pt[0]+c);\n",
    "        distance=distance/np.sqrt(a*a+b*b);\n",
    "    \n",
    "    return distance;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getPointLineDistance(line_pt_1,line_pt_2,pt):\n",
    "    assert line_pt_1.size==2;\n",
    "    assert line_pt_2.size==2;\n",
    "    assert pt.size==2;\n",
    "    diffs=line_pt_2-line_pt_1;\n",
    "    epsilon=1e-8;\n",
    "    if np.sum(np.abs(diffs))<epsilon:\n",
    "        distance=float('nan')\n",
    "    elif np.abs(diffs[0])<epsilon:\n",
    "        distance=np.abs(pt[0]-line_pt_1[0]);\n",
    "    elif np.abs(diffs[1])<epsilon:\n",
    "        distance=np.abs(pt[1]-line_pt_1[1]);\n",
    "    else:\n",
    "        deno=np.sqrt(np.sum(np.power(diffs,2)));\n",
    "        numo=(diffs[0]*pt[1])-(diffs[1]*pt[0])+(line_pt_1[0]*line_pt_2[1])-(line_pt_1[1]*line_pt_2[0]);\n",
    "        numo=np.abs(numo);\n",
    "        distance=numo/deno;\n",
    "    \n",
    "    return distance;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import sqrt, fabs\n",
    "def pdis(a, b, c):\n",
    "    t = b[0]-a[0], b[1]-a[1]           # Vector ab\n",
    "    dd = sqrt(t[0]**2+t[1]**2)         # Length of ab\n",
    "    t = t[0]/dd, t[1]/dd               # unit vector of ab\n",
    "    n = -t[1], t[0]                    # normal unit vector to ab\n",
    "    ac = c[0]-a[0], c[1]-a[1]          # vector ac\n",
    "    return fabs(ac[0]*n[0]+ac[1]*n[1]) # Projection of ac to n (the minimum distance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def isValidPair((horse_file,human_file,threshold)):\n",
    "    valid_horse,valid_human=getValidPoints(horse_file,human_file);\n",
    "    isValid=True;\n",
    "    if valid_horse.shape[0]<3:\n",
    "        isValid=True;\n",
    "    elif (valid_horse.shape[0]==3):\n",
    "        distances=getAllPointLineDistancesThreePts(valid_human[:,:2]);\n",
    "        if min(distances)<=threshold:\n",
    "            isValid=False;\n",
    "    return isValid;\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getAllPointLineDistancesThreePts(pts):\n",
    "    assert pts.shape[0]==3;\n",
    "    distances=[];\n",
    "    for i in range(pts.shape[0]):\n",
    "        line_pt_1=pts[i,:2];\n",
    "        line_pt_2=pts[(i+1)%3,:2];\n",
    "        pt=pts[(i+2)%3,:2];\n",
    "        distance=getPointLineDistance(line_pt_1,line_pt_2,pt);\n",
    "        distances.append(distance);\n",
    "    return distances;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____\n",
      "0.230473 0.230473 0.230476183688\n",
      "2.3938 2.3938 2.39384069641\n",
      "0.25502 0.25502 0.255017702711\n",
      "____\n",
      "14.493 14.493 14.4930517836\n",
      "70.2129 70.2129 70.2129323472\n",
      "14.6133 14.6133 14.6132808141\n",
      "____\n",
      "5.54196 5.54196 5.54196591232\n",
      "13.4672 13.4672 13.4672137079\n",
      "9.25634 9.25634 9.25633523869\n",
      "____\n",
      "10.8994 10.8994 10.8993741142\n",
      "21.6956 21.6956 21.6955547217\n",
      "20.3328 20.3328 20.3327773952\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "out_dir='/home/SSD3/maheen-data/temp/tps_debug_problem';\n",
    "out_file_horse=os.path.join(out_dir,'out_file_horse.txt')\n",
    "out_file_human=os.path.join(out_dir,'out_file_human.txt')\n",
    "threshold=11.2;\n",
    "horse_data=util.readLinesFromFile(out_file_horse);\n",
    "human_data=util.readLinesFromFile(out_file_human);\n",
    "\n",
    "assert len(horse_data)==len(human_data);\n",
    "num_files=len(horse_data);\n",
    "\n",
    "horse_data=[line_curr.split(' ') for line_curr in horse_data];\n",
    "human_data=[line_curr.split(' ') for line_curr in human_data];\n",
    "\n",
    "\n",
    "valid_humans=[];\n",
    "for idx in range(num_files):\n",
    "    valid_horse,valid_human=getValidPoints(horse_data[idx][1],human_data[idx][1]);\n",
    "    if valid_human.shape[0]!=3:\n",
    "        continue;\n",
    "    valid_humans.append(valid_human);\n",
    "\n",
    "for valid_human in valid_humans:\n",
    "    print ('____')\n",
    "    for i in range(valid_human.shape[0]):\n",
    "        line_pt_1=valid_human[i,:2];\n",
    "        line_pt_2=valid_human[(i+1)%3,:2];\n",
    "        pt=valid_human[(i+2)%3,:2];\n",
    "        distance=getPointLineDistance(line_pt_1,line_pt_2,pt);\n",
    "        distance_2=getPointLineDistanceNew(line_pt_1,line_pt_2,pt);\n",
    "        distance_3=pdis([line_pt_1[1],line_pt_1[0]],[line_pt_2[1],line_pt_2[0]],[pt[1],pt[0]]);\n",
    "        print distance,distance_2,distance_3;\n",
    "\n",
    "\n",
    "for idx in range(num_files):\n",
    "    print isValidPair((horse_data[idx][1],human_data[idx][1],threshold))\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17719 17787 68\n",
      "/home/SSD3/maheen-data/horse_project/data_resize/horse/matches_5_train_allKP_allBad.txt\n",
      "/home/SSD3/maheen-data/horse_project/data_resize/aflw/matches_5_train_allKP_allBad.txt\n",
      "/home/SSD3/maheen-data/horse_project/data_resize/aflw/matches_5_train_allKP_noIm_allBad.txt\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing;\n",
    "input_data_file='/home/SSD3/maheen-data/horse_project/data_resize/horse/matches_5_train_allKP.txt';\n",
    "input_human_file='/home/SSD3/maheen-data/horse_project/data_resize/aflw/matches_5_train_allKP.txt';\n",
    "input_human_file_noIm='/home/SSD3/maheen-data/horse_project/data_resize/aflw/matches_5_train_allKP_noIm.txt';\n",
    "\n",
    "data_horse=util.readLinesFromFile(input_data_file);\n",
    "input_horse=[file_curr[file_curr.index(' ')+1:] for file_curr in data_horse];\n",
    "data_human=util.readLinesFromFile(input_human_file);\n",
    "input_human=[file_curr[file_curr.index(' ')+1:] for file_curr in data_human];\n",
    "threshold=11.2;\n",
    "\n",
    "args=[];\n",
    "for idx,horse_file in enumerate(input_horse):\n",
    "    human_file=input_human[idx];\n",
    "    args.append((horse_file,human_file,threshold));\n",
    "\n",
    "p=multiprocessing.Pool(multiprocessing.cpu_count());\n",
    "include_list=p.map(isValidPair,args);\n",
    "include_list=np.array(include_list);\n",
    "print np.sum(include_list),include_list.size,np.sum(~include_list);\n",
    "\n",
    "files=[input_data_file,input_human_file,input_human_file_noIm];\n",
    "for file_curr in files:\n",
    "    out_file=file_curr[:file_curr.rindex('.')]+'_allBad.txt';\n",
    "    data_curr=util.readLinesFromFile(file_curr);\n",
    "    data_rel=np.array(data_curr)[~include_list];\n",
    "    util.writeFile(out_file,data_rel);\n",
    "    print out_file;\n",
    "    \n",
    "\n",
    "# out_file_horse=input_data_file[:input_data_file.rindex('.')]+'_allBad.txt';\n",
    "# out_file_human=input_human_file[:input_human_file.rindex('.')]+'_allBad.txt';\n",
    "# out_file_human_noIm=input_human_file_noIm[:input_human_file_noIm.rindex('.')]+'_allBad.txt';\n",
    "\n",
    "# util.writeFile(out_file_horse,np.array(data_horse)[~include_list])\n",
    "# util.writeFile(out_file_human,np.array(data_human)[~include_list])\n",
    "# util.writeFile(out_file_human_noIm,np.array(data_human_noIm)[~include_list])\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def comparativeLossViz(img_dirs,file_post,loss_post,range_batches,range_images,out_file_html,dir_server,img_caption_pre=None):\n",
    "    img_files_all=[];\n",
    "    captions_all=[];\n",
    "    if img_caption_pre is not None:\n",
    "        assert len(img_caption_pre)==len(img_dirs);\n",
    "    batch_str=True;\n",
    "    if range_batches is None:\n",
    "        range_batches=range(1);\n",
    "        batch_str=False;\n",
    "    \n",
    "    for batch_num in range_batches:\n",
    "    # range(1,num_batches+1):\n",
    "        for im_num in range_images:\n",
    "            for idx_img_dir,img_dir in enumerate(img_dirs):\n",
    "                if loss_post is None:\n",
    "                    loss_str='';\n",
    "                else:\n",
    "                    loss_all=np.load(os.path.join(img_dir,str(batch_num)+loss_post));\n",
    "                    if im_num>loss_all.shape[0]:\n",
    "                        continue;\n",
    "                    loss_curr=loss_all[im_num-1,0];\n",
    "                    loss_str=\"{:10.4f}\".format(loss_curr);\n",
    "                if not batch_str:\n",
    "                    files_curr=[os.path.join(img_dir,str(im_num)+file_post_curr) for file_post_curr in file_post];\n",
    "                else:\n",
    "                    files_curr=[os.path.join(img_dir,str(batch_num)+'_'+str(im_num)+file_post_curr) for file_post_curr in file_post];\n",
    "                files_curr=[util.getRelPath(file_curr,dir_server) for file_curr in files_curr];\n",
    "                captions_curr=[os.path.split(file_curr)[1]+' '+loss_str for file_curr in files_curr];\n",
    "                if img_caption_pre is not None:\n",
    "                    captions_curr=[img_caption_pre[idx_img_dir]+' '+caption_curr for caption_curr in captions_curr];\n",
    "                img_files_all.append(files_curr);\n",
    "                captions_all.append(captions_curr);\n",
    "    print 'hello again'\n",
    "    visualize.writeHTML(out_file_html,img_files_all,captions_all,224,224);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dir_server='/home/SSD3/maheen-data';\n",
    "range_batches=range(1,2);\n",
    "range_images=range(1,69);\n",
    "img_dir=['/home/SSD3/maheen-data/temp/tps_debug_problem/test_viz_allBad']\n",
    "out_file_html=img_dir[0]+'.html';\n",
    "\n",
    "file_post=['_horse.jpg','_human.jpg','_gtwarp.jpg','_predwarp.jpg']\n",
    "loss_post='_loss.npy';\n",
    "comparativeLossViz(img_dir,file_post,loss_post,range_batches,range_images,out_file_html,dir_server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "hello again\n",
      "http://vision1.idav.ucdavis.edu:1000/horse_project/ft_horse_allKp_full_halfBack_llr_debug/debug.html\n"
     ]
    }
   ],
   "source": [
    "# '/home/SSD3/maheen-data/training_5_kp_withWarp_fixed_adam__1e-05/test'\n",
    "# dir_curr='/home/SSD3/maheen-data/training_5_kp_withWarp_fixed_adam__1e-05/test_scrap';\n",
    "import visualize;\n",
    "import os;\n",
    "dir_server='/home/SSD3/maheen-data';\n",
    "# dir_curr='/home/SSD3/maheen-data/temp/tps_train_allKP_adam_noBad_bn/val_transformed_im'\n",
    "dir_curr='/home/SSD3/maheen-data/horse_project/ft_horse_allKp_full_halfBack_llr_debug/debug'\n",
    "img_dirs=[dir_curr];\n",
    "out_file_html=dir_curr+'.html';\n",
    "loss_post=None;\n",
    "range_batches=None;\n",
    "range_images=range(1,65);\n",
    "file_post=['.jpg','_org.jpg'];\n",
    "# visualize.writeHTMLForFolder(dir_curr);\n",
    "# print dir_curr.replace(dir_server,'http://vision1.idav.ucdavis.edu:1000')+'/'+os.path.split(dir_curr)[1]+'.html';\n",
    "print 'hello';\n",
    "comparativeLossViz(img_dirs,file_post,loss_post,range_batches,range_images,out_file_html,dir_server)\n",
    "print out_file_html.replace(dir_server,'http://vision1.idav.ucdavis.edu:1000')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello again\n",
      "http://vision1.idav.ucdavis.edu:1000/horse_project/ft_horse_allKp_full_halfBack_llr/test_images.html\n"
     ]
    }
   ],
   "source": [
    "import util;\n",
    "import os;\n",
    "import visualize;\n",
    "dir_server='/home/SSD3/maheen-data';\n",
    "img_dirs=['/home/SSD3/maheen-data/temp/tps_train_allKP_adam_noBad_bn/val_transformed_im',\\\n",
    "          '/home/SSD3/maheen-data/temp/tps_train_allKP_adam_noBad_bn/val_transformed_im_40'];\n",
    "\n",
    "img_dirs=['/home/SSD3/maheen-data/horse_project/ft_horse_allKp_full_halfBack_llr/test_images']          \n",
    "file_post=['.jpg','_org.jpg'];\n",
    "loss_post=None;\n",
    "range_batches=range(1,3);\n",
    "range_images=range(1,101);\n",
    "out_file_html=img_dirs[0]+'.html';\n",
    "\n",
    "comparativeLossViz(img_dirs,file_post,loss_post,range_batches,range_images,out_file_html,dir_server)\n",
    "print out_file_html.replace(dir_server,'http://vision1.idav.ucdavis.edu:1000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[75, 675, 1275, 1875, 2475, 3075, 3675, 4275, 4875]\n",
      "/home/SSD3/maheen-data/training_5_kp_withWarp_fixed_adam__1e-05/over_time.sh\n"
     ]
    }
   ],
   "source": [
    "# path_models='/home/SSD3/maheen-data/training_5_kp_withWarp_fixed_adam__1e-05/intermediate/'\n",
    "dir_meta='/home/SSD3/maheen-data/training_5_kp_withWarp_fixed_adam__1e-05'\n",
    "path_to_models=os.path.join(dir_meta,'intermediate')\n",
    "out_dir_meta=os.path.join(dir_meta,'over_time');\n",
    "util.mkdir(out_dir_meta);\n",
    "\n",
    "model_pre=os.path.join(path_to_models,'model_all_');\n",
    "model_post='.dat'\n",
    "range_models=range(75,5000,75*8);\n",
    "print range_models\n",
    "\n",
    "path_to_th='/home/maheenrashid/Downloads/horses/torch/test_xiuye.th';\n",
    "out_file_sh=out_dir_meta+'.sh'\n",
    "commands=[];\n",
    "for model_idx,model_num in enumerate(range_models):\n",
    "    model_curr=model_pre+str(model_num)+model_post;\n",
    "    assert os.path.exists(model_curr);\n",
    "    out_dir_curr=os.path.join(out_dir_meta,str(model_num));\n",
    "    command=['th',path_to_th];\n",
    "    command.extend(['-kp_net_file',model_curr]);\n",
    "    command.extend(['out_dir',out_dir_curr]);\n",
    "    command=' '.join(command);\n",
    "    commands.append(command);\n",
    "# print commands\n",
    "util.writeFile(out_file_sh,commands)\n",
    "print out_file_sh\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('List of arrays in this file: \\n', [u'confidence', u'data', u'landmark', u'weight_in', u'weight_out'])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "h5_file='/home/laoreja/new-deep-landmark/train/vanilla/aflw_40/aflw_vanilla_train_40_0_weight.h5';\n",
    "with h5py.File(h5_file,'r') as hf:\n",
    "    print('List of arrays in this file: \\n', hf.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
