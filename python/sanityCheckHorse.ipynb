{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import util;\n",
    "import numpy as np;\n",
    "import visualize;\n",
    "# import sklearn;\n",
    "import os;\n",
    "import time;\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def getDataAndLabels(data_path,batchSize,numBatches,fc6_dir,dim_size=4096,limit=-1):\n",
    "    lines=util.readLinesFromFile(data_path);\n",
    "    if limit>-1:\n",
    "        lines=lines[:limit]\n",
    "    out_data=np.zeros((len(lines),dim_size));\n",
    "#     print out_data.shape,limit,dim_size\n",
    "    start_curr=0;\n",
    "    for i in range(numBatches):\n",
    "        file_curr=os.path.join(fc6_dir,str(i+1)+'.npy');\n",
    "        fc6_curr=np.load(file_curr);\n",
    "#         print file_curr;\n",
    "        end_curr=min(start_curr+fc6_curr.shape[0],len(lines));\n",
    "        len_curr=end_curr-start_curr\n",
    "        out_data[start_curr:end_curr,:]=fc6_curr[:len_curr];\n",
    "        start_curr=end_curr;\n",
    "    return out_data,lines;\n",
    "\n",
    "def normalizeData(data):\n",
    "    norm_data=np.linalg.norm(data,axis=1);\n",
    "    norm_data=np.expand_dims(norm_data,axis=1);\n",
    "    norm_data=np.tile(norm_data,(1,data.shape[1]));\n",
    "    data=data/norm_data;\n",
    "    return data;\n",
    "\n",
    "def getClusterIdx(data,num_clusters):\n",
    "#     norm_data=np.linalg.norm(data,axis=1);\n",
    "#     norm_data=np.expand_dims(norm_data,axis=1);\n",
    "#     norm_data=np.tile(norm_data,(1,data.shape[1]));\n",
    "#     data=data/norm_data;\n",
    "    data=normalizeData(data)\n",
    "    kmeaner=KMeans(n_clusters=num_clusters,n_jobs=12);\n",
    "    cluster_idx=kmeaner.fit_predict(data);\n",
    "    return cluster_idx;\n",
    "\n",
    "def saveClusterIdx(data_path,batchSize,numBatches,fc6_dir,num_clusters,out_file_clusters,dim_size=4096,limit=-1):\n",
    "    data,labels = getDataAndLabels(data_path,batchSize,numBatches,fc6_dir,dim_size,limit);\n",
    "    cluster_idx = getClusterIdx(data,num_clusters);\n",
    "    print out_file_clusters,cluster_idx.shape\n",
    "    np.save(out_file_clusters,cluster_idx);\n",
    "\n",
    "def makeClusterHTML(out_file_html,labels,num_cols,size_im,dir_server):\n",
    "    ims=[];\n",
    "    captions=[];\n",
    "    start_idx=0;\n",
    "    while start_idx<len(labels):\n",
    "        row_curr=[];\n",
    "        caption_curr=[];\n",
    "        if start_idx+num_cols>len(labels):\n",
    "            num_cols_real=len(labels)-start_idx;\n",
    "        else:\n",
    "            num_cols_real=num_cols;\n",
    "        for col_no in range(num_cols_real):\n",
    "            idx_curr=start_idx+col_no;\n",
    "            label_curr=labels[idx_curr];\n",
    "            row_curr.append(util.getRelPath(label_curr,dir_server));\n",
    "            caption_curr.append('');\n",
    "        ims.append(row_curr);\n",
    "        captions.append(caption_curr);\n",
    "        start_idx=start_idx+num_cols_real;\n",
    "    visualize.writeHTML(out_file_html,ims,captions,size_im,size_im);\n",
    "    print out_file_html.replace(dir_server,'http://vision1.idav.ucdavis.edu:1000')\n",
    "\n",
    "def script_makeClusterHTML(labels,clusters,out_dir,dir_server):\n",
    "    labels=np.array(labels);\n",
    "    clusters_uni=list(set(clusters));\n",
    "    labels_clusters=[];\n",
    "    for cluster_idx in clusters_uni:\n",
    "        labels_rel=labels[clusters==cluster_idx];\n",
    "        print len(labels_rel);\n",
    "        labels_clusters.append(labels_rel);\n",
    "        out_file_html=os.path.join(out_dir,str(cluster_idx)+'.html');\n",
    "        num_cols=40;\n",
    "        size_im=20;\n",
    "        makeClusterHTML(out_file_html,labels_rel,num_cols,size_im,dir_server);\n",
    "\n",
    "\n",
    "def getDotProduct(out_file_dot,\n",
    "                  batchSize,\n",
    "                  dim_size,\n",
    "                  limit,\n",
    "                horse_data_path,\n",
    "                  horse_numBatches,\n",
    "                  horse_fc6_dir,\n",
    "                  human_data_path,\n",
    "                 human_numBatches,\n",
    "                 human_fc6_dir):\n",
    "    horse_data,horse_labels=getDataAndLabels(horse_data_path,batchSize,horse_numBatches,horse_fc6_dir,dim_size,limit);\n",
    "    horse_data=normalizeData(horse_data);\n",
    "    \n",
    "    face_data,face_labels=getDataAndLabels(human_data_path,batchSize,human_numBatches,human_fc6_dir,dim_size,limit);\n",
    "    face_data=normalizeData(face_data);\n",
    "#     print (horse_data.shape,face_data.shape);\n",
    "    dot_product=np.dot(horse_data,face_data.T);\n",
    "#     print (dot_product.shape);\n",
    "\n",
    "    np.save(out_file_dot,dot_product)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    print 'hello';\n",
    "    dir_server='/home/SSD3/maheen-data';\n",
    "    \n",
    "    data_path='/home/SSD3/maheen-data/horse_project/data_check/horse/pairs_all.txt';\n",
    "    out_dir='/home/SSD3/maheen-data/horse_project/sanityCheckHorse_face'\n",
    "    fc6_dir=os.path.join(out_dir,'horse_fc6');\n",
    "    numBatches=60;\n",
    "    \n",
    "#     data_path='/home/SSD3/maheen-data/data_face_network/aflw_cvpr_train.txt';\n",
    "#     out_dir='/home/SSD3/maheen-data/horse_project/sanityCheckHorse_face'\n",
    "#     fc6_dir=os.path.join(out_dir,'face_fc6');\n",
    "#     numBatches=493;\n",
    "    \n",
    "    batchSize=64;\n",
    "    \n",
    "\n",
    "    num_clusters=32;\n",
    "    dim_size=100;\n",
    "    limit=-1;\n",
    "    out_file_clusters=os.path.join(fc6_dir,'clusters.npy');\n",
    "    saveClusterIdx(data_path,batchSize,numBatches,fc6_dir,num_clusters,out_file_clusters,dim_size,limit);\n",
    "\n",
    "    labels=util.readLinesFromFile(data_path);\n",
    "    labels=[line_curr[:line_curr.index(' ')] for line_curr in labels];\n",
    "    if limit>-1:\n",
    "        labels=labels[:limit];\n",
    "\n",
    "    clusters=np.load(out_file_clusters);\n",
    "    script_makeClusterHTML(labels,clusters,out_dir,dir_server)\n",
    "\n",
    "    data,labels=getDataAndLabels(data_path,batchSize,numBatches,fc6_dir,dim_size,limit);\n",
    "    data=normalizeData(data);\n",
    "    clusters_uni=list(set(clusters));\n",
    "    for cluster_idx in clusters_uni:\n",
    "        data_rel=data[clusters==cluster_idx,:];\n",
    "        labels_rel=data[clusters==cluster_idx];\n",
    "        pairwise=np.dot(data_rel,data_rel.T);\n",
    "        print pairwise.shape;\n",
    "        print np.min(pairwise),np.max(pairwise);\n",
    "        break;\n",
    "\n",
    "main();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def getImgPaths(dot_product,top_num,bottom_num,horse_paths,human_paths,dir_server):\n",
    "    img_paths=[];\n",
    "    idx_sort=np.argsort(dot_product,1)[:,::-1];\n",
    "    assert dot_product.shape[0]==len(horse_paths);\n",
    "    assert dot_product.shape[1]==len(human_paths);\n",
    "    for idx in range(dot_product.shape[0]):\n",
    "        dot_sorted=dot_product[idx,idx_sort[idx]];   \n",
    "        horse_curr=horse_paths[idx];\n",
    "\n",
    "        if top_num>0:\n",
    "            human_best=[human_paths[idx_curr] for idx_curr in idx_sort[idx,:top_num]];\n",
    "            best_row=[horse_curr]+human_best;\n",
    "            best_row=[util.getRelPath(file_curr,dir_server) for file_curr in best_row]\n",
    "            img_paths.append(best_row);\n",
    "            \n",
    "        if bottom_num>0:\n",
    "            human_worst=[human_paths[idx_curr] for idx_curr in idx_sort[idx,-bottom_num:]];\n",
    "            worst_row=[horse_curr]+human_worst;\n",
    "            worst_row=[util.getRelPath(file_curr,dir_server) for file_curr in worst_row]\n",
    "            img_paths.append(worst_row);\n",
    "    return img_paths;\n",
    "\n",
    "def main():\n",
    "    print 'hello';\n",
    "    dir_server='/home/SSD3/maheen-data';\n",
    "    click_str='http://vision1.idav.ucdavis.edu:1000'\n",
    "    batchSize=64;\n",
    "    num_clusters=32;\n",
    "    dim_size=100;\n",
    "    limit=-1;\n",
    "    out_dir='/home/SSD3/maheen-data/horse_project/sanityCheckHorse_face'\n",
    "    out_file_dot=os.path.join(out_dir,'dot_product.npy')\n",
    "    out_file_html=os.path.join(out_dir,'matches.html');\n",
    "    top_num=5;\n",
    "    bottom_num=0;\n",
    "    \n",
    "    horse_data_path='/home/SSD3/maheen-data/horse_project/data_check/horse/pairs_all.txt';\n",
    "    horse_fc6_dir=os.path.join(out_dir,'horse_fc6');\n",
    "    horse_numBatches=60;\n",
    "    \n",
    "\n",
    "    human_data_path='/home/SSD3/maheen-data/data_face_network/aflw_cvpr_train.txt';\n",
    "    human_fc6_dir=os.path.join(out_dir,'face_fc6');\n",
    "    human_numBatches=493;\n",
    "    \n",
    "#     getDotProduct(out_file_dot,batchSize,dim_size,limit,\\\n",
    "#                 horse_data_path,horse_numBatches,horse_fc6_dir,\\\n",
    "#                   human_data_path,human_numBatches,human_fc6_dir);\n",
    "    \n",
    "    dot_product=np.load(out_file_dot);\n",
    "    data_horse=util.readLinesFromFile(horse_data_path);\n",
    "    data_horse=[file_curr.split(' ')[0] for file_curr in data_horse]\n",
    "    data_human=util.readLinesFromFile(human_data_path);\n",
    "    data_human=[file_curr.split(' ')[0] for file_curr in data_human]\n",
    "    \n",
    "    img_paths=getImgPaths(dot_product,top_num,bottom_num,data_horse,data_human,dir_server);\n",
    "    captions=[];\n",
    "\n",
    "    for r in range(len(img_paths)):\n",
    "        caption_row=[];\n",
    "        for c in range(len(img_paths[r])):\n",
    "            caption_row.append(' ');\n",
    "        captions.append(caption_row);\n",
    "    \n",
    "    visualize.writeHTML(out_file_html,img_paths,captions,100,100);\n",
    "    print out_file_html.replace(dir_server,click_str)\n",
    "#     dot_curr=dot_product[idx];\n",
    "   \n",
    "#     print idx_sort.shape\n",
    "#     horse_curr=data_horse[idx];\n",
    "    \n",
    "    \n",
    "    \n",
    "    print 'done'\n",
    "\n",
    "main();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186\n",
      "/home/SSD3/maheen-data/horse_project/data_check/horse/matches_5_val_allKP_minLoss_clean.txt (186,) 192 /home/SSD3/maheen-data/horse_project/data_check/horse/matches_5_val_allKP_minLoss_clean_noDuplicates.txt\n",
      "/home/SSD3/maheen-data/horse_project/data_check/aflw/matches_5_val_allKP_minLoss_clean.txt (186,) 192 /home/SSD3/maheen-data/horse_project/data_check/aflw/matches_5_val_allKP_minLoss_clean_noDuplicates.txt\n",
      "/home/SSD3/maheen-data/horse_project/data_check/aflw/matches_5_val_allKP_minLoss_noIm_clean.txt (186,) 192 /home/SSD3/maheen-data/horse_project/data_check/aflw/matches_5_val_allKP_minLoss_noIm_clean_noDuplicates.txt\n"
     ]
    }
   ],
   "source": [
    "def getImgPaths(dot_product,top_num,bottom_num,horse_paths,human_paths,dir_server):\n",
    "    img_paths=[];\n",
    "    idx_sort=np.argsort(dot_product,1)[:,::-1];\n",
    "    assert dot_product.shape[0]==len(horse_paths);\n",
    "    assert dot_product.shape[1]==len(human_paths);\n",
    "    for idx in range(dot_product.shape[0]):\n",
    "        dot_sorted=dot_product[idx,idx_sort[idx]];   \n",
    "        horse_curr=horse_paths[idx];\n",
    "\n",
    "        if top_num>0:\n",
    "            human_best=[human_paths[idx_curr] for idx_curr in idx_sort[idx,:top_num]];\n",
    "            best_row=[horse_curr]+human_best;\n",
    "            best_row=[util.getRelPath(file_curr,dir_server) for file_curr in best_row]\n",
    "            img_paths.append(best_row);\n",
    "            \n",
    "        if bottom_num>0:\n",
    "            human_worst=[human_paths[idx_curr] for idx_curr in idx_sort[idx,-bottom_num:]];\n",
    "            worst_row=[horse_curr]+human_worst;\n",
    "            worst_row=[util.getRelPath(file_curr,dir_server) for file_curr in worst_row]\n",
    "            img_paths.append(worst_row);\n",
    "    return img_paths;\n",
    "\n",
    "def main():\n",
    "    print 'hello';\n",
    "    dir_server='/home/SSD3/maheen-data';\n",
    "    click_str='http://vision1.idav.ucdavis.edu:1000'\n",
    "    batchSize=128;\n",
    "#     num_clusters=32;\n",
    "    dim_size=4096;\n",
    "    limit=-1;\n",
    "    out_dir='/home/SSD3/maheen-data/horse_project/sanityCheckHorse_cvpr'\n",
    "    out_file_dot=os.path.join(out_dir,'dot_product.npy')\n",
    "    out_file_html=os.path.join(out_dir,'matches.html');\n",
    "    top_num=2;\n",
    "    bottom_num=0;\n",
    "    \n",
    "    horse_data_path='/home/SSD3/maheen-data/horse_project/data_check/horse/matches_5_val_allKP_minLoss_clean.txt';\n",
    "    horse_fc6_dir=os.path.join(out_dir,'test');\n",
    "    horse_numBatches=2;\n",
    "    \n",
    "\n",
    "    human_data_path='/home/SSD3/maheen-data/horse_project/neighbor_data/small_datasets/matches_5_3531_horse_minloss.txt';\n",
    "    human_fc6_dir=os.path.join(out_dir,'train');\n",
    "    human_numBatches=28;\n",
    "    \n",
    "#     getDotProduct(out_file_dot,batchSize,dim_size,limit,\\\n",
    "#                 horse_data_path,horse_numBatches,horse_fc6_dir,\\\n",
    "#                   human_data_path,human_numBatches,human_fc6_dir);\n",
    "    \n",
    "    dot_product=np.load(out_file_dot);\n",
    "    data_horse=util.readLinesFromFile(horse_data_path);\n",
    "    data_horse=[file_curr.split(' ')[0] for file_curr in data_horse]\n",
    "    data_human=util.readLinesFromFile(human_data_path);\n",
    "    data_human=[file_curr.split(' ')[0] for file_curr in data_human]\n",
    "    \n",
    "    img_paths=getImgPaths(dot_product,top_num,bottom_num,data_horse,data_human,dir_server);\n",
    "    captions=[];\n",
    "\n",
    "    for r in range(len(img_paths)):\n",
    "        caption_row=[];\n",
    "        for c in range(len(img_paths[r])):\n",
    "            img_path_curr=img_paths[r][c];\n",
    "            img_path_curr=img_path_curr.split('/')[-2:];\n",
    "            img_path_curr='/'.join(img_path_curr);\n",
    "            caption_row.append(img_path_curr);\n",
    "        captions.append(caption_row);\n",
    "    \n",
    "    visualize.writeHTML(out_file_html,img_paths,captions,100,100);\n",
    "    print out_file_html.replace(dir_server,click_str)\n",
    "    \n",
    "    print 'done'\n",
    "\n",
    "    \n",
    "def problemRemoval():\n",
    "    test_data=['_04_Aug16_png/horse+head181.jpg',\n",
    "    'error_deleted/horse+head122_2.jpg',\n",
    "    'flickr-horses/4375.jpg',\n",
    "    'flickr-horses/452.jpg',\n",
    "    'flickr-horses/7397.jpg',\n",
    "    'google_horse_face/horse+face25.jpg']    \n",
    "    \n",
    "    val_data_path='/home/SSD3/maheen-data/horse_project/data_check/horse/matches_5_val_allKP_minLoss_clean.txt';\n",
    "    val_data_path_human='/home/SSD3/maheen-data/horse_project/data_check/aflw/matches_5_val_allKP_minLoss_clean.txt';\n",
    "    val_data_path_human_noIm='/home/SSD3/maheen-data/horse_project/data_check/aflw/matches_5_val_allKP_minLoss_noIm_clean.txt';\n",
    "    out_file_post='_noDuplicates.txt';\n",
    "    \n",
    "    lines_val=util.readLinesFromFile(val_data_path);\n",
    "    idx_keep=[];\n",
    "    for idx_curr, line_curr in enumerate(lines_val):\n",
    "        keep=True;\n",
    "        for check_line in test_data:\n",
    "            if check_line in line_curr:\n",
    "                keep=False;\n",
    "                break;\n",
    "        if keep:\n",
    "            idx_keep.append(idx_curr);\n",
    "    \n",
    "    print len(idx_keep);\n",
    "    for file_curr in [val_data_path,val_data_path_human,val_data_path_human_noIm]:\n",
    "        lines_curr=util.readLinesFromFile(file_curr);\n",
    "        out_file_curr=file_curr[:file_curr.rindex('.')]+out_file_post;\n",
    "        \n",
    "        lines_keep=np.array(lines_curr)[idx_keep];\n",
    "        print file_curr,lines_keep.shape,len(lines_curr),out_file_curr;\n",
    "        util.writeFile(out_file_curr,lines_keep);\n",
    "    \n",
    "    \n",
    "problemRemoval();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
