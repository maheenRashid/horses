{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/SSD3/maheen-data/horse_data_cleaned/_04_Aug16_png/gxy/horse+head262.png\n",
      "(1000, 1000, 3)\n",
      "(5, 3)\n",
      "[288 348   1]\n",
      "[469 373   1]\n",
      "[252 716   1]\n",
      "[350 759   1]\n",
      "http://vision1.idav.ucdavis.edu:1000/temp/temp.png\n"
     ]
    }
   ],
   "source": [
    "import visualize;\n",
    "import os;\n",
    "import util;\n",
    "import numpy as np;\n",
    "import shutil;\n",
    "import random;\n",
    "import cv2;\n",
    "dir_server='/home/SSD3/maheen-data';\n",
    "click_str='http://vision1.idav.ucdavis.edu:1000';\n",
    "\n",
    "def main():\n",
    "    out_dir='/home/SSD3/maheen-data/horse_data_cleaned';\n",
    "    new_horse_train_list=os.path.join(out_dir,'valImageList.txt');\n",
    "    out_dir_im=os.path.join(dir_server,'temp');\n",
    "    out_file=os.path.join(out_dir_im,'temp.png')\n",
    "    data=util.readLinesFromFile(new_horse_train_list);\n",
    "#     random.shuffle(data);\n",
    "    img_line=data[0];\n",
    "    img_line=img_line.split(' ');\n",
    "    img_path=img_line[0].replace('./',out_dir+'/');\n",
    "    print img_path;\n",
    "    box_data=[int(num) for num in img_line[1:1+4]];\n",
    "    im=cv2.imread(img_path);\n",
    "    print im.shape\n",
    "    cv2.rectangle(im,(box_data[0],box_data[2]),(box_data[1],box_data[3]),(255,0,0));\n",
    "\n",
    "    points=[int(num) for num in img_line[5:]];\n",
    "\n",
    "    points=np.array(points);\n",
    "\n",
    "    points=np.reshape(points,(5,3));\n",
    "    print points.shape\n",
    "    \n",
    "    colors=[(255,0,0),(0,255,0),(0,0,255),(255,255,0),(0,255,255)]\n",
    "    for idx,point in enumerate(points):\n",
    "        if point[2]>0:\n",
    "            print point\n",
    "            cv2.circle(im,(point[0],point[1]),2,colors[idx],-1);\n",
    "#             break;\n",
    "    cv2.imwrite(out_file,im);\n",
    "    print out_file.replace(dir_server,click_str);\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "if __name__=='__main__':\n",
    "    main();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def moveData():\n",
    "    out_dir='/home/SSD3/maheen-data/horse_data_cleaned';\n",
    "    old_meta_path='/home/laoreja/data/horse-images/annotation';\n",
    "    \n",
    "    horse_train_list=os.path.join(out_dir,'valImageList_old_paths.txt');\n",
    "    new_horse_train_list=os.path.join(out_dir,'valImageList.txt');\n",
    "    \n",
    "    \n",
    "    im_paths=[line_curr.split(' ')[0] for line_curr in util.readLinesFromFile(horse_train_list)];\n",
    "    new_paths=[path_curr.replace(old_meta_path,out_dir) for path_curr in im_paths];\n",
    "    for old_path,new_path in zip(im_paths,new_paths):\n",
    "        new_path_dir=os.path.split(new_path)[0];\n",
    "        if not os.path.exists(new_path_dir):\n",
    "            os.makedirs(new_path_dir);\n",
    "\n",
    "        if not os.path.exists(new_path):\n",
    "            print old_path,new_path;\n",
    "            shutil.copyfile(old_path,new_path);\n",
    "            \n",
    "    print 'done'\n",
    "    \n",
    "    data_old=util.readLinesFromFile(horse_train_list);\n",
    "    data_new=[line_curr.replace(old_meta_path,'.') for line_curr in data_old];\n",
    "    util.writeFile(new_horse_train_list,data_new);\n",
    "    print new_horse_train_list;\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def makeCleanFiles():\n",
    "    out_dir='/home/SSD3/maheen-data/horse_data_cleaned';\n",
    "    util.mkdir(out_dir);\n",
    "\n",
    "    horse_old_file='/home/SSD3/maheen-data/horse_project/data_check/horse/matches_5_train_allKP_minLoss.txt';\n",
    "    horse_new_file='/home/SSD3/maheen-data/horse_project/data_check/horse/matches_5_train_allKP_minLoss_clean.txt';\n",
    "    horse_train_list='/home/laoreja/new-deep-landmark/dataset/train/trainImageList_2.txt';\n",
    "    new_horse_train_list=os.path.join(out_dir,'trainImageList_old_paths.txt');\n",
    "    \n",
    "    horse_old_file='/home/SSD3/maheen-data/horse_project/data_check/horse/matches_5_val_allKP_minLoss.txt';\n",
    "    horse_new_file='/home/SSD3/maheen-data/horse_project/data_check/horse/matches_5_val_allKP_minLoss_clean_noDuplicates.txt';\n",
    "    horse_train_list='/home/laoreja/new-deep-landmark/dataset/train/valImageList_2.txt';\n",
    "    new_horse_train_list=os.path.join(out_dir,'valImageList_old_paths.txt');\n",
    "    \n",
    "    horse_old_data=np.array([line_curr.split(' ')[0] for line_curr in util.readLinesFromFile(horse_old_file)]);\n",
    "    horse_new_data=np.array([line_curr.split(' ')[0] for line_curr in util.readLinesFromFile(horse_new_file)]);\n",
    "    \n",
    "    horse_train_data=np.array(util.readLinesFromFile(horse_train_list));\n",
    "    horse_train_data_pruned=[];\n",
    "    \n",
    "    for line_curr in horse_train_data:\n",
    "        line_curr=line_curr.split(' ')[0];\n",
    "        line_curr=line_curr.split('/');\n",
    "        if line_curr[-2]=='gxy':\n",
    "            path_pre=line_curr[-3];\n",
    "        else:\n",
    "            path_pre=line_curr[-2];\n",
    "        line_curr=path_pre+'/'+line_curr[-1];\n",
    "        line_curr=line_curr[:line_curr.rindex('.')]+'.jpg';\n",
    "        horse_train_data_pruned.append(line_curr);\n",
    "    \n",
    "    horse_new_data_pruned=['/'.join(line_curr.split('/')[-2:]) for line_curr in horse_new_data];\n",
    "    horse_problem_bin=np.in1d(horse_train_data_pruned,horse_new_data_pruned, invert=False);\n",
    "    horse_train_data_new=horse_train_data[horse_problem_bin];\n",
    "    print horse_train_data_new.shape;\n",
    "    util.writeFile(new_horse_train_list,horse_train_data_new);\n",
    "    \n",
    "    \n",
    "    print horse_problem_bin.shape,sum(horse_problem_bin);\n",
    "    assert sum(horse_problem_bin)==len(horse_new_data_pruned);\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
